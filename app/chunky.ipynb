{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d69a276f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-groq in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3.6)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.13.1)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3.72)\n",
      "Requirement already satisfied: ipython in c:\\users\\ashish\\appdata\\roaming\\python\\python311\\site-packages (9.4.0)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3.25)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langgraph) (2.1.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langgraph) (0.1.8)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langgraph) (0.1.70)\n",
      "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core) (0.3.45)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core) (6.0.2)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core) (2.11.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.10.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.18)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith>=0.3.45->langchain-core) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.7.4->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.7.4->langchain-core) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic>=2.7.4->langchain-core) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: groq<1,>=0.29.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-groq) (0.30.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.29.0->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from groq<1,>=0.29.0->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-community) (3.12.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-community) (2.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jedi>=0.16->ipython) (0.8.4)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack_data->ipython) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack_data->ipython) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack_data->ipython) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-faiss in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-google-genai in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-google-genai) (0.6.18)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.49 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-google-genai) (0.3.72)\n",
      "Requirement already satisfied: pydantic<3,>=2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-google-genai) (2.11.2)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.25.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.40.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (6.31.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.73.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.73.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (0.3.45)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (4.13.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2025.4.26)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygraphviz\n",
      "  Using cached pygraphviz-1.14.tar.gz (106 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: pygraphviz\n",
      "  Building wheel for pygraphviz (pyproject.toml): started\n",
      "  Building wheel for pygraphviz (pyproject.toml): finished with status 'error'\n",
      "Failed to build pygraphviz\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for pygraphviz (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [88 lines of output]\n",
      "      C:\\Users\\Ashish\\AppData\\Local\\Temp\\pip-build-env-68mht6r6\\overlay\\Lib\\site-packages\\setuptools\\config\\_apply_pyprojecttoml.py:82: SetuptoolsDeprecationWarning: `project.license` as a TOML table is deprecated\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please use a simple string containing a SPDX expression for `project.license`. You can also use `project.license-files`. (Both options available on setuptools>=77.0.0).\n",
      "      \n",
      "              By 2026-Feb-18, you need to update your project and remove deprecated calls\n",
      "              or your builds will no longer be supported.\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        corresp(dist, value, root_dir)\n",
      "      C:\\Users\\Ashish\\AppData\\Local\\Temp\\pip-build-env-68mht6r6\\overlay\\Lib\\site-packages\\setuptools\\config\\_apply_pyprojecttoml.py:61: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "              License :: OSI Approved :: BSD License\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        dist._finalize_license_expression()\n",
      "      C:\\Users\\Ashish\\AppData\\Local\\Temp\\pip-build-env-68mht6r6\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "              License :: OSI Approved :: BSD License\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        self._finalize_license_expression()\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-311\\pygraphviz\n",
      "      copying pygraphviz\\agraph.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\n",
      "      copying pygraphviz\\graphviz.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\n",
      "      copying pygraphviz\\scraper.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\n",
      "      copying pygraphviz\\testing.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\n",
      "      copying pygraphviz\\__init__.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\n",
      "      creating build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_attribute_defaults.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_clear.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_close.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_drawing.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_edge_attributes.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_graph.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_html.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_layout.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_node_attributes.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_readwrite.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_repr_mimebundle.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_scraper.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_string.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_subgraph.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\test_unicode.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      copying pygraphviz\\tests\\__init__.py -> build\\lib.win-amd64-cpython-311\\pygraphviz\\tests\n",
      "      running egg_info\n",
      "      writing pygraphviz.egg-info\\PKG-INFO\n",
      "      writing dependency_links to pygraphviz.egg-info\\dependency_links.txt\n",
      "      writing top-level names to pygraphviz.egg-info\\top_level.txt\n",
      "      reading manifest file 'pygraphviz.egg-info\\SOURCES.txt'\n",
      "      reading manifest template 'MANIFEST.in'\n",
      "      warning: no files found matching '*.swg'\n",
      "      warning: no files found matching '*.png' under directory 'doc'\n",
      "      warning: no files found matching '*.html' under directory 'doc'\n",
      "      warning: no files found matching '*.txt' under directory 'doc'\n",
      "      warning: no files found matching '*.css' under directory 'doc'\n",
      "      warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "      warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
      "      warning: no previously-included files matching '.svn' found anywhere in distribution\n",
      "      no previously-included directories found matching 'doc\\build'\n",
      "      adding license file 'LICENSE'\n",
      "      writing manifest file 'pygraphviz.egg-info\\SOURCES.txt'\n",
      "      copying pygraphviz\\graphviz.i -> build\\lib.win-amd64-cpython-311\\pygraphviz\n",
      "      copying pygraphviz\\graphviz_wrap.c -> build\\lib.win-amd64-cpython-311\\pygraphviz\n",
      "      running build_ext\n",
      "      building 'pygraphviz._graphviz' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pygraphviz\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (pygraphviz)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (4.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langgraph langchain langchain-groq typing_extensions langchain-core ipython langchain-community\n",
    "%pip install langchain-faiss\n",
    "%pip install langchain-google-genai\n",
    "%pip install pygraphviz\n",
    "%pip install beautifulsoup4\n",
    "\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import AIMessage, SystemMessage, HumanMessage, RemoveMessage, AIMessageChunk\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.graph.message import MessagesState, add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Literal\n",
    "\n",
    "from langgraph.types import interrupt, Command\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3826fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-2.0-flash',temperature=0,api_key=\"\")\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model = \"models/text-embedding-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "701e0ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\ashish\\desktop\\trial\\ai_bot\\venv\\lib\\site-packages (1.26.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymupdf\n",
    "loader = PyMuPDFLoader(\"class10phy.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "642cb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 100)\n",
    "split_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "00e39070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.11.0.post1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from faiss-cpu) (2.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu\n",
    "\n",
    "vector_store = FAISS.from_documents(embedding = embedding_model, documents = split_docs)\n",
    "vector_store.save_local(folder_path='/content', index_name='class_10_phy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9d74e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.load_local(folder_path='/content', index_name='class_10_phy', embeddings = embedding_model, allow_dangerous_deserialization=True)\n",
    "retriever = vector_store.as_retriever(search_type = \"mmr\", search_kwargs = {\"k\" : 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9f8d9c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what are the 2 laws of reflection?\"\n",
    "relevant_docs = retriever.invoke(query)\n",
    "relevant_docs = [m.page_content for m in relevant_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e4de11c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two laws of reflection are:\n",
      "\n",
      "1.  **The angle of incidence is equal to the angle of reflection.**\n",
      "2.  **The incident ray, the normal to the mirror at the point of incidence, and the reflected ray, all lie in the same plane.**\n",
      "\n",
      "These laws are applicable to all types of reflecting surfaces, including spherical surfaces.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are given the query from the user , along with the content extracted form the vector embeddings related to the query.\n",
    "Using this give the answer to the query.\n",
    "These are questions related to a physics chapter of class 10.\n",
    "Give the response of the solution in a complete way.\n",
    "User Query: {query}\n",
    "Relevant Content: {relevant_docs}\n",
    "If the given content doesn't include the answer to the query, Give the response as 'I don't know'\n",
    "\"\"\"\n",
    "new_prompt = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "chain = new_prompt | llm\n",
    "response = chain.invoke({\"query\": query, \"relevant_docs\" : relevant_docs})\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d57a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "dd52b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load PDF using PyMuPDF\n",
    "loader = PyMuPDFLoader(\"class10phy.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split documents with overlap\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=50)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "# Metadata and UUID assignment\n",
    "metadata_json = []\n",
    "\n",
    "for i, doc in enumerate(split_docs):\n",
    "    page = doc.metadata.get(\"page\", 0) + 1  # PyMuPDF is 0-indexed\n",
    "    uid = str(uuid.uuid4())\n",
    "    chunk_text = doc.page_content.strip()\n",
    "\n",
    "    # Update LangChain doc metadata (for potential future use)\n",
    "    doc.metadata.update({\n",
    "        \"uuid\": uid,\n",
    "        \"page\": page,\n",
    "        \"source\": \"class10phy.pdf\"\n",
    "    })\n",
    "\n",
    "    # Store in metadata JSON\n",
    "    metadata_json.append({\n",
    "        \"uuid\": uid,\n",
    "        \"page\": page,\n",
    "        \"chunk_id\": f\"page_{page}_chunk_{i}\",\n",
    "        \"text\": chunk_text,\n",
    "        \"source\": \"class10phy.pdf\"\n",
    "    })\n",
    "\n",
    "# Save metadata to file\n",
    "with open(\"chunk_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata_json, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0adcbbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Answer: The two laws of reflection are: (i) the angle of incidence is equal to the angle of reflection, and (ii) the incident ray, the normal to the mirror at the point of incidence, and the reflected ray, all lie in the same plane.\n",
      "\n",
      "🔍 Chunks used:\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# --- Load vector store ---\n",
    "vector_store = FAISS.load_local(\n",
    "    folder_path='/content',\n",
    "    index_name='class_10_phy',\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})\n",
    "\n",
    "# --- Load chunk metadata JSON ---\n",
    "with open(\"chunk_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunk_metadata = json.load(f)\n",
    "metadata_lookup = {entry[\"text\"]: entry for entry in chunk_metadata}\n",
    "\n",
    "# --- Run the query ---\n",
    "query = \"what are the 2 laws of reflection?\"\n",
    "docs: list[Document] = retriever.invoke(query)\n",
    "relevant_chunks = [doc.page_content for doc in docs]\n",
    "\n",
    "# --- Prepare prompt and invoke LLM ---\n",
    "prompt = \"\"\"You are given the query from the user, along with the content extracted from the vector embeddings related to the query.\n",
    "Using this, give the answer to the query.\n",
    "These are questions related to a physics chapter of class 10.\n",
    "Give the response in a complete sentence.\n",
    "User Query: {query}\n",
    "Relevant Content: {relevant_docs}\n",
    "If the given content doesn't include the answer to the query, respond with 'I don't know'.\"\"\"\n",
    "\n",
    "new_prompt = ChatPromptTemplate.from_template(prompt)\n",
    "chain = new_prompt | llm\n",
    "response = chain.invoke({\"query\": query, \"relevant_docs\": relevant_chunks})\n",
    "\n",
    "# --- Print final answer ---\n",
    "print(\"✅ Answer:\", response.content)\n",
    "print(\"\\n🔍 Chunks used:\\n\" + \"-\" * 60)\n",
    "\n",
    "# --- Print metadata and chunk content ---\n",
    "for chunk_text in relevant_chunks:\n",
    "    match = metadata_lookup.get(chunk_text)\n",
    "    if match:\n",
    "        print(f\"🆔 Chunk ID: {match['chunk_id']}\")\n",
    "        print(f\"📄 Page: {match['page']}\")\n",
    "        print(f\"📄 Text: {match['text']}\\n{'-' * 60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0628fa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Answer: The two laws of reflection are: (i) the angle of incidence is equal to the angle of reflection, and (ii) the incident ray, the normal to the mirror at the point of incidence and the reflected ray, all lie in the same plane.\n",
      "\n",
      "🔍 Chunks used:\n",
      "------------------------------------------------------------\n",
      "🆔 Chunk ID: page_2_chunk_7\n",
      "📄 Page: 2\n",
      "📄 Text: Light – Reflection and Refraction\n",
      "135\n",
      "Let us recall these laws –\n",
      "(i)\n",
      "The angle of incidence is equal to the angle of reflection, and\n",
      "(ii)\n",
      "The incident ray, the normal to the mirror at the point of incidence\n",
      "and the reflected ray, all lie in the same plane.\n",
      "These laws of reflection are applicable to all types of reflecting surfaces\n",
      "including spherical surfaces. You are familiar with the formation of image\n",
      "by a plane mirror. What are the properties of the image? Image formed\n",
      "------------------------------------------------------------\n",
      "🆔 Chunk ID: page_25_chunk_142\n",
      "📄 Page: 25\n",
      "📄 Text: What you have learnt\n",
      "n\n",
      "Light seems to travel in straight lines.\n",
      "n\n",
      "Mirrors and lenses form images of objects. Images can be either real or virtual,\n",
      "depending on the position of the object.\n",
      "n\n",
      "The reflecting surfaces, of all types, obey the laws of reflection. The refracting\n",
      "surfaces obey the laws of refraction.\n",
      "n\n",
      "New Cartesian Sign Conventions are followed for spherical mirrors and lenses.\n",
      "More to Know!\n",
      "Reprint 2025-26\n",
      "------------------------------------------------------------\n",
      "🆔 Chunk ID: page_1_chunk_6\n",
      "📄 Page: 1\n",
      "📄 Text: 9.1 REFLECTION OF LIGHT\n",
      "9.1 REFLECTION OF LIGHT\n",
      "9.1 REFLECTION OF LIGHT\n",
      "9.1 REFLECTION OF LIGHT\n",
      "A highly polished surface, such as a mirror, reflects most of the light\n",
      "falling on it. You are already familiar with the laws of reflection of light.\n",
      "Reprint 2025-26\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import json\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# --- Load PDF and split into chunks ---\n",
    "loader = PyMuPDFLoader(\"class10phy.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "# --- Assign UUIDs + metadata + save JSON ---\n",
    "metadata_json = []\n",
    "for i, doc in enumerate(split_docs):\n",
    "    page = doc.metadata.get(\"page\", 0) + 1\n",
    "    uid = str(uuid.uuid4())\n",
    "    chunk_text = doc.page_content.strip()\n",
    "\n",
    "    doc.metadata.update({\n",
    "        \"uuid\": uid,\n",
    "        \"page\": page,\n",
    "        \"source\": \"class10phy.pdf\"\n",
    "    })\n",
    "\n",
    "    metadata_json.append({\n",
    "        \"uuid\": uid,\n",
    "        \"page\": page,\n",
    "        \"chunk_id\": f\"page_{page}_chunk_{i}\",\n",
    "        \"text\": chunk_text,\n",
    "        \"source\": \"class10phy.pdf\"\n",
    "    })\n",
    "\n",
    "with open(\"chunk_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metadata_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# --- Save to FAISS vector store ---\n",
    "vector_store = FAISS.from_documents(split_docs, embedding=embedding_model)\n",
    "vector_store.save_local(folder_path='/content', index_name='class_10_phy')\n",
    "\n",
    "# --- Load vector store and metadata ---\n",
    "vector_store = FAISS.load_local(\n",
    "    folder_path='/content',\n",
    "    index_name='class_10_phy',\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "with open(\"chunk_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunk_metadata = json.load(f)\n",
    "metadata_lookup = {entry[\"text\"]: entry for entry in chunk_metadata}\n",
    "\n",
    "# --- Query and run through LLM ---\n",
    "query = \"what are the 2 laws of reflection?\"\n",
    "docs: list[Document] = retriever.invoke(query)\n",
    "relevant_chunks = [doc.page_content for doc in docs]\n",
    "\n",
    "prompt = \"\"\"You are given the query from the user, along with the content extracted from the vector embeddings related to the query.\n",
    "Using this, give the answer to the query.\n",
    "These are questions related to a physics chapter of class 10.\n",
    "Give the response in a complete sentence based on the sentence from the context provided.\n",
    "User Query: {query}\n",
    "Relevant Content: {relevant_docs}\n",
    "If the given content doesn't include the answer to the query, respond with 'I don't know'.\"\"\"\n",
    "\n",
    "new_prompt = ChatPromptTemplate.from_template(prompt)\n",
    "chain = new_prompt | llm\n",
    "response = chain.invoke({\"query\": query, \"relevant_docs\": relevant_chunks})\n",
    "\n",
    "# --- Print output ---\n",
    "print(\"✅ Answer:\", response.content)\n",
    "print(\"\\n🔍 Chunks used:\\n\" + \"-\" * 60)\n",
    "\n",
    "for chunk_text in relevant_chunks:\n",
    "    match = metadata_lookup.get(chunk_text)\n",
    "    if match:\n",
    "        print(f\"🆔 Chunk ID: {match['chunk_id']}\")\n",
    "        print(f\"📄 Page: {match['page']}\")\n",
    "        print(f\"📄 Text: {match['text']}\\n{'-' * 60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43691e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.15.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<8.2,>=8.0.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "UsageError: Line magic function `%python` not found (But cell magic `%%python` exists, did you mean that instead?).\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "%python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 1️⃣ Load PDF pages\n",
    "docs = PyMuPDFLoader(\"class10phy.pdf\").load()\n",
    "\n",
    "# 2️⃣ Split into sentence-aware chunks via SpacyTextSplitter\n",
    "text_splitter = SpacyTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3️⃣ Add UUIDs + metadata\n",
    "sentence_metadata = []\n",
    "uuid_lookup = {}\n",
    "processed_docs = []\n",
    "\n",
    "for doc in split_docs:\n",
    "    uid = str(uuid.uuid4())\n",
    "    page = doc.metadata.get(\"page\", 0) + 1\n",
    "    metadata = {**doc.metadata, \"uuid\": uid, \"page\": page, \"source\": \"class10phy.pdf\"}\n",
    "\n",
    "    processed_docs.append(Document(page_content=doc.page_content, metadata=metadata))\n",
    "\n",
    "    entry = {\n",
    "        \"uuid\": uid,\n",
    "        \"page\": page,\n",
    "        \"text\": doc.page_content,\n",
    "        \"source\": \"class10phy.pdf\"\n",
    "    }\n",
    "\n",
    "    sentence_metadata.append(entry)\n",
    "    uuid_lookup[uid] = entry\n",
    "\n",
    "# 🔒 Save metadata\n",
    "with open(\"sentence_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sentence_metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(\"uuid_lookup.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(uuid_lookup, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# 4️⃣ Create and reload FAISS vector store\n",
    "vector_store = FAISS.from_documents(processed_docs, embedding=embedding_model)\n",
    "vector_store.save_local(folder_path=\"vector_index\", index_name=\"class_10_phy\")\n",
    "\n",
    "vector_store = FAISS.load_local(\n",
    "    folder_path=\"vector_index\",\n",
    "    index_name=\"class_10_phy\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# 5️⃣ Run query\n",
    "query = \"what are the two laws of reflection?\"\n",
    "retrieved = retriever.invoke(query)\n",
    "relevant = [doc.page_content for doc in retrieved]\n",
    "\n",
    "# 6️⃣ Prompt LLM for answer\n",
    "prompt = \"\"\"You are given the query from the user, along with the content extracted from the vector embeddings related to the query.\n",
    "Using this, give the answer to the query.\n",
    "These are questions related to a physics chapter of class 10.\n",
    "Give the response in a complete sentence based on the sentence from the context provided.\n",
    "User Query: {query}\n",
    "Relevant Content: {relevant_docs}\n",
    "If the given content doesn't include the answer to the query, respond with 'I don't know'.\"\"\"\n",
    "\n",
    "# 7️⃣ Load lookup and print results\n",
    "print(\"✅ Answer:\", response.content)\n",
    "print(\"\\n🔍 Supporting Sentences:\\n\" + \"-\" * 60)\n",
    "\n",
    "with open(\"uuid_lookup.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    uuid_lookup = json.load(f)\n",
    "\n",
    "for doc in retrieved:\n",
    "    uid = doc.metadata.get(\"uuid\")\n",
    "    entry = uuid_lookup.get(uid)\n",
    "    if entry:\n",
    "        print(f\"🆔 {uid} | 📄 Page: {entry['page']}\")\n",
    "        print(entry[\"text\"] + \"\\n\" + \"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a254c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27871c9a",
   "metadata": {},
   "source": [
    "# 2nd LLM trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2ebb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 1️⃣ Load PDF pages\n",
    "docs = PyMuPDFLoader(\"class10phy.pdf\").load()\n",
    "# 2️⃣ Split into sentence-aware chunks via SpacyTextSplitter\n",
    "text_splitter = SpacyTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3️⃣ Add UUIDs + metadata\n",
    "sentence_metadata = []\n",
    "uuid_lookup = {}\n",
    "processed_docs = []\n",
    "\n",
    "for doc in split_docs:\n",
    "    uid = str(uuid.uuid4())\n",
    "    page = doc.metadata.get(\"page\", 0) + 1\n",
    "    metadata = {**doc.metadata, \"uuid\": uid, \"page\": page, \"source\": \"class10phy.pdf\"}\n",
    "\n",
    "    processed_docs.append(Document(page_content=doc.page_content, metadata=metadata))\n",
    "\n",
    "    entry = {\n",
    "        \"uuid\": uid,\n",
    "        \"page\": page,\n",
    "        \"text\": doc.page_content,\n",
    "        \"source\": \"class10phy.pdf\"\n",
    "    }\n",
    "\n",
    "    sentence_metadata.append(entry)\n",
    "    uuid_lookup[uid] = entry\n",
    "\n",
    "# 🔒 Save metadata\n",
    "with open(\"sentence_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sentence_metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(\"uuid_lookup.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(uuid_lookup, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# 4️⃣ Create and reload FAISS vector store\n",
    "vector_store = FAISS.from_documents(processed_docs, embedding=embedding_model)\n",
    "vector_store.save_local(folder_path=\"vector_index\", index_name=\"class_10_phy\")\n",
    "\n",
    "vector_store = FAISS.load_local(\n",
    "    folder_path=\"vector_index\",\n",
    "    index_name=\"class_10_phy\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# 5️⃣ Run query\n",
    "query = \"what are the two laws of reflection?\"\n",
    "retrieved = retriever.invoke(query)\n",
    "relevant = [doc.page_content for doc in retrieved]\n",
    "\n",
    "# 6️⃣ Prompt LLM for answer\n",
    "prompt = \"\"\"You are given the query from the user, along with the content extracted from the vector embeddings related to the query.\n",
    "Using this, give the answer to the query.\n",
    "These are questions related to a physics chapter of class 10.\n",
    "Give the response in a complete sentence based on the sentence from the context provided.\n",
    "User Query: {query}\n",
    "Relevant Content: {relevant_docs}\n",
    "If the given content doesn't include the answer to the query, respond with 'I don't know'.\"\"\"\n",
    "qa_chain = ChatPromptTemplate.from_template(prompt) | llm\n",
    "response = qa_chain.invoke({\"query\": query, \"relevant_docs\": relevant})\n",
    "\n",
    "# 7️⃣ Load lookup and print results\n",
    "print(\"✅ Answer:\", response.content)\n",
    "print(\"\\n🔍 Supporting Chunks:\\n\" + \"-\" * 60)\n",
    "\n",
    "with open(\"uuid_lookup.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    uuid_lookup = json.load(f)\n",
    "\n",
    "for doc in retrieved:\n",
    "    uid = doc.metadata.get(\"uuid\")\n",
    "    entry = uuid_lookup.get(uid)\n",
    "    if entry:\n",
    "        print(f\"🆔 {uid} | 📄 Page: {entry['page']}\")\n",
    "        print(entry[\"text\"] + \"\\n\" + \"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2257fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import spacy\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 🧠 Load spaCy for sentence splitting\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 1️⃣ Break retrieved chunks into individual sentences with sentence-level UUIDs\n",
    "sentence_candidates = []\n",
    "sentence_lookup = {}\n",
    "\n",
    "for doc in retrieved:\n",
    "    chunk_uuid = doc.metadata[\"uuid\"]\n",
    "    page = doc.metadata[\"page\"]\n",
    "    source = doc.metadata[\"source\"]\n",
    "    chunk_text = doc.page_content\n",
    "\n",
    "    for sent in nlp(chunk_text).sents:\n",
    "        sentence_text = sent.text.strip()\n",
    "        if not sentence_text:\n",
    "            continue\n",
    "\n",
    "        sentence_uuid = str(uuid.uuid4())\n",
    "\n",
    "        sentence_candidates.append({\n",
    "            \"uuid\": sentence_uuid,\n",
    "            \"sentence\": sentence_text\n",
    "        })\n",
    "\n",
    "        sentence_lookup[sentence_uuid] = {\n",
    "            \"sentence\": sentence_text,\n",
    "            \"page\": page,\n",
    "            \"chunk_uuid\": chunk_uuid,\n",
    "            \"source\": source\n",
    "        }\n",
    "\n",
    "# 🔒 Save sentence-level metadata for lookup\n",
    "with open(\"sentence_lookup.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sentence_lookup, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# 2️⃣ Format prompt for second LLM pass\n",
    "formatted_sentences = \"\\n\".join(\n",
    "    f\"- [uuid: {s['uuid']}] {s['sentence']}\" for s in sentence_candidates\n",
    ")\n",
    "\n",
    "# 🖨️ Debug: Print everything we're passing to the second LLM\n",
    "\n",
    "print(\"\\n🔹 User Query:\")\n",
    "print(query)\n",
    "\n",
    "print(\"\\n🔹 Answer from First LLM:\")\n",
    "print(response.content)\n",
    "\n",
    "print(\"\\n🔹 Candidate Sentences:\")\n",
    "for s in sentence_candidates:\n",
    "    print(f\"[uuid: {s['uuid']}] {s['sentence']}\")\n",
    "\n",
    "\n",
    "justification_prompt = \"\"\"\n",
    "You are helping a student understand a textbook answer.\n",
    "\n",
    "Below is a user question and the answer given by an AI.\n",
    "\n",
    "You are also given a list of textbook sentences, each with a unique UUID.\n",
    "\n",
    "Your task is to return only the UUIDs of the sentences that support the answer.\n",
    "\n",
    "⚠️ Respond ONLY with a JSON list like: [\"uuid1\", \"uuid2\"]. No explanation. No markdown.\n",
    "\n",
    "---\n",
    "\n",
    "Question: {query}\n",
    "Answer: {answer}\n",
    "\n",
    "Candidate Sentences:\n",
    "{sentences}\n",
    "\"\"\"\n",
    "\n",
    "justification_chain = ChatPromptTemplate.from_template(justification_prompt) | llm\n",
    "\n",
    "justification_response = justification_chain.invoke({\n",
    "    \"query\": query,\n",
    "    \"answer\": response.content,\n",
    "    \"sentences\": formatted_sentences\n",
    "})\n",
    "\n",
    "# 3️⃣ Parse returned UUIDs safely\n",
    "raw_output = justification_response.content.strip()\n",
    "\n",
    "# Clean code block if needed\n",
    "if raw_output.startswith(\"```\"):\n",
    "    raw_output = re.sub(r\"^```(?:json)?\\n?\", \"\", raw_output)\n",
    "    raw_output = re.sub(r\"\\n?```$\", \"\", raw_output)\n",
    "\n",
    "try:\n",
    "    returned_uuids = ast.literal_eval(raw_output)\n",
    "    if not isinstance(returned_uuids, list):\n",
    "        raise ValueError(\"Output is not a list\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Could not parse LLM output:\", raw_output)\n",
    "    returned_uuids = []\n",
    "\n",
    "# 4️⃣ Lookup and print only the supporting sentences\n",
    "print(\"📚 Sentences to Highlight:\\n\" + \"=\" * 50)\n",
    "for uid in returned_uuids:\n",
    "    entry = sentence_lookup.get(uid)\n",
    "    if entry:\n",
    "        print(f\"📄 Page: {entry['page']}\")\n",
    "        print(f\"📝 {entry['sentence']}\")\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe57f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syntok import segmenter\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 1️⃣ Sentence splitting using syntok\n",
    "sentence_candidates = []\n",
    "sentence_lookup = {}\n",
    "sentence_index = 1\n",
    "\n",
    "for doc in retrieved:\n",
    "    chunk_uuid = doc.metadata[\"uuid\"]\n",
    "    page = doc.metadata[\"page\"]\n",
    "    source = doc.metadata[\"source\"]\n",
    "    chunk_text = doc.page_content\n",
    "\n",
    "    # Process chunk with syntok\n",
    "    for paragraph in segmenter.process(chunk_text):\n",
    "        for sentence in paragraph:\n",
    "            sentence_text = \" \".join(token.value for token in sentence).strip()\n",
    "            if not sentence_text:\n",
    "                continue\n",
    "\n",
    "            sentence_candidates.append({\n",
    "                \"index\": sentence_index,\n",
    "                \"sentence\": sentence_text,\n",
    "                \"page\": page,\n",
    "                \"chunk_uuid\": chunk_uuid,\n",
    "                \"source\": source\n",
    "            })\n",
    "\n",
    "            sentence_lookup[sentence_index] = {\n",
    "                \"sentence\": sentence_text,\n",
    "                \"page\": page,\n",
    "                \"chunk_uuid\": chunk_uuid,\n",
    "                \"source\": source\n",
    "            }\n",
    "\n",
    "            sentence_index += 1\n",
    "\n",
    "# 🔒 Save for later use\n",
    "with open(\"sentence_lookup.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sentence_lookup, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# 🖨️ Print for debugging\n",
    "print(\"\\n🔹 User Query:\")\n",
    "print(query)\n",
    "print(\"\\n🔹 Answer from First LLM:\")\n",
    "print(response.content)\n",
    "print(\"\\n🔹 Candidate Sentences:\")\n",
    "for s in sentence_candidates:\n",
    "    print(f\"[#{s['index']}] {s['sentence']}\")\n",
    "\n",
    "# 2️⃣ Prepare LLM prompt\n",
    "formatted_sentences = \"\\n\".join(\n",
    "    f\"- [#{s['index']}] {s['sentence']}\" for s in sentence_candidates\n",
    ")\n",
    "\n",
    "justification_prompt = \"\"\"\n",
    "You are helping a student understand a textbook answer.\n",
    "\n",
    "Below is a user question and the AI's answer.\n",
    "\n",
    "You are also given a list of textbook sentences, each numbered like [#1], [#2], etc.\n",
    "\n",
    "Return only the sentence numbers that support the answer.\n",
    "\n",
    "⚠️ Respond ONLY with a JSON list of numbers. Example: [1, 4, 7]\n",
    "\n",
    "---\n",
    "\n",
    "Question: {query}\n",
    "Answer: {answer}\n",
    "\n",
    "Candidate Sentences:\n",
    "{sentences}\n",
    "\"\"\"\n",
    "\n",
    "# 3️⃣ Invoke LLM\n",
    "justification_chain = ChatPromptTemplate.from_template(justification_prompt) | llm\n",
    "justification_response = justification_chain.invoke({\n",
    "    \"query\": query,\n",
    "    \"answer\": response.content,\n",
    "    \"sentences\": formatted_sentences\n",
    "})\n",
    "\n",
    "# 4️⃣ Parse the output\n",
    "raw_output = justification_response.content.strip()\n",
    "if raw_output.startswith(\"```\"):\n",
    "    raw_output = re.sub(r\"^```(?:json)?\\n?\", \"\", raw_output)\n",
    "    raw_output = re.sub(r\"\\n?```$\", \"\", raw_output)\n",
    "\n",
    "try:\n",
    "    sentence_numbers = ast.literal_eval(raw_output)\n",
    "    if not isinstance(sentence_numbers, list):\n",
    "        raise ValueError(\"Output is not a list\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Could not parse LLM output:\", raw_output)\n",
    "    sentence_numbers = []\n",
    "\n",
    "# 5️⃣ Show final highlighted sentences\n",
    "print(\"\\n📚 Sentences to Highlight:\\n\" + \"=\" * 50)\n",
    "for idx in sentence_numbers:\n",
    "    entry = sentence_lookup.get(idx)\n",
    "    if entry:\n",
    "        print(f\"📄 Page: {entry['page']}\")\n",
    "        print(f\"📝 {entry['sentence']}\")\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ebf78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: syntok in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: regex>2016 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from syntok) (2024.11.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install syntok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af0ce30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e1943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     -- ------------------------------------- 0.8/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     -- ------------------------------------- 0.8/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     --- ------------------------------------ 1.0/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 1.2 MB/s eta 0:00:10\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 1.2 MB/s eta 0:00:10\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.2 MB/s eta 0:00:10\n",
      "     ------ --------------------------------- 2.1/12.8 MB 1.2 MB/s eta 0:00:10\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.2 MB/s eta 0:00:09\n",
      "     -------- ------------------------------- 2.6/12.8 MB 1.2 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 2.9/12.8 MB 1.2 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.2 MB/s eta 0:00:09\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ------------ --------------------------- 3.9/12.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ------------- -------------------------- 4.2/12.8 MB 1.2 MB/s eta 0:00:08\n",
      "     ------------- -------------------------- 4.5/12.8 MB 1.2 MB/s eta 0:00:08\n",
      "     -------------- ------------------------- 4.7/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     -------------- ------------------------- 4.7/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     --------------- ------------------------ 5.0/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 1.2 MB/s eta 0:00:07\n",
      "     ------------------ --------------------- 5.8/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     ------------------ --------------------- 6.0/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 6.3/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 6.6/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 6.8/12.8 MB 1.2 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "     ------------------------ --------------- 7.9/12.8 MB 1.2 MB/s eta 0:00:05\n",
      "     ------------------------- -------------- 8.1/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 8.1/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 8.4/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 8.7/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 8.9/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 1.2 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 9.7/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 10.0/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 10.0/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 10.2/12.8 MB 1.2 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 10.5/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 10.7/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 11.3/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 11.5/12.8 MB 1.2 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 11.8/12.8 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 1.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 1.2 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e919d6",
   "metadata": {},
   "source": [
    "# trial 3 (Spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e4ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ashish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Answer: The sign conventions for reflection by spherical mirrors, according to the New Cartesian Sign Convention, are as follows:\n",
      "\n",
      "1.  The object is always placed to the left of the mirror, implying that light from the object falls on the mirror from the left-hand side.\n",
      "2.  All distances are measured from the pole (P) of the mirror. The pole is taken as the origin.\n",
      "3.  All distances parallel to the principal axis are measured from the pole of the mirror. The principal axis of the mirror is taken as the x-axis of the coordinate system.\n",
      "4.  Distances measured to the right of the origin (along + x-axis) are taken as positive, while those measured to the left of the origin (along – x-axis) are taken as negative.\n",
      "5.  Distances measured perpendicular to and above the principal axis (along + y-axis) are taken as positive.\n",
      "6.  Distances measured perpendicular to and below the principal axis (along – y-axis) are taken as negative.\n",
      "\n",
      "Additionally:\n",
      "\n",
      "*   The height of the object is taken to be positive as the object is usually placed above the principal axis.\n",
      "*   The height of the image should be taken as positive for virtual images and negative for real images.\n",
      "*   A negative sign in the value of the magnification indicates that the image is real. A positive sign in the value of the magnification indicates that the image is virtual.\n",
      "\n",
      "🔍 Supporting Chunks:\n",
      "------------------------------------------------------------\n",
      "🆔 7cbabb87-5565-413e-852c-52ce2a7eb080 | 📄 Page: 10\n",
      "Light – Reflection and Refraction\n",
      "143\n",
      "The New Cartesian Sign Convention described above is illustrated in\n",
      "Fig.9.9 for your reference.\n",
      "\n",
      "These sign conventions are applied to obtain\n",
      "the mirror formula and solve related numerical problems.\n",
      "\n",
      "\n",
      "9.2.4  Mirror Formula and  Magnification\n",
      "In a spherical mirror, the distance of the\n",
      "object from its pole is called the object\n",
      "distance (u).  \n",
      "\n",
      "The distance of the image from\n",
      "the pole of the mirror is called the image\n",
      "distance (v).\n",
      "\n",
      "You already know that the\n",
      "distance of the principal focus from the pole\n",
      "is called the focal length (f).\n",
      "\n",
      "There is a\n",
      "relationship between these three quantities\n",
      "given by the mirror formula which is\n",
      "expressed as\n",
      "1\n",
      "1\n",
      "1\n",
      "v\n",
      "u\n",
      "f\n",
      "+\n",
      "=\n",
      "(9.1)\n",
      "This formula is valid in all situations for all\n",
      "spherical mirrors for all positions of the\n",
      "object.\n",
      "\n",
      "You must use the New Cartesian Sign\n",
      "Convention while substituting numerical\n",
      "values for u, v, f, and R in the mirror formula\n",
      "for solving problems.\n",
      "\n",
      "\n",
      "Magnification\n",
      "Magnification produced by a spherical mirror gives the relative extent to\n",
      "which the image of an object is magnified with respect to the object size.\n",
      "\n",
      "\n",
      "It is expressed as the ratio of the height of the image to the height of the\n",
      "object.\n",
      "\n",
      "It is usually represented by the letter m.\n",
      "\n",
      "\n",
      "If h is the height of the object and h′ is the height of the image, then\n",
      "the magnification m produced by a spherical mirror is given by\n",
      "m = \n",
      "Height of the image (\n",
      ")\n",
      "\n",
      "\n",
      "Height of the object ( )\n",
      "′\n",
      "h\n",
      "h\n",
      "m = \n",
      "′\n",
      "h\n",
      "h\n",
      "(9.2)\n",
      "------------------------------------------------------------\n",
      "🆔 8d06a137-45db-45a0-ac87-6e7c244130a4 | 📄 Page: 9\n",
      "Q\n",
      "U\n",
      "E\n",
      "S\n",
      "T\n",
      "I\n",
      "O\n",
      "N\n",
      "S\n",
      "?\n",
      "1.\n",
      "\n",
      "\n",
      "Define the principal focus of a concave mirror.\n",
      "2.\n",
      "\n",
      "\n",
      "The radius of curvature of a spherical mirror is 20 cm.  \n",
      "\n",
      "What is its focal\n",
      "length?\n",
      "3.\n",
      "Name a mirror that can give an erect and enlarged image of an object.\n",
      "\n",
      "\n",
      "4.\n",
      "\n",
      "\n",
      "Why do we prefer a convex mirror as a rear-view mirror in vehicles?\n",
      "\n",
      "\n",
      "9.2.3 Sign Convention for Reflection by Spherical Mirrors\n",
      "\n",
      "\n",
      "While dealing with the reflection of light by spherical mirrors, we shall\n",
      "follow a set of sign conventions called the New Cartesian Sign\n",
      "Convention.\n",
      "\n",
      "In this convention, the pole (P) of the mirror is taken as the\n",
      "origin (Fig. 9.9).  \n",
      "\n",
      "The principal axis of the mirror is taken as the x-axis\n",
      "(X’X) of the coordinate system.\n",
      "\n",
      "The conventions are as follows –\n",
      "(i)\n",
      "The object is always placed to the left of the mirror.\n",
      "\n",
      "This implies\n",
      "that the light from the object falls on the mirror from the left-hand\n",
      "side.\n",
      "\n",
      "\n",
      "(ii)\n",
      "All distances parallel to the principal axis are measured from the\n",
      "pole of the mirror.\n",
      "\n",
      "\n",
      "(iii)\n",
      "All the distances measured to the right of the origin (along\n",
      "+ x-axis) are taken as positive while those measured to the left of\n",
      "the origin (along – x-axis) are taken as negative.\n",
      "\n",
      "\n",
      "(iv)\n",
      "Distances measured perpendicular to and above the principal axis\n",
      "(along + y-axis) are taken as positive.\n",
      "(v)\n",
      "\n",
      "\n",
      "Distances measured perpendicular to and below the principal axis\n",
      "(along –y-axis) are taken as negative.\n",
      "\n",
      "\n",
      "Reprint 2025-26\n",
      "------------------------------------------------------------\n",
      "🆔 693edac4-4ac5-4303-832f-8ed1a5e5eaf0 | 📄 Page: 10\n",
      "It is expressed as the ratio of the height of the image to the height of the\n",
      "object.\n",
      "\n",
      "It is usually represented by the letter m.\n",
      "\n",
      "\n",
      "If h is the height of the object and h′ is the height of the image, then\n",
      "the magnification m produced by a spherical mirror is given by\n",
      "m = \n",
      "Height of the image (\n",
      ")\n",
      "\n",
      "\n",
      "Height of the object ( )\n",
      "′\n",
      "h\n",
      "h\n",
      "m = \n",
      "′\n",
      "h\n",
      "h\n",
      "(9.2)\n",
      "\n",
      "\n",
      "The magnification m is also related to the object distance (u) and\n",
      "image distance (v).  \n",
      "\n",
      "It can be expressed as:\n",
      "Magnification (m)  = \n",
      "′ = −\n",
      "h\n",
      "h\n",
      "v\n",
      "u\n",
      "(9.3)\n",
      "\n",
      "\n",
      "You may note that the height of the object is taken to be positive as\n",
      "the object is usually placed above the principal axis.  \n",
      "\n",
      "The height of the\n",
      "image should be taken as positive for virtual images.  \n",
      "\n",
      "However, it is to be\n",
      "taken as negative for real images.\n",
      "\n",
      "A negative sign in the value of the\n",
      "magnification indicates that the image is real.\n",
      "\n",
      "A positive sign in the value\n",
      "of the magnification indicates that the image is virtual.\n",
      "\n",
      "\n",
      "Figure 9.9\n",
      "Figure 9.9\n",
      "Figure 9.9\n",
      "Figure 9.9\n",
      "Figure 9.9\n",
      "The New Cartesian Sign Convention for spherical mirrors\n",
      "Reprint 2025-26\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import json\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 1️⃣ Load PDF pages\n",
    "docs = PyMuPDFLoader(\"class10phy.pdf\").load()\n",
    "\n",
    "# 2️⃣ Split into overlapping chunks (not sentence-level)\n",
    "text_splitter = SpacyTextSplitter(chunk_size=1500, chunk_overlap=375)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3️⃣ Add UUIDs + metadata to chunks\n",
    "chunk_metadata = []\n",
    "uuid_lookup = {}\n",
    "processed_docs = []\n",
    "\n",
    "for doc in split_docs:\n",
    "    uid = str(uuid.uuid4())\n",
    "    page = doc.metadata.get(\"page\", 0) + 1\n",
    "    metadata = {**doc.metadata, \"uuid\": uid, \"page\": page, \"source\": \"class10phy.pdf\"}\n",
    "\n",
    "    processed_docs.append(Document(page_content=doc.page_content, metadata=metadata))\n",
    "\n",
    "    entry = {\n",
    "        \"uuid\": uid,\n",
    "        \"page\": page,\n",
    "        \"text\": doc.page_content,\n",
    "        \"source\": \"class10phy.pdf\"\n",
    "    }\n",
    "\n",
    "    chunk_metadata.append(entry)\n",
    "    uuid_lookup[uid] = entry\n",
    "\n",
    "# 🔒 Save chunk metadata\n",
    "with open(\"sentence_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunk_metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(\"uuid_lookup.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(uuid_lookup, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# 4️⃣ Create and reload FAISS vector store\n",
    "vector_store = FAISS.from_documents(processed_docs, embedding=embedding_model)\n",
    "vector_store.save_local(folder_path=\"vector_index\", index_name=\"class_10_phy\")\n",
    "\n",
    "vector_store = FAISS.load_local(\n",
    "    folder_path=\"vector_index\",\n",
    "    index_name=\"class_10_phy\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# 5️⃣ Run query\n",
    "query = \"what are the different sign convertions for reflection in spherical mirrors?\"\n",
    "retrieved = retriever.invoke(query)\n",
    "relevant = [doc.page_content for doc in retrieved]\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are helping a Class 10 Physics student answer a textbook-based question.\n",
    "\n",
    "You are given a set of extracted text chunks from the textbook that may contain the answer.\n",
    "\n",
    "Your task is to write a complete and accurate answer to the question using **only the information provided** in these chunks.\n",
    "\n",
    "If the answer depends on specific definitions, formulas, rules, steps, or listed conventions — make sure to **include all of them** clearly and accurately.\n",
    "\n",
    "You may also include any additional relevant points found in the provided content, **as long as you do not skip or miss any part that is essential to correctly answering the question**.\n",
    "\n",
    "Do not invent or assume anything beyond what is given. If the content does not contain enough information to answer the question, respond only with: \"I don't know\".\n",
    "\n",
    "---\n",
    "\n",
    "📘 Question:\n",
    "{query}\n",
    "\n",
    "📚 Extracted Text:\n",
    "{relevant_docs}\n",
    "\"\"\"\n",
    "qa_chain = ChatPromptTemplate.from_template(prompt) | llm\n",
    "response = qa_chain.invoke({\"query\": query, \"relevant_docs\": relevant})\n",
    "\n",
    "# 7️⃣ Print output\n",
    "print(\"✅ Answer:\", response.content)\n",
    "print(\"\\n🔍 Supporting Chunks:\\n\" + \"-\" * 60)\n",
    "\n",
    "for doc in retrieved:\n",
    "    uid = doc.metadata.get(\"uuid\")\n",
    "    entry = uuid_lookup.get(uid)\n",
    "    if entry:\n",
    "        print(f\"🆔 {uid} | 📄 Page: {entry['page']}\")\n",
    "        print(entry[\"text\"] + \"\\n\" + \"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import json\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain.output_parsers import PydanticOutputParser\n",
    "# from pydantic import BaseModel\n",
    "# from typing import List\n",
    "\n",
    "# # 🧠 Load spaCy for sentence splitting\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # 1️⃣ Break retrieved chunks into individual sentences with metadata\n",
    "# sentence_candidates = []\n",
    "# sentence_lookup = {}\n",
    "# sid_counter = 1\n",
    "\n",
    "# for doc in retrieved:\n",
    "#     chunk_uuid = doc.metadata[\"uuid\"]\n",
    "#     page = doc.metadata[\"page\"]\n",
    "#     source = doc.metadata[\"source\"]\n",
    "#     chunk_text = doc.page_content\n",
    "\n",
    "#     for sent in nlp(chunk_text).sents:\n",
    "#         sentence_text = sent.text.strip()\n",
    "#         if not sentence_text:\n",
    "#             continue\n",
    "\n",
    "#         sid = f\"s{sid_counter}\"\n",
    "#         sid_counter += 1\n",
    "\n",
    "#         sentence_entry = {\n",
    "#             \"sid\": sid,\n",
    "#             \"sentence\": sentence_text,\n",
    "#             \"page\": page,\n",
    "#             \"chunk_uuid\": chunk_uuid,\n",
    "#             \"source\": source\n",
    "#         }\n",
    "\n",
    "#         sentence_candidates.append({\n",
    "#             \"sid\": sid,\n",
    "#             \"sentence\": sentence_text\n",
    "#         })\n",
    "\n",
    "#         sentence_lookup[sid] = sentence_entry\n",
    "\n",
    "# # 🔒 Save sentence-level metadata\n",
    "# with open(\"sentence_lookup.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(sentence_lookup, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# # 2️⃣ Format sentences for the LLM\n",
    "# formatted_sentences = \"\\n\".join(\n",
    "#     f\"- [sid: {s['sid']}] {s['sentence']}\" for s in sentence_candidates\n",
    "# )\n",
    "\n",
    "# # 3️⃣ Define Pydantic schema\n",
    "# class SupportIDs(BaseModel):\n",
    "#     ids: List[str]\n",
    "\n",
    "# # 4️⃣ Create output parser\n",
    "# parser = PydanticOutputParser(pydantic_object=SupportIDs)\n",
    "# format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# # 5️⃣ Define prompt with format instructions\n",
    "# prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "# You are helping a student understand a textbook answer.\n",
    "\n",
    "# You are given:\n",
    "# - A user question\n",
    "# - The AI-generated answer to that question\n",
    "# - A list of textbook sentences, each with a short ID\n",
    "\n",
    "# Your task is to return only the sentence IDs that support the answer.\n",
    "\n",
    "# {format_instructions}\n",
    "\n",
    "# ---\n",
    "\n",
    "# Question: {query}\n",
    "# Answer: {answer}\n",
    "\n",
    "# Candidate Sentences:\n",
    "# {sentences}\n",
    "# \"\"\")\n",
    "\n",
    "# # 6️⃣ Run the structured chain\n",
    "# structured_chain = prompt | llm | parser\n",
    "\n",
    "# result = structured_chain.invoke({\n",
    "#     \"query\": query,\n",
    "#     \"answer\": response.content,\n",
    "#     \"sentences\": formatted_sentences,\n",
    "#     \"format_instructions\": format_instructions\n",
    "# })\n",
    "\n",
    "# returned_sids = result.ids\n",
    "\n",
    "# # ✅ Final Output\n",
    "# print(\"\\n✅ Supporting Sentence IDs:\\n\" + \"=\" * 50)\n",
    "# print(returned_sids)\n",
    "\n",
    "# print(\"\\n📝 Sentences to Highlight:\\n\" + \"=\" * 50)\n",
    "# for sid in returned_sids:\n",
    "#     entry = sentence_lookup.get(sid)\n",
    "#     if entry:\n",
    "#         print(f\"{sid} (Page {entry['page']}): {entry['sentence']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ec340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Supporting Sentence IDs:\n",
      "==================================================\n",
      "['s1', 's19', 's20', 's23', 's24', 's25', 's26', 's27', 's28', 's36', 's37', 's38', 's39', 's40', 's41']\n",
      "\n",
      "📝 Sentences to Highlight:\n",
      "==================================================\n",
      "s1 (Page 10): Light – Reflection and Refraction\n",
      "143\n",
      "The New Cartesian Sign Convention described above is illustrated in\n",
      "Fig.9.9 for your reference.\n",
      "s19 (Page 9): 9.2.3 Sign Convention for Reflection by Spherical Mirrors\n",
      "\n",
      "\n",
      "While dealing with the reflection of light by spherical mirrors, we shall\n",
      "follow a set of sign conventions called the New Cartesian Sign\n",
      "Convention.\n",
      "s20 (Page 9): In this convention, the pole (P) of the mirror is taken as the\n",
      "origin (Fig. 9.9).\n",
      "s23 (Page 9): The object is always placed to the left of the mirror.\n",
      "s24 (Page 9): This implies\n",
      "that the light from the object falls on the mirror from the left-hand\n",
      "side.\n",
      "s25 (Page 9): (ii)\n",
      "All distances parallel to the principal axis are measured from the\n",
      "pole of the mirror.\n",
      "s26 (Page 9): (iii)\n",
      "All the distances measured to the right of the origin (along\n",
      "+ x-axis) are taken as positive while those measured to the left of\n",
      "the origin (along – x-axis) are taken as negative.\n",
      "s27 (Page 9): (iv)\n",
      "Distances measured perpendicular to and above the principal axis\n",
      "(along + y-axis) are taken as positive.\n",
      "(v)\n",
      "s28 (Page 9): Distances measured perpendicular to and below the principal axis\n",
      "(along –y-axis) are taken as negative.\n",
      "s36 (Page 10): You may note that the height of the object is taken to be positive as\n",
      "the object is usually placed above the principal axis.\n",
      "s37 (Page 10): The height of the\n",
      "image should be taken as positive for virtual images.\n",
      "s38 (Page 10): However, it is to be\n",
      "taken as negative for real images.\n",
      "s39 (Page 10): A negative sign in the value of the\n",
      "magnification indicates that the image is real.\n",
      "s40 (Page 10): A positive sign in the value\n",
      "of the magnification indicates that the image is virtual.\n",
      "s41 (Page 10): Figure 9.9\n",
      "Figure 9.9\n",
      "Figure 9.9\n",
      "Figure 9.9\n",
      "Figure 9.9\n",
      "The New Cartesian Sign Convention for spherical mirrors\n",
      "Reprint 2025-26\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from math import ceil\n",
    "\n",
    "# 🧠 Load spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 1️⃣ Sentence segmentation\n",
    "sentence_candidates = []\n",
    "sentence_lookup = {}\n",
    "sid_counter = 1\n",
    "\n",
    "for doc in retrieved:\n",
    "    chunk_uuid = doc.metadata[\"uuid\"]\n",
    "    page = doc.metadata[\"page\"]\n",
    "    source = doc.metadata[\"source\"]\n",
    "    chunk_text = doc.page_content\n",
    "\n",
    "    for sent in nlp(chunk_text).sents:\n",
    "        sentence_text = sent.text.strip()\n",
    "        if not sentence_text:\n",
    "            continue\n",
    "\n",
    "        sid = f\"s{sid_counter}\"\n",
    "        sid_counter += 1\n",
    "\n",
    "        entry = {\n",
    "            \"sid\": sid,\n",
    "            \"sentence\": sentence_text,\n",
    "            \"page\": page,\n",
    "            \"chunk_uuid\": chunk_uuid,\n",
    "            \"source\": source\n",
    "        }\n",
    "\n",
    "        sentence_candidates.append({\"sid\": sid, \"sentence\": sentence_text})\n",
    "        sentence_lookup[sid] = entry\n",
    "\n",
    "# 🔒 Save sentence metadata\n",
    "with open(\"sentence_lookup.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sentence_lookup, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# 2️⃣ Schema\n",
    "class SupportIDs(BaseModel):\n",
    "    ids: List[str]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=SupportIDs)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# 3️⃣ Prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are helping a student understand a textbook answer.\n",
    "\n",
    "You are given:\n",
    "- A user question\n",
    "- The AI-generated answer to that question\n",
    "- A list of textbook sentences, each with a short ID\n",
    "\n",
    "Your task is to return only the sentence IDs that support the answer.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {query}\n",
    "Answer: {answer}\n",
    "\n",
    "Candidate Sentences:\n",
    "{sentences}\n",
    "\"\"\")\n",
    "\n",
    "# 4️⃣ Process in batches (to avoid context overload)\n",
    "batch_size = 20\n",
    "num_batches = ceil(len(sentence_candidates) / batch_size)\n",
    "all_ids = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch = sentence_candidates[i * batch_size:(i + 1) * batch_size]\n",
    "    formatted_sentences = \"\\n\".join(\n",
    "        f\"- [sid: {s['sid']}] {s['sentence']}\" for s in batch\n",
    "    )\n",
    "\n",
    "    structured_chain = prompt | llm | parser\n",
    "\n",
    "    try:\n",
    "        result = structured_chain.invoke({\n",
    "            \"query\": query,\n",
    "            \"answer\": response.content,\n",
    "            \"sentences\": formatted_sentences,\n",
    "            \"format_instructions\": format_instructions\n",
    "        })\n",
    "\n",
    "        # Normalize and collect IDs\n",
    "        cleaned = [sid.replace(\"sid:\", \"\").strip() for sid in result.ids]\n",
    "        all_ids.extend(cleaned)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Batch {i+1} failed: {e}\")\n",
    "        continue\n",
    "\n",
    "# ✅ Clean final IDs\n",
    "returned_sids = sorted(set(all_ids), key=lambda x: int(x[1:]))\n",
    "\n",
    "# ✅ Output\n",
    "print(\"\\n✅ Supporting Sentence IDs:\\n\" + \"=\" * 50)\n",
    "print(returned_sids)\n",
    "\n",
    "print(\"\\n📝 Sentences to Highlight:\\n\" + \"=\" * 50)\n",
    "for sid in returned_sids:\n",
    "    entry = sentence_lookup.get(sid)\n",
    "    if entry:\n",
    "        print(f\"{sid} (Page {entry['page']}): {entry['sentence']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73262b89",
   "metadata": {},
   "source": [
    "# Trial 3 (Syntok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d80fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: syntok in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: regex>2016 in c:\\users\\ashish\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from syntok) (2024.11.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install syntok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598347d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ashish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Answer: In Brazil, agriculture is the main occupation of people living in the highlands and coastal areas. Favorable climate and topography allow for growing a variety of crops. The main cereal crops are rice and maize, with maize production largely concentrated in the central part. Commercial crops like coffee, cocoa, rubber, soyabean, and sugarcane are cultivated on a large scale. Brazil is the largest exporter of coffee and soyabean in the world. The major states growing coffee are Minas Gerais and Sao Paulo. Besides these crops, fruits like bananas, pineapples, oranges, and other citrus fruits are also produced.\n",
      "\n",
      "🔍 Supporting Chunks:\n",
      "------------------------------------------------------------\n",
      "🆔 fb0a8fb9-b52f-40aa-ae61-589cf94426f9 | 📄 Page: 65\n",
      "54\n",
      "Geographical explanation\n",
      "Agriculture : In Brazil, agriculture is the main \n",
      "occupation of the people living in the highlands \n",
      "and coastal areas.\n",
      "\n",
      "Favourable climate and \n",
      "topography make it possible for growing a \n",
      "variety of crops.\n",
      "\n",
      "Rice and maize are the main \n",
      "cereal crops.\n",
      "\n",
      "Production of maize is largely \n",
      "concentrated in the central part.\n",
      "\n",
      "Commercial \n",
      "crops like coffee, cocoa, rubber, soyabean and \n",
      "sugarcane are cultivated on a large scale.\n",
      "\n",
      "Brazil \n",
      "is the largest exporter of coffee and soyabean \n",
      "in the world.\n",
      "\n",
      "The major states growing coffee \n",
      "are Minas Gerais and Sao Paulo.\n",
      "\n",
      "Besides \n",
      "these crops, production of fruits like bananas, \n",
      "Figure 8.3 \n",
      "pineapples, oranges and other citrus fruits \n",
      "is also done.\n",
      "\n",
      "Cattle, sheep and goats are also \n",
      "reared in the Savannah grasslands in the south. \n",
      "\n",
      "\n",
      "Consequently, meat and dairy products are \n",
      "produced on a large scale.\n",
      "\n",
      "\n",
      "Mining : \n",
      "Observe the fig 8.3 and answer the \n",
      "following questions.\n",
      "\n",
      "\n",
      "¾ Prepare a table of mining products and \n",
      "regions of production in Brazil.\n",
      "¾ In which part of Brazil has mining activity \n",
      "not developed?\n",
      "\n",
      "What could be the reasons?\n",
      "¾\n",
      "\n",
      "Considering the availability of resources, \n",
      "were has the development of industries \n",
      "occurred?\n",
      "\n",
      "\n",
      "Make friends with maps!\n",
      "------------------------------------------------------------\n",
      "🆔 7cbe6e0b-1dbf-4448-a482-a6ba40612bb2 | 📄 Page: 64\n",
      "Can you think of a \n",
      "reason for the same?\n",
      "\n",
      "\n",
      "Use the following table and make a \n",
      "polyline graph with the help of computer.\n",
      "\n",
      "\n",
      "Figure 8.2\n",
      "Brazil\n",
      "5.5\n",
      "27.5\n",
      "67\n",
      "10\n",
      "19\n",
      "71\n",
      "26.9\n",
      "24.3\n",
      "48.8\n",
      "17\n",
      "26\n",
      "57\n",
      "India\n",
      "Tertiary\n",
      "Primary\n",
      "Secondary\n",
      "Look at the map given in Fig 8.3.\n",
      "\n",
      "The \n",
      "major primary occupations in Brazil are shown \n",
      "here.\n",
      "\n",
      "Discuss the following points and write \n",
      "your observations in the notebook.\n",
      "¾ In which part of Brazil is coffee mainly \n",
      "produced? \n",
      "¾\n",
      "\n",
      "Which food crops  are mainly grown in \n",
      "Brazil?\n",
      "¾\n",
      "\n",
      "Can you relate the production of these crops \n",
      "with the climate there? \n",
      "¾\n",
      "\n",
      "Where \n",
      "are \n",
      "the \n",
      "rubber \n",
      "plantations \n",
      "concentrated?\n",
      "¾\n",
      "\n",
      "Complete the table.\n",
      "\n",
      "\n",
      "Type of crops \n",
      "Crops \n",
      "Areas of production\n",
      "Food crops\n",
      "Cash Crops \n",
      "Fruits and Vegetables \n",
      "Contribution of sectors in GDP (2016)\n",
      "Percentage of population engaged in various sectors (2016)    \n",
      "\n",
      "\n",
      "Per Capita Income from 1960 to 2016 ( in US $)\n",
      "Country \n",
      "Name/Year \n",
      "1960\n",
      "1980\n",
      "2000\n",
      "2016\n",
      "Brazil\n",
      "240\n",
      "2010\n",
      "3060\n",
      "8840\n",
      "India\n",
      "90\n",
      "280\n",
      "450\n",
      "1680\n",
      "USA \n",
      "3250\n",
      "14230\n",
      "37470 56280\n",
      "The United States is a developed country. \n",
      "\n",
      "\n",
      "The population of this country is well educated. \n",
      "\n",
      "\n",
      "This country has the strength of many patents, \n",
      "modern technology and mechanical strength. \n",
      "\n",
      "\n",
      "This country is far ahead of Brazil and India in \n",
      "terms of national per capita income.\n",
      " \n",
      "\n",
      "India and Brazil are developing countries. \n",
      "\n",
      "\n",
      "These countries are progressing in the field \n",
      "of technological advancement, education and \n",
      "industry.\n",
      "\n",
      "\n",
      "The national per capita income of the \n",
      "countries is low.\n",
      "------------------------------------------------------------\n",
      "🆔 f7a0e3f3-9ec4-4f61-8004-219b71424f68 | 📄 Page: 67\n",
      "56\n",
      "Amazon River are not exploited much and \n",
      "fishing only takes place on a small scale.\n",
      "\n",
      "\n",
      "Agriculture in India :\n",
      "Try this.\n",
      "¾\n",
      "\n",
      "Show the distribution of crops like wheat, \n",
      "jowar, rice, cotton, sugarcane, tea and apple \n",
      "in the outline map of India using symbols. \n",
      "\n",
      "\n",
      "Name the map.\n",
      "\n",
      "\n",
      "Geographical explanation\n",
      "Unlike \n",
      "Brazil, \n",
      "India’s \n",
      "agriculture \n",
      "contributes more towards GDP and also \n",
      "engages a larger chunk of population.\n",
      "\n",
      "Around \n",
      "60% of land in India is under cultivation.\n",
      "\n",
      "Its \n",
      "enormous expanse of level plains, rich soils, \n",
      "high percentage of cultivable land, wide \n",
      "climatic variety, long growing season, etc \n",
      "provide a strong base to agriculture.\n",
      "\n",
      "In India, \n",
      "agriculture has been a long standing activity.\n",
      "\n",
      "\n",
      "Indian agriculture is mainly subsistence \n",
      "type.\n",
      "\n",
      "India produces rice, wheat, maize, \n",
      "sorghum and millets as major food crops; \n",
      "plantations of tea, coffee, rubber and cash \n",
      "crops like sugarcane, cotton, jute, etc are also \n",
      "produced.\n",
      "\n",
      "India is also a major producer of a \n",
      "variety of fruits and vegetables. \n",
      "\n",
      "\n",
      "Fishing in India : Fishing plays an important \n",
      "role in the economy of India.\n",
      "\n",
      "India is one of \n",
      "the largest producers of fish, both marine \n",
      "and inland.\n",
      "\n",
      "Fisheries help in augmenting \n",
      "food supply, generating employment, raising \n",
      "nutritional level and earning foreign exchange. \n",
      "\n",
      "\n",
      "Fish forms an important part of the diet \n",
      "of many people living in the coastal areas of \n",
      "Kerala, West Bengal, Orissa, Andhra Pradesh, \n",
      "Tamil Nadu, Goa and Maharashtra.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import json\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 1️⃣ Load PDF pages\n",
    "# docs = PyMuPDFLoader(\"class10phy.pdf\").load()\n",
    "docs = PyMuPDFLoader(\"geography.pdf\").load()\n",
    "\n",
    "# 2️⃣ Split into overlapping chunks (not sentence-level)\n",
    "text_splitter = SpacyTextSplitter(chunk_size=1500, chunk_overlap=375)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3️⃣ Add UUIDs + metadata to chunks\n",
    "chunk_metadata = []\n",
    "uuid_lookup = {}\n",
    "processed_docs = []\n",
    "\n",
    "for doc in split_docs:\n",
    "    uid = str(uuid.uuid4())\n",
    "    page = doc.metadata.get(\"page\", 0) + 1\n",
    "    # metadata = {**doc.metadata, \"uuid\": uid, \"page\": page, \"source\": \"class10phy.pdf\"}\n",
    "    metadata = {**doc.metadata, \"uuid\": uid, \"page\": page, \"source\": \"geography.pdf\"}\n",
    "\n",
    "    processed_docs.append(Document(page_content=doc.page_content, metadata=metadata))\n",
    "\n",
    "    entry = {\n",
    "        \"uuid\": uid,\n",
    "        \"page\": page,\n",
    "        \"text\": doc.page_content,\n",
    "        # \"source\": \"class10phy.pdf\"\n",
    "        \"source\": \"geography.pdf\"\n",
    "    }\n",
    "\n",
    "    chunk_metadata.append(entry)\n",
    "    uuid_lookup[uid] = entry\n",
    "\n",
    "# 🔒 Save chunk metadata\n",
    "with open(\"sentence_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunk_metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(\"uuid_lookup.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(uuid_lookup, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# 4️⃣ Create and reload FAISS vector store\n",
    "vector_store = FAISS.from_documents(processed_docs, embedding=embedding_model)\n",
    "# vector_store.save_local(folder_path=\"vector_index\", index_name=\"class_10_phy\")\n",
    "vector_store.save_local(folder_path=\"vector_index\", index_name=\"geography_vector\")\n",
    "\n",
    "vector_store = FAISS.load_local(\n",
    "    folder_path=\"vector_index\",\n",
    "    # index_name=\"class_10_phy\",\n",
    "    index_name=\"geography_vector\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# 5️⃣ Run query\n",
    "# query = \"Tell me about where the ganga river originates\"\n",
    "query = \"What are the main crops in brazil?\"\n",
    "retrieved = retriever.invoke(query)\n",
    "relevant = [doc.page_content for doc in retrieved]\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are helping a Class 10 Geography student answer a textbook-based question.\n",
    "\n",
    "You are given a set of extracted text chunks from the textbook that may contain the answer.\n",
    "\n",
    "Your task is to write a complete and accurate answer to the question using **only the information provided** in these chunks.\n",
    "\n",
    "If the answer depends on specific definitions, formulas, rules, steps, or listed conventions — make sure to **include all of them** clearly and accurately.\n",
    "\n",
    "You may also include any additional relevant points found in the provided content, **as long as you do not skip or miss any part that is essential to correctly answering the question**.\n",
    "\n",
    "Do not invent or assume anything beyond what is given. If the content does not contain enough information to answer the question, respond only with: \"I don't know\".\n",
    "\n",
    "---\n",
    "\n",
    "📘 Question:\n",
    "{query}\n",
    "\n",
    "📚 Extracted Text:\n",
    "{relevant_docs}\n",
    "\"\"\"\n",
    "qa_chain = ChatPromptTemplate.from_template(prompt) | llm\n",
    "response = qa_chain.invoke({\"query\": query, \"relevant_docs\": relevant})\n",
    "\n",
    "# 7️⃣ Print output\n",
    "print(\"✅ Answer:\", response.content)\n",
    "print(\"\\n🔍 Supporting Chunks:\\n\" + \"-\" * 60)\n",
    "\n",
    "for doc in retrieved:\n",
    "    uid = doc.metadata.get(\"uuid\")\n",
    "    entry = uuid_lookup.get(uid)\n",
    "    if entry:\n",
    "        print(f\"🆔 {uid} | 📄 Page: {entry['page']}\")\n",
    "        print(entry[\"text\"] + \"\\n\" + \"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e0f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Supporting Sentence IDs:\n",
      "==================================================\n",
      "['s1', 's2', 's3', 's5', 's8', 's51', 's52']\n",
      "\n",
      "📝 Sentences to Highlight:\n",
      "==================================================\n",
      "s1 (Page 65): 54 Geographical explanation Agriculture : In Brazil , agriculture is the main occupation of the people living in the highlands and coastal areas .\n",
      "s2 (Page 65): Favourable climate and topography make it possible for growing a variety of crops .\n",
      "s3 (Page 65): Rice and maize are the main cereal crops .\n",
      "s5 (Page 65): Commercial crops like coffee , cocoa , rubber , soyabean and sugarcane are cultivated on a large scale .\n",
      "s8 (Page 65): Besides these crops , production of fruits like bananas , Figure 8.3 pineapples , oranges and other citrus fruits is also done .\n",
      "s51 (Page 67): India produces rice , wheat , maize , sorghum and millets as major food crops ; plantations of tea , coffee , rubber and cash crops like sugarcane , cotton , jute , etc are also produced .\n",
      "s52 (Page 67): India is also a major producer of a variety of fruits and vegetables .\n"
     ]
    }
   ],
   "source": [
    "import syntok.segmenter as segmenter\n",
    "import json\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from math import ceil\n",
    "\n",
    "# 1️⃣ Sentence segmentation using syntok\n",
    "sentence_candidates = []\n",
    "sentence_lookup = {}\n",
    "sid_counter = 1\n",
    "\n",
    "for doc in retrieved:\n",
    "    chunk_uuid = doc.metadata[\"uuid\"]\n",
    "    page = doc.metadata[\"page\"]\n",
    "    source = doc.metadata[\"source\"]\n",
    "    chunk_text = doc.page_content\n",
    "\n",
    "    # Use syntok to break into paragraphs and sentences\n",
    "    paragraphs = segmenter.analyze(chunk_text)\n",
    "    for paragraph in paragraphs:\n",
    "        for sentence in paragraph:\n",
    "            sentence_text = \" \".join(token.value for token in sentence).strip()\n",
    "            if not sentence_text:\n",
    "                continue\n",
    "\n",
    "            sid = f\"s{sid_counter}\"\n",
    "            sid_counter += 1\n",
    "\n",
    "            entry = {\n",
    "                \"sid\": sid,\n",
    "                \"sentence\": sentence_text,\n",
    "                \"page\": page,\n",
    "                \"chunk_uuid\": chunk_uuid,\n",
    "                \"source\": source\n",
    "            }\n",
    "\n",
    "            sentence_candidates.append({\"sid\": sid, \"sentence\": sentence_text})\n",
    "            sentence_lookup[sid] = entry\n",
    "\n",
    "# 🔒 Save sentence metadata\n",
    "with open(\"sentence_lookup.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sentence_lookup, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# 2️⃣ Schema for LLM response\n",
    "class SupportIDs(BaseModel):\n",
    "    ids: List[str]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=SupportIDs)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# 3️⃣ Prompt for justification\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are helping a student understand which textbook sentences support a given answer.\n",
    "\n",
    "You are given:\n",
    "- A user question\n",
    "- An answer to that question\n",
    "- A list of textbook sentences, each with a short ID\n",
    "\n",
    "Your task is to identify the **minimum set** of sentence IDs that directly support the answer.\n",
    "\n",
    "Only include:\n",
    "- Sentences that clearly explain the final answer\n",
    "- Definitions, laws, rules, or steps that are explicitly used in the answer\n",
    "\n",
    "Do NOT include:\n",
    "- General background statements\n",
    "- Sentences that are only loosely related\n",
    "- Sentences that repeat the same content with different wording\n",
    "\n",
    "If multiple sentences say the same thing, prefer the **clearest and most complete** one.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {query}\n",
    "Answer: {answer}\n",
    "\n",
    "Candidate Sentences:\n",
    "{sentences}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# 4️⃣ Process in batches\n",
    "batch_size = 20\n",
    "num_batches = ceil(len(sentence_candidates) / batch_size)\n",
    "all_ids = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch = sentence_candidates[i * batch_size:(i + 1) * batch_size]\n",
    "    formatted_sentences = \"\\n\".join(\n",
    "        f\"- [sid: {s['sid']}] {s['sentence']}\" for s in batch\n",
    "    )\n",
    "\n",
    "    structured_chain = prompt | llm | parser\n",
    "\n",
    "    try:\n",
    "        result = structured_chain.invoke({\n",
    "            \"query\": query,\n",
    "            \"answer\": response.content,\n",
    "            \"sentences\": formatted_sentences,\n",
    "            \"format_instructions\": format_instructions\n",
    "        })\n",
    "\n",
    "        # Normalize and collect IDs\n",
    "        cleaned = [sid.replace(\"sid:\", \"\").strip() for sid in result.ids]\n",
    "        all_ids.extend(cleaned)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Batch {i+1} failed: {e}\")\n",
    "        continue\n",
    "\n",
    "# ✅ Final cleanup\n",
    "returned_sids = sorted(set(all_ids), key=lambda x: int(x[1:]))\n",
    "\n",
    "# ✅ Output\n",
    "print(\"\\n✅ Supporting Sentence IDs:\\n\" + \"=\" * 50)\n",
    "print(returned_sids)\n",
    "\n",
    "print(\"\\n📝 Sentences to Highlight:\\n\" + \"=\" * 50)\n",
    "for sid in returned_sids:\n",
    "    entry = sentence_lookup.get(sid)\n",
    "    if entry:\n",
    "        print(f\"{sid} (Page {entry['page']}): {entry['sentence']}\")\n",
    "\n",
    "with open(\"highlight_ids.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(returned_sids, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269687f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Not found: 54 Geographical explanation Agriculture : In Brazil , agriculture is the main oc\n",
      "✅ Highlighted: s2\n",
      "✅ Highlighted: s3\n",
      "✅ Highlighted: s5\n",
      "✅ Highlighted: s8\n",
      "✅ Highlighted: s51\n",
      "✅ Highlighted: s52\n",
      "✅ Highlighted PDF saved to: highlighted_output.pdf\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# 📁 Paths\n",
    "base_dir = Path(\".\")\n",
    "highlight_ids_path = base_dir / \"highlight_ids.json\"\n",
    "sentence_lookup_path = base_dir / \"sentence_lookup.json\"\n",
    "# input_pdf_path = base_dir / \"class10phy.pdf\"\n",
    "input_pdf_path = base_dir / \"geography.pdf\"\n",
    "output_pdf_path = base_dir / \"highlighted_output.pdf\"\n",
    "\n",
    "# 🧹 Normalization helper\n",
    "def normalize(text):\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "# 📄 Load data\n",
    "with open(highlight_ids_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    highlight_ids = json.load(f)\n",
    "\n",
    "with open(sentence_lookup_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    sentence_lookup = json.load(f)\n",
    "\n",
    "# 📘 Open PDF\n",
    "doc = fitz.open(input_pdf_path)\n",
    "\n",
    "# 🟨 Highlight loop\n",
    "for sid in highlight_ids:\n",
    "    entry = sentence_lookup[sid]\n",
    "    sentence = entry[\"sentence\"]\n",
    "    page = doc[entry[\"page\"] - 1]\n",
    "    sentence_norm = normalize(sentence)\n",
    "\n",
    "    # Step 1: get all words with bounding boxes\n",
    "    words = page.get_text(\"words\")  # list of [x0, y0, x1, y1, \"word\"]\n",
    "    words_norm = [normalize(w[4]) for w in words]\n",
    "\n",
    "    # Step 2: sliding window match\n",
    "    match_found = False\n",
    "    for i in range(len(words_norm)):\n",
    "        for j in range(i + 5, len(words_norm) + 1):  # allow sentences at least 5 words\n",
    "            phrase = \" \".join(words_norm[i:j])\n",
    "            if phrase == sentence_norm:\n",
    "                match_found = True\n",
    "                rects = [fitz.Rect(words[k][0], words[k][1], words[k][2], words[k][3]) for k in range(i, j)]\n",
    "                for r in rects:\n",
    "                    page.add_highlight_annot(r)\n",
    "                print(f\"✅ Highlighted: {sid}\")\n",
    "                break\n",
    "        if match_found:\n",
    "            break\n",
    "\n",
    "    if not match_found:\n",
    "        print(f\"❌ Not found: {sentence[:80]}\")\n",
    "\n",
    "# 💾 Save\n",
    "doc.save(output_pdf_path, garbage=4, deflate=True)\n",
    "doc.close()\n",
    "print(f\"✅ Highlighted PDF saved to: {output_pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33222b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1: Science\n",
      "Line 2: 134\n",
      "Line 3: Light – Reflection and\n",
      "Line 4: Refraction\n",
      "Line 5: 9\n",
      "Line 6: CHAPTER\n",
      "Line 7: W\n",
      "Line 8: e see a variety of objects in the world around us. However, we are\n",
      "Line 9: unable to see anything in a dark room. On lighting up the room,\n",
      "Line 10: things become visible. What makes things visible? During the day, the\n",
      "Line 11: sunlight helps us to see objects. An object reflects light that falls on it.\n",
      "Line 12: This reflected light, when received by our eyes, enables us to see things.\n",
      "Line 13: We are able to see through a transparent medium as light is transmitted\n",
      "Line 14: through it. There are a number of common wonderful phenomena\n",
      "Line 15: associated with light such as image formation by mirrors, the twinkling\n",
      "Line 16: of stars, the beautiful colours of a rainbow, bending of light by a medium\n",
      "Line 17: and so on.  A study of the properties of light helps us to explore them.\n",
      "Line 18: By observing the common optical phenomena around us, we may\n",
      "Line 19: conclude that light seems to travel in straight lines. The fact that a small\n",
      "Line 20: source of light casts a sharp shadow of an opaque object points to this\n",
      "Line 21: straight-line path of light, usually indicated as a ray of light.\n",
      "Line 22: More to Know!\n",
      "Line 23: If an opaque object on the path of light becomes very small, light has a tendency to\n",
      "Line 24: bend around it and not walk in a straight line – an effect known as the diffraction of\n",
      "Line 25: light. Then the straight-line treatment of optics using rays fails. To explain phenomena\n",
      "Line 26: such as diffraction, light is thought of as a wave, the details of which you will study\n",
      "Line 27: in higher classes. Again, at the beginning of the 20th century, it became known that\n",
      "Line 28: the wave theory of light often becomes inadequate for treatment of the interaction of\n",
      "Line 29: light with matter, and light often behaves somewhat like a stream of particles. This\n",
      "Line 30: confusion about the true nature of light continued for some years till a modern\n",
      "Line 31: quantum theory of light emerged in which light is neither a ‘wave’ nor a ‘particle’ –\n",
      "Line 32: the new theory reconciles the particle properties of light with the wave nature.\n",
      "Line 33: In this Chapter, we shall study the phenomena of reflection and\n",
      "Line 34: refraction of light using the straight-line propagation of light. These basic\n",
      "Line 35: concepts will help us in the study of some of the optical phenomena in\n",
      "Line 36: nature. We shall try to understand in this Chapter the reflection of light\n",
      "Line 37: by spherical mirrors and refraction of light and their application in real\n",
      "Line 38: life situations.\n",
      "Line 39: 9.1 REFLECTION OF LIGHT\n",
      "Line 40: 9.1 REFLECTION OF LIGHT\n",
      "Line 41: 9.1 REFLECTION OF LIGHT\n",
      "Line 42: 9.1 REFLECTION OF LIGHT\n",
      "Line 43: 9.1 REFLECTION OF LIGHT\n",
      "Line 44: A highly polished surface, such as a mirror, reflects most of the light\n",
      "Line 45: falling on it. You are already familiar with the laws of reflection of light.\n",
      "Line 46: Reprint 2025-26\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_lines_with_line_numbers(pdf_path, page_num=0):\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    # Extract text blocks from the first page (page_num = 0 for first page)\n",
    "    page = doc.load_page(page_num)\n",
    "    text_instances = page.get_text(\"dict\")[\"blocks\"]\n",
    "    \n",
    "    lines = []\n",
    "    line_number = 1  # Start with line 1\n",
    "    \n",
    "    for block in text_instances:\n",
    "        if block['type'] == 0:  # Text block (only type 0 is text)\n",
    "            for line in block[\"lines\"]:\n",
    "                line_text = \"\"\n",
    "                for span in line[\"spans\"]:\n",
    "                    line_text += span[\"text\"]  # Concatenate the text from each span\n",
    "                \n",
    "                # Append line text with the line number\n",
    "                lines.append((line_number, line_text.strip()))\n",
    "                line_number += 1  # Increment line number\n",
    "    \n",
    "    return lines\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = \"class10phy.pdf\"\n",
    "\n",
    "# Extract lines with line numbers from the first page\n",
    "lines = extract_lines_with_line_numbers(pdf_path, page_num=0)\n",
    "\n",
    "# Print each line with its line number\n",
    "for line_number, line_text in lines:\n",
    "    print(f\"Line {line_number}: {line_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105acc9d",
   "metadata": {},
   "source": [
    "## 2 step thinking model and Agentic/ReAct loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943be488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ashish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import json\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 1️⃣ Load PDF pages\n",
    "# docs = PyMuPDFLoader(\"class10phy.pdf\").load()\n",
    "docs = PyMuPDFLoader(\"geography.pdf\").load()\n",
    "\n",
    "# 2️⃣ Split into overlapping chunks (not sentence-level)\n",
    "text_splitter = SpacyTextSplitter(chunk_size=1500, chunk_overlap=375)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3️⃣ Add UUIDs + metadata to chunks\n",
    "chunk_metadata = []\n",
    "uuid_lookup = {}\n",
    "processed_docs = []\n",
    "\n",
    "for doc in split_docs:\n",
    "    uid = str(uuid.uuid4())\n",
    "    page = doc.metadata.get(\"page\", 0) + 1\n",
    "    # metadata = {**doc.metadata, \"uuid\": uid, \"page\": page, \"source\": \"class10phy.pdf\"}\n",
    "    metadata = {**doc.metadata, \"uuid\": uid, \"page\": page, \"source\": \"geography.pdf\"}\n",
    "\n",
    "    processed_docs.append(Document(page_content=doc.page_content, metadata=metadata))\n",
    "\n",
    "    entry = {\n",
    "        \"uuid\": uid,\n",
    "        \"page\": page,\n",
    "        \"text\": doc.page_content,\n",
    "        # \"source\": \"class10phy.pdf\"\n",
    "        \"source\": \"geography.pdf\"\n",
    "    }\n",
    "\n",
    "    chunk_metadata.append(entry)\n",
    "    uuid_lookup[uid] = entry\n",
    "\n",
    "# 🔒 Save chunk metadata\n",
    "with open(\"sentence_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunk_metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(\"uuid_lookup.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(uuid_lookup, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# 4️⃣ Create and reload FAISS vector store\n",
    "vector_store = FAISS.from_documents(processed_docs, embedding=embedding_model)\n",
    "# vector_store.save_local(folder_path=\"vector_index\", index_name=\"class_10_phy\")\n",
    "vector_store.save_local(folder_path=\"vector_index\", index_name=\"geography_vector\")\n",
    "\n",
    "vector_store = FAISS.load_local(\n",
    "    folder_path=\"vector_index\",\n",
    "    # index_name=\"class_10_phy\",\n",
    "    index_name=\"geography_vector\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec6ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# # 5️⃣ Run query\n",
    "# # query = \"Tell me about where the ganga river originates\"\n",
    "# # query = \"What are the main crops in brazil?\"\n",
    "# query = \"What place in india has the most urbanisation?\"\n",
    "# retrieved = retriever.invoke(query)\n",
    "# relevant = [doc.page_content for doc in retrieved]\n",
    "\n",
    "# # Step 1️⃣ Extract facts\n",
    "# facts_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "# You are a helpful assistant extracting information from a textbook.\n",
    "\n",
    "# Task: From the given chunks, extract ONLY the key facts, rules, or definitions \n",
    "# that directly answer the question.\n",
    "\n",
    "# ⚠️ If the question asks for a \"most\", \"highest\", or \"first\", make sure to identify \n",
    "# the item with the **highest value** or most relevant occurrence, \n",
    "# even if the text mentions a lower value first.\n",
    "\n",
    "# - Copy text closely from the chunks (do not paraphrase heavily).\n",
    "# - Skip irrelevant details.\n",
    "# - If no facts are relevant, return \"NONE\".\n",
    "\n",
    "# 📘 Question:\n",
    "# {query}\n",
    "\n",
    "# 📚 Extracted Text:\n",
    "# {relevant_docs}\n",
    "# \"\"\")\n",
    "\n",
    "\n",
    "# facts_chain = facts_prompt | llm\n",
    "# facts_response = facts_chain.invoke({\"query\": query, \"relevant_docs\": relevant})\n",
    "\n",
    "# print(\"🔍 Extracted Facts:\\n\", facts_response.content)\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# answer_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "# You are a helpful tutor writing a final answer for a student.\n",
    "\n",
    "# Use ONLY the extracted facts provided. \n",
    "# Do not add any outside knowledge. \n",
    "# Do not skip or invent any essential facts.\n",
    "\n",
    "# Write the answer in a clear, concise, and textbook-like style. \n",
    "# Follow the content in the extracted facts closely, \n",
    "# but it’s okay to combine sentences for readability. \n",
    "# Do not change factual details.\n",
    "\n",
    "# 📘 Question:\n",
    "# {query}\n",
    "\n",
    "# 📝 Extracted Facts:\n",
    "# {facts}\n",
    "# \"\"\")\n",
    "\n",
    "\n",
    "# answer_chain = answer_prompt | llm\n",
    "# answer_response = answer_chain.invoke({\"query\": query, \"facts\": facts_response.content})\n",
    "\n",
    "# print(\"✅ Final Answer:\\n\", answer_response.content)\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # 🔎 Print supporting chunks for transparency\n",
    "# print(\"\\n🔍 Supporting Chunks:\\n\" + \"-\" * 60)\n",
    "# for doc in retrieved:\n",
    "#     uid = doc.metadata.get(\"uuid\")\n",
    "#     entry = uuid_lookup.get(uid)\n",
    "#     if entry:\n",
    "#         print(f\"🆔 {uid} | 📄 Page: {entry['page']}\")\n",
    "#         print(entry[\"text\"] + \"\\n\" + \"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5121856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Reasoning Phase:\n",
      " FACTS:\n",
      "- There are four seasons as per the Indian Meteorological Department.\n",
      "- The hot weather season\n",
      "- The season of rainfall (Monsoon)\n",
      "- The season of Retreating Monsoon\n",
      "- The cold weather season\n",
      "\n",
      "ENTITIES:\n",
      "- hot weather season\n",
      "- season of rainfall (Monsoon)\n",
      "- season of Retreating Monsoon\n",
      "- cold weather season\n",
      "\n",
      "JUSTIFICATION: The text states that there are four seasons according to the Indian Meteorological Department, and then lists those four seasons.\n",
      "================================================================================\n",
      "✅ Final Answer:\n",
      " The four seasons, according to the Indian Meteorological Department, are: the hot weather season, the season of rainfall (Monsoon), the season of Retreating Monsoon, and the cold weather season.\n",
      "================================================================================\n",
      "\n",
      "🔍 Supporting Chunks:\n",
      "------------------------------------------------------------\n",
      "🆔 b5c638cb-0edc-4640-a3ba-b2d8f81bc7d8 | 📄 Page: 40\n",
      "z\n",
      "The season of Retreating Monsoon\n",
      "z\n",
      "The cold weather season\n",
      "Observe the pictures given below from \n",
      "figure 4.6 to 4.13 and write a brief description \n",
      "about them.\n",
      "\n",
      "\n",
      "Use your brain power!\n",
      ")\n",
      "\n",
      "Group the months into seasons for a whole year \n",
      "according to the charts given.\n",
      ")\n",
      "\n",
      "Find out more about different ways of \n",
      "classification and the seasons.\n",
      "\n",
      "For example, \n",
      "what is summer?\n",
      "\n",
      "\n",
      "Ob\n",
      "h\n",
      "i\n",
      "i\n",
      "b l\n",
      "f\n",
      "Colours of Both\n",
      "Considering the location extent  and \n",
      "climatic conditions of both the countries, \n",
      "write months as per in the seasons.\n",
      "\n",
      "\n",
      "Seasons /\n",
      "India\n",
      "Brazil\n",
      "Summer \n",
      "Winter\n",
      "------------------------------------------------------------\n",
      "🆔 c9fbc580-ced4-41b2-91d7-e58ffc87f6f8 | 📄 Page: 40\n",
      "The rainfall reduces in the leeward side \n",
      "of the hills.\n",
      "\n",
      "These winds blow parallel to the \n",
      "Aravalis.\n",
      "\n",
      "As a result, rainfall is low in parts \n",
      "of Gujarat and Rajasthan.\n",
      "\n",
      "Later, these winds \n",
      "move towards the Himalayas.\n",
      "\n",
      "Their moisture-\n",
      "-carrying capacity increases.\n",
      "\n",
      "Orograpghic \n",
      "type of rainfall occurs because of the natural \n",
      "obstruction of the Himalayas.\n",
      "\n",
      "These winds \n",
      "return from the Himalayan ranges and their \n",
      "retreating journey starts.\n",
      "\n",
      "While blowing from \n",
      "the north-east towards the Indian Ocean, these \n",
      "winds bring rainfall again to some parts of \n",
      "the Peninsula.\n",
      "\n",
      "This is the Retreating Monsoon \n",
      "season in India.\n",
      "\n",
      "In general, the climate of India \n",
      "is hot throughout the year.\n",
      "\n",
      "\n",
      "As the Tropic of Cancer passes through \n",
      "the middle of India, India is considered to \n",
      "be in the tropical region.  \n",
      "\n",
      "India faces natural \n",
      "disasters like erratic rainfall, droughts, \n",
      "cyclones, floods, etc.  frequently.\n",
      "\n",
      "\n",
      "There are four seasons as per the Indian \n",
      "Meteorological Department. \n",
      "\n",
      "\n",
      "z\n",
      "The hot weather season\n",
      "z\n",
      "The season of rainfall (Monsoon)\n",
      "\n",
      "\n",
      "z\n",
      "The season of Retreating Monsoon\n",
      "z\n",
      "The cold weather season\n",
      "Observe the pictures given below from \n",
      "figure 4.6 to 4.13 and write a brief description \n",
      "about them.\n",
      "\n",
      "\n",
      "Use your brain power!\n",
      ")\n",
      "\n",
      "Group the months into seasons for a whole year \n",
      "according to the charts given.\n",
      ")\n",
      "\n",
      "Find out more about different ways of \n",
      "classification and the seasons.\n",
      "\n",
      "For example, \n",
      "what is summer?\n",
      "------------------------------------------------------------\n",
      "🆔 0b2d77cc-c1e2-4b68-a427-99c6e026977c | 📄 Page: 38\n",
      "27\n",
      "Figure 4.4 : Annual average Temperature and Rainfall graph\n",
      "Belem\n",
      "Rio de Janeiro\n",
      "Porto Alegre\n",
      "Manaus\n",
      "Rainfall (mm)\n",
      "Rainfall (mm)\n",
      "Temperature °\n",
      "\n",
      "C\n",
      "Temperature ° C\n",
      "Temperature ° C\n",
      "Temperature ° C\n",
      "Rainfall (mm)\n",
      "Rainfall (mm)\n",
      "Max.\n",
      "\n",
      "Temperature\n",
      "Min.\n",
      "\n",
      "Temperature\n",
      "Give it a try.\n",
      "\n",
      "\n",
      "Considering the various factors affecting \n",
      "Brazil’s climate, complete the adjoining table.\n",
      "\n",
      "\n",
      "Regions \n",
      "Climatic characteristics \n",
      " Amazon Valley\n",
      "Highlands \n",
      "Pantanal\n",
      "Northern Coastal \n",
      "region \n",
      "Southern Coastal \n",
      "region \n",
      "Southernmost region \n",
      "of Brazil  \n",
      "Can you tell?\n",
      "\n",
      "\n",
      "Study the graphs given in fig 4.4 and \n",
      "answer the following questions.\n",
      "¾ In which month is the highest temperature \n",
      "found in all the four cities? \n",
      "¾ In which month does it rain the most in the \n",
      "given cities ? \n",
      "¾\n",
      "\n",
      "When does Brazil have its rainy season ?\n",
      "¾ Which city has the maximum range of \n",
      "temperature?\n",
      "\n",
      "How much is it?\n",
      "¾\n",
      "\n",
      "What type of climate will be found in Rio \n",
      "De Janerio ? \n",
      "\n",
      "\n",
      "Geographical explanation\n",
      "BRAZIL:\n",
      "Because of the vast latitudinal extent of \n",
      "Brazil, it experiences wide range of climatic \n",
      "variations in climate.\n",
      "\n",
      "For example near equator \n",
      "it is hot while temperate type of climate is \n",
      "found near Tropic of Capricorn.\n",
      "\n",
      "Brazil gets \n",
      "rainfall from the South-East Trade winds and \n",
      "the North-east Trade Winds. \n",
      "\n",
      "\n",
      "Parts of the Brazilian highlands extend \n",
      "upto the northern coast.\n",
      "\n",
      "The escarpments act as \n",
      "an obstruction to the winds blowing from the \n",
      "sea and cause orographic type of rainfall in the \n",
      "coastal region.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 5️⃣ Run query\n",
    "query = \"what are the different seasons\"\n",
    "retrieved = retriever.invoke(query)\n",
    "relevant = [doc.page_content for doc in retrieved]\n",
    "\n",
    "# STEP 1 — Evidence + Selection (reasoning phase)\n",
    "reasoning_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are analyzing textbook excerpts to answer a question.\n",
    "\n",
    "📘 Question:\n",
    "{query}\n",
    "\n",
    "📚 Textbook Chunks:\n",
    "{relevant_docs}\n",
    "\n",
    "Instructions:\n",
    "1. Extract **all key facts** relevant to the question.\n",
    "   - Include numbers, percentages, dates, or terms exactly as written.\n",
    "   - Include comparative or regional statements if they help.\n",
    "   - Ignore irrelevant text.\n",
    "2. Identify the **list of entities** that directly answer the question.\n",
    "   - Each entity should be a single item (e.g., season, concept, person, measurement).\n",
    "3. Provide a short **justification** for why these entities are correct, based only on the extracted facts.\n",
    "\n",
    "Return EXACTLY in this format:\n",
    "\n",
    "FACTS:\n",
    "- <fact1>\n",
    "- <fact2>\n",
    "...\n",
    "\n",
    "ENTITIES:\n",
    "- <entity1>\n",
    "- <entity2>\n",
    "...\n",
    "\n",
    "JUSTIFICATION: <one short sentence based only on the facts>\n",
    "\"\"\")\n",
    "\n",
    "reasoning_chain = reasoning_prompt | llm\n",
    "reasoning_response = reasoning_chain.invoke({\n",
    "    \"query\": query,\n",
    "    \"relevant_docs\": relevant\n",
    "})\n",
    "\n",
    "print(\"🔍 Reasoning Phase:\\n\", reasoning_response.content)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# STEP 2 — Final Answer (teaching phase)\n",
    "answer_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a tutor writing a **clear, textbook-style answer**.\n",
    "\n",
    "📘 Question:\n",
    "{query}\n",
    "\n",
    "🔎 Reasoning Output:\n",
    "{reasoning}\n",
    "\n",
    "Instructions:\n",
    "- Start by clearly listing the entities that answer the question.\n",
    "- Expand slightly by summarizing the supporting facts in simple sentences.\n",
    "- Maintain all numbers, names, and phrases exactly as in the reasoning output.\n",
    "- Keep it concise, readable, and like a textbook explanation for students.\n",
    "- Do not add outside knowledge or speculation.\n",
    "\n",
    "Return **only the final answer paragraph**.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "answer_chain = answer_prompt | llm\n",
    "answer_response = answer_chain.invoke({\n",
    "    \"query\": query,\n",
    "    \"reasoning\": reasoning_response.content\n",
    "})\n",
    "\n",
    "print(\"✅ Final Answer:\\n\", answer_response.content)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 🔎 Print supporting chunks for transparency\n",
    "print(\"\\n🔍 Supporting Chunks:\\n\" + \"-\" * 60)\n",
    "for doc in retrieved:\n",
    "    uid = doc.metadata.get(\"uuid\")\n",
    "    entry = uuid_lookup.get(uid)\n",
    "    if entry:\n",
    "        print(f\"🆔 {uid} | 📄 Page: {entry['page']}\")\n",
    "        print(entry[\"text\"] + \"\\n\" + \"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d823b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Supporting Sentence IDs:\n",
      "==================================================\n",
      "['s16', 's20', 's21', 's22']\n",
      "\n",
      "📝 Sentences to Highlight:\n",
      "==================================================\n",
      "s16 (Page 40): This is the Retreating Monsoon season in India .\n",
      "s20 (Page 40): There are four seasons as per the Indian Meteorological Department .\n",
      "s21 (Page 40): z The hot weather season z The season of rainfall ( Monsoon )\n",
      "s22 (Page 40): z The season of Retreating Monsoon z The cold weather season Observe the pictures given below from figure 4.6 to 4.13 and write a brief description about them .\n"
     ]
    }
   ],
   "source": [
    "import syntok.segmenter as segmenter\n",
    "import json\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from math import ceil\n",
    "\n",
    "# 1️⃣ Sentence segmentation using syntok\n",
    "sentence_candidates = []\n",
    "sentence_lookup = {}\n",
    "sid_counter = 1\n",
    "\n",
    "for doc in retrieved:\n",
    "    chunk_uuid = doc.metadata[\"uuid\"]\n",
    "    page = doc.metadata[\"page\"]\n",
    "    source = doc.metadata[\"source\"]\n",
    "    chunk_text = doc.page_content\n",
    "\n",
    "    # Break into paragraphs and sentences\n",
    "    paragraphs = segmenter.analyze(chunk_text)\n",
    "    for paragraph in paragraphs:\n",
    "        for sentence in paragraph:\n",
    "            sentence_text = \" \".join(token.value for token in sentence).strip()\n",
    "            if not sentence_text:\n",
    "                continue\n",
    "\n",
    "            sid = f\"s{sid_counter}\"\n",
    "            sid_counter += 1\n",
    "\n",
    "            entry = {\n",
    "                \"sid\": sid,\n",
    "                \"sentence\": sentence_text,\n",
    "                \"page\": page,\n",
    "                \"chunk_uuid\": chunk_uuid,\n",
    "                \"source\": source\n",
    "            }\n",
    "\n",
    "            sentence_candidates.append({\"sid\": sid, \"sentence\": sentence_text})\n",
    "            sentence_lookup[sid] = entry\n",
    "\n",
    "# 🔒 Save sentence metadata\n",
    "with open(\"sentence_lookup.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sentence_lookup, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# 2️⃣ Schema for LLM response\n",
    "class SupportIDs(BaseModel):\n",
    "    ids: List[str]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=SupportIDs)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# 3️⃣ Prompt for justification\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are helping a student understand which textbook sentences support a given answer.\n",
    "\n",
    "You are given:\n",
    "- A user question\n",
    "- An answer to that question\n",
    "- A list of textbook sentences, each with a short ID\n",
    "\n",
    "Your task is to identify the **minimum set** of sentence IDs that directly support the answer.\n",
    "\n",
    "Only include:\n",
    "- Sentences that clearly explain the final answer\n",
    "- Definitions, laws, rules, or steps that are explicitly used in the answer\n",
    "\n",
    "Do NOT include:\n",
    "- General background statements\n",
    "- Sentences that are only loosely related\n",
    "- Sentences that repeat the same content with different wording\n",
    "\n",
    "If multiple sentences say the same thing, prefer the **clearest and most complete** one.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {query}\n",
    "Answer: {answer}\n",
    "\n",
    "Candidate Sentences:\n",
    "{sentences}\n",
    "\"\"\")\n",
    "\n",
    "# 4️⃣ Process in batches\n",
    "batch_size = 20\n",
    "num_batches = ceil(len(sentence_candidates) / batch_size)\n",
    "all_ids = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch = sentence_candidates[i * batch_size:(i + 1) * batch_size]\n",
    "    formatted_sentences = \"\\n\".join(\n",
    "        f\"- [sid: {s['sid']}] {s['sentence']}\" for s in batch\n",
    "    )\n",
    "\n",
    "    structured_chain = prompt | llm | parser\n",
    "\n",
    "    try:\n",
    "        result = structured_chain.invoke({\n",
    "            \"query\": query,\n",
    "            \"answer\": answer_response.content,   # ✅ use final two-step answer\n",
    "            \"sentences\": formatted_sentences,\n",
    "            \"format_instructions\": format_instructions\n",
    "        })\n",
    "\n",
    "        cleaned = [sid.replace(\"sid:\", \"\").strip() for sid in result.ids]\n",
    "        all_ids.extend(cleaned)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Batch {i+1} failed: {e}\")\n",
    "        continue\n",
    "\n",
    "# ✅ Final cleanup\n",
    "returned_sids = sorted(set(all_ids), key=lambda x: int(x[1:]))\n",
    "\n",
    "# ✅ Output\n",
    "print(\"\\n✅ Supporting Sentence IDs:\\n\" + \"=\" * 50)\n",
    "print(returned_sids)\n",
    "\n",
    "print(\"\\n📝 Sentences to Highlight:\\n\" + \"=\" * 50)\n",
    "for sid in returned_sids:\n",
    "    entry = sentence_lookup.get(sid)\n",
    "    if entry:\n",
    "        print(f\"{sid} (Page {entry['page']}): {entry['sentence']}\")\n",
    "\n",
    "with open(\"highlight_ids.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(returned_sids, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74f7f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Highlighted: s16\n",
      "✅ Highlighted: s20\n",
      "✅ Highlighted: s21\n",
      "✅ Highlighted: s22\n",
      "✅ Highlighted PDF saved to: highlighted_output.pdf\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# 📁 Paths\n",
    "base_dir = Path(\".\")\n",
    "highlight_ids_path = base_dir / \"highlight_ids.json\"\n",
    "sentence_lookup_path = base_dir / \"sentence_lookup.json\"\n",
    "# input_pdf_path = base_dir / \"class10phy.pdf\"\n",
    "input_pdf_path = base_dir / \"geography.pdf\"\n",
    "output_pdf_path = base_dir / \"highlighted_output.pdf\"\n",
    "\n",
    "# 🧹 Normalization helper\n",
    "def normalize(text):\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "# 📄 Load data\n",
    "with open(highlight_ids_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    highlight_ids = json.load(f)\n",
    "\n",
    "with open(sentence_lookup_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    sentence_lookup = json.load(f)\n",
    "\n",
    "# 📘 Open PDF\n",
    "doc = fitz.open(input_pdf_path)\n",
    "\n",
    "# 🟨 Highlight loop\n",
    "for sid in highlight_ids:\n",
    "    entry = sentence_lookup[sid]\n",
    "    sentence = entry[\"sentence\"]\n",
    "    page = doc[entry[\"page\"] - 1]\n",
    "    sentence_norm = normalize(sentence)\n",
    "\n",
    "    # Step 1: get all words with bounding boxes\n",
    "    words = page.get_text(\"words\")  # list of [x0, y0, x1, y1, \"word\"]\n",
    "    words_norm = [normalize(w[4]) for w in words]\n",
    "\n",
    "    # Step 2: sliding window match\n",
    "    match_found = False\n",
    "    for i in range(len(words_norm)):\n",
    "        for j in range(i + 5, len(words_norm) + 1):  # allow sentences at least 5 words\n",
    "            phrase = \" \".join(words_norm[i:j])\n",
    "            if phrase == sentence_norm:\n",
    "                match_found = True\n",
    "                rects = [fitz.Rect(words[k][0], words[k][1], words[k][2], words[k][3]) for k in range(i, j)]\n",
    "                for r in rects:\n",
    "                    page.add_highlight_annot(r)\n",
    "                print(f\"✅ Highlighted: {sid}\")\n",
    "                break\n",
    "        if match_found:\n",
    "            break\n",
    "\n",
    "    if not match_found:\n",
    "        print(f\"❌ Not found: {sentence[:80]}\")\n",
    "\n",
    "# 💾 Save\n",
    "doc.save(output_pdf_path, garbage=4, deflate=True)\n",
    "doc.close()\n",
    "print(f\"✅ Highlighted PDF saved to: {output_pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f92c9ca",
   "metadata": {},
   "source": [
    "## Handling bullets and other special symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0687df4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ashish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Ashish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e516f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ashish\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import json\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "# 1️⃣ Load PDF pages\n",
    "# docs = PyMuPDFLoader(\"class10phy.pdf\").load()\n",
    "docs = PyMuPDFLoader(\"class10phy.pdf\").load()\n",
    "\n",
    "# 2️⃣ Split into overlapping chunks (not sentence-level)\n",
    "text_splitter = SpacyTextSplitter(chunk_size=1500, chunk_overlap=375)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3️⃣ Add UUIDs + metadata to chunks\n",
    "chunk_metadata = []\n",
    "uuid_lookup = {}\n",
    "processed_docs = []\n",
    "\n",
    "for doc in split_docs:\n",
    "    uid = str(uuid.uuid4())\n",
    "    page = doc.metadata.get(\"page\", 0) + 1\n",
    "    # metadata = {**doc.metadata, \"uuid\": uid, \"page\": page, \"source\": \"class10phy.pdf\"}\n",
    "    metadata = {**doc.metadata, \"uuid\": uid, \"page\": page, \"source\": \"geography.pdf\"}\n",
    "\n",
    "    processed_docs.append(Document(page_content=doc.page_content, metadata=metadata))\n",
    "\n",
    "    entry = {\n",
    "        \"uuid\": uid,\n",
    "        \"page\": page,\n",
    "        \"text\": doc.page_content,\n",
    "        \"source\": \"class10phy.pdf\"\n",
    "        # \"source\": \"history.pdf\"\n",
    "        # \"source\": \"geography.pdf\"\n",
    "    }\n",
    "\n",
    "    chunk_metadata.append(entry)\n",
    "    uuid_lookup[uid] = entry\n",
    "\n",
    "# 🔒 Save chunk metadata\n",
    "with open(\"sentence_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunk_metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(\"uuid_lookup.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(uuid_lookup, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# 4️⃣ Create and reload FAISS vector store\n",
    "vector_store = FAISS.from_documents(processed_docs, embedding=embedding_model)\n",
    "vector_store.save_local(folder_path=\"vector_index\", index_name=\"class_10_phy\")\n",
    "# vector_store.save_local(folder_path=\"vector_index\", index_name=\"geography_vector\")\n",
    "# vector_store.save_local(folder_path=\"vector_index\", index_name=\"history_vector\")\n",
    "\n",
    "vector_store = FAISS.load_local(\n",
    "    folder_path=\"vector_index\",\n",
    "    index_name=\"class_10_phy\",\n",
    "    # index_name=\"geography_vector\",\n",
    "    # index_name=\"history_vector\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1d687cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Reasoning Phase:\n",
      " FACTS:\n",
      "- The refractive index of medium 2 with respect to medium 1 is given by the ratio of the speed of light in medium 1 and the speed of light in medium 2.\n",
      "- n21= Speed of light in medium 1 / Speed of light in medium 2 = v1/v2 (9.5)\n",
      "- The refractive index of medium 1 with respect to medium 2 is represented as n12.\n",
      "- n12= Speed of light in medium 2 / Speed of light in medium 1 = v2/v1 (9.6)\n",
      "- If medium 1 is vacuum or air, then the refractive index of medium 2 is considered with respect to vacuum, called the absolute refractive index of the medium, represented as n2.\n",
      "- nm = Speed of light in air / Speed of light in the medium = c/v (9.7)\n",
      "\n",
      "GLOSSARY:\n",
      "- n21 = refractive index of medium 2 with respect to medium 1\n",
      "- v1 = speed of light in medium 1\n",
      "- v2 = speed of light in medium 2\n",
      "- n12 = refractive index of medium 1 with respect to medium 2\n",
      "- n2 = absolute refractive index of medium 2\n",
      "- nm = refractive index of the medium\n",
      "- c = Speed of light in air\n",
      "- v = Speed of light in the medium\n",
      "\n",
      "ENTITIES:\n",
      "- n21= Speed of light in medium 1 / Speed of light in medium 2 = v1/v2\n",
      "- n12= Speed of light in medium 2 / Speed of light in medium 1 = v2/v1\n",
      "- nm = Speed of light in air / Speed of light in the medium = c/v\n",
      "\n",
      "JUSTIFICATION: The refractive index between two mediums can be calculated using the ratio of the speed of light in each medium, or if one medium is air or vacuum, it can be calculated using the ratio of the speed of light in air to the speed of light in the other medium.\n",
      "================================================================================\n",
      "✅ Final Answer:\n",
      " To calculate the refractive index between two mediums, you can use the following formulas:\n",
      "\n",
      "1.  If you know the speed of light in both mediums, the refractive index of medium 2 with respect to medium 1 ($n_{21}$) is given by:\n",
      "    $n_{21} = \\frac{\\text{Speed of light in medium 1}}{\\text{Speed of light in medium 2}} = \\frac{v_1}{v_2}$.\n",
      "    This gives the ratio of the speed of light in medium 1 ($v_1$) to the speed of light in medium 2 ($v_2$).\n",
      "\n",
      "2.  Similarly, the refractive index of medium 1 with respect to medium 2 ($n_{12}$) is given by:\n",
      "    $n_{12} = \\frac{\\text{Speed of light in medium 2}}{\\text{Speed of light in medium 1}} = \\frac{v_2}{v_1}$.\n",
      "    This gives the ratio of the speed of light in medium 2 ($v_2$) to the speed of light in medium 1 ($v_1$).\n",
      "\n",
      "3.  If medium 1 is a vacuum or air, you can calculate the absolute refractive index of medium 2 ($n_m$) using:\n",
      "    $n_m = \\frac{\\text{Speed of light in air}}{\\text{Speed of light in the medium}} = \\frac{c}{v}$.\n",
      "    This gives the ratio of the speed of light in air ($c$) to the speed of light in the medium ($v$).\n",
      "\n",
      "**Additional Points:**\n",
      "\n",
      "*   The absolute refractive index is a common way to express the refractive properties of a material.\n",
      "*   Note that $n_{12} = \\frac{1}{n_{21}}$.\n",
      "================================================================================\n",
      "\n",
      "🔍 Supporting Chunks:\n",
      "------------------------------------------------------------\n",
      "🆔 005dc9ca-366e-4b5a-b00b-743ed89786f6 | 📄 Page: 15\n",
      "The refractive index can be linked to an important physical quantity,\n",
      "the relative speed of propagation of light in different media.\n",
      "\n",
      "It turns\n",
      "out that light propagates with different speeds in different media.\n",
      "\n",
      "Light\n",
      "travels fastest in vacuum with speed of 3×108 m s–1.  \n",
      "\n",
      "In air, the speed of\n",
      "light is only marginally less, compared to that in vacuum.\n",
      "\n",
      "It reduces\n",
      "considerably in glass or water.\n",
      "\n",
      "The value of the refractive index for a\n",
      "given pair of media depends upon the speed of light in the two media, as\n",
      "given below.\n",
      "\n",
      "\n",
      "Consider a ray of light travelling from medium 1 into medium 2, as\n",
      "shown in Fig.9.11.  \n",
      "\n",
      "Let v1 be the speed of light in medium 1 and v2 be the\n",
      "speed of light in medium 2.  \n",
      "\n",
      "The refractive index of medium 2 with respect\n",
      "to medium 1 is given by the ratio of the speed of light in medium 1 and\n",
      "the speed of light in medium 2.\n",
      "\n",
      "This is usually represented by the symbol\n",
      "n21.\n",
      "\n",
      "This can be expressed in an equation form as\n",
      "n21= Speed of light in medium 1 \n",
      "Speed of light in medium 2 = v\n",
      "v\n",
      "1\n",
      "2\n",
      "(9.5)\n",
      "\n",
      "\n",
      "By the same argument, the refractive index of medium\n",
      "1 with respect to medium 2 is represented as n12.\n",
      "\n",
      "It is given\n",
      "by\n",
      "n12= Speed of light in medium 2 \n",
      "Speed of light in medium 1 = v\n",
      "v\n",
      "2\n",
      "1\n",
      "(9.6)\n",
      "\n",
      "\n",
      "If medium 1 is vacuum or air, then the refractive index of medium 2\n",
      "is considered with respect to vacuum.  \n",
      "\n",
      "This is called the absolute refractive\n",
      "index of the medium.  \n",
      "\n",
      "It is simply represented as n2.\n",
      "------------------------------------------------------------\n",
      "🆔 ca42abcd-cfcb-4e09-819b-25a105102ea2 | 📄 Page: 16\n",
      "For example, kerosene having higher refractive\n",
      "index, is optically denser than water, although its mass density is less\n",
      "than water.\n",
      "\n",
      "\n",
      "Material\n",
      "Refractive\n",
      "Material\n",
      "Refractive\n",
      "medium\n",
      "index\n",
      "medium\n",
      "index\n",
      "Air\n",
      "1.0003\n",
      "Canada\n",
      "1.53\n",
      "Balsam\n",
      "Ice\n",
      "1.31\n",
      "Water\n",
      "1.33\n",
      "Rock salt\n",
      "1.54\n",
      "Alcohol\n",
      "1.36\n",
      "Kerosene\n",
      "1.44\n",
      "Carbon\n",
      "1.63\n",
      "disulphide\n",
      "Fused\n",
      "1.46\n",
      "quartz\n",
      "Dense\n",
      "1.65\n",
      "\n",
      "\n",
      "flint glass\n",
      "Turpentine\n",
      "1.47\n",
      "oil\n",
      "Ruby\n",
      "1.71\n",
      "Benzene\n",
      "1.50\n",
      "Sapphire\n",
      "1.77\n",
      "Crown\n",
      "1.52\n",
      "glass\n",
      "Diamond\n",
      " 2.42\n",
      "light in air and v is the speed of light in the medium, then, the refractive\n",
      "index of the medium nm is given by\n",
      "nm = \n",
      "Speed of light in air \n",
      "Speed of light in the medium = c\n",
      "v\n",
      "(9.7)\n",
      "\n",
      "\n",
      "The absolute refractive index of a medium is simply called its refractive\n",
      "index.\n",
      "\n",
      "The refractive index of several media is given in Table 9.3.\n",
      "\n",
      "From\n",
      "the Table you can know that the refractive index of water, nw = 1.33.\n",
      "\n",
      "\n",
      "This means that the ratio of the speed of light in air and the speed of\n",
      "light in water is equal to 1.33.  \n",
      "\n",
      "Similarly, the refractive index of crown\n",
      "glass, ng =1.52.\n",
      "\n",
      "Such data are helpful in many places.  \n",
      "\n",
      "However, you\n",
      "need not memorise the data.\n",
      "\n",
      "\n",
      "Table 9.3 Absolute refractive index of some material media\n",
      "More to Know!\n",
      "\n",
      "\n",
      "Reprint 2025-26\n",
      "------------------------------------------------------------\n",
      "🆔 6a239aff-1e18-4d52-9d89-7180c7027b10 | 📄 Page: 15\n",
      "It is given\n",
      "by\n",
      "n12= Speed of light in medium 2 \n",
      "Speed of light in medium 1 = v\n",
      "v\n",
      "2\n",
      "1\n",
      "(9.6)\n",
      "\n",
      "\n",
      "If medium 1 is vacuum or air, then the refractive index of medium 2\n",
      "is considered with respect to vacuum.  \n",
      "\n",
      "This is called the absolute refractive\n",
      "index of the medium.  \n",
      "\n",
      "It is simply represented as n2.\n",
      "\n",
      "If c is the speed of\n",
      "Figure 9.11\n",
      "Figure 9.11\n",
      "Figure 9.11\n",
      "Figure 9.11\n",
      "Figure 9.11\n",
      "Reprint 2025-26\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 5️⃣ Run query\n",
    "query = \"how to calculate refractive index between 2 mediums?\"  # replace as needed\n",
    "retrieved = retriever.invoke(query)\n",
    "relevant = [doc.page_content for doc in retrieved]\n",
    "\n",
    "# STEP 1 — Evidence + Selection (reasoning phase)\n",
    "reasoning_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are analyzing textbook excerpts to answer a question.\n",
    "\n",
    "📘 Question:\n",
    "{query}\n",
    "\n",
    "📚 Textbook Chunks:\n",
    "{relevant_docs}\n",
    "\n",
    "Instructions:\n",
    "1) Extract ALL key facts that directly address the question.\n",
    "   - Always include the main laws, rules, definitions, named results, and formulas.\n",
    "   - Also include any interpretation conventions needed to read/apply those items\n",
    "     (generic rule: if a sign/value/symbol implies a meaning or case, capture that mapping).\n",
    "   - Include any applicability constraints the source states (domains, ranges, bounds, validity conditions).\n",
    "   - Include any concise conceptual statement that explains what an item represents or relates\n",
    "     (e.g., “gives the relationship between …”, “is expressed as the ratio of …”).\n",
    "   - Keep numbers, symbols, equations, variable names, constants, and technical terms EXACTLY as written.\n",
    "   - Ignore only text that is clearly unrelated.\n",
    "\n",
    "2) Create a brief GLOSSARY for items used in those facts:\n",
    "   - List abbreviations, acronyms, symbols, variables, constants, or units that appear in the facts/equations.\n",
    "   - For each, give its meaning exactly as stated in the text (precise and minimal).\n",
    "   - If the text maps a sign/value to a meaning (e.g., “positive means …”), record that mapping once.\n",
    "\n",
    "3) Identify the list of ENTITIES that directly answer the question.\n",
    "   - Each entity should be one atomic item (law, rule, definition, condition, or equation).\n",
    "\n",
    "4) Provide a short JUSTIFICATION based only on the extracted facts.\n",
    "\n",
    "Return EXACTLY in this format:\n",
    "\n",
    "FACTS:\n",
    "- <fact1>\n",
    "- <fact2>\n",
    "...\n",
    "\n",
    "GLOSSARY:\n",
    "- <token> = <meaning>\n",
    "- <token> = <meaning>\n",
    "...\n",
    "\n",
    "ENTITIES:\n",
    "- <entity1>\n",
    "- <entity2>\n",
    "...\n",
    "\n",
    "JUSTIFICATION: <one short sentence based only on the facts>\n",
    "\"\"\")\n",
    "\n",
    "reasoning_chain = reasoning_prompt | llm\n",
    "reasoning_response = reasoning_chain.invoke({\n",
    "    \"query\": query,\n",
    "    \"relevant_docs\": relevant\n",
    "})\n",
    "\n",
    "print(\"🔍 Reasoning Phase:\\n\", reasoning_response.content)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# STEP 2 — Final Answer (self-contained items, adaptive formatting)\n",
    "answer_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a tutor writing a clear, textbook-style answer.\n",
    "\n",
    "📘 Question:\n",
    "{query}\n",
    "\n",
    "🔎 Reasoning Output:\n",
    "{reasoning}\n",
    "\n",
    "Goal:\n",
    "Return a polished answer that includes the core item(s) (e.g., formulas/definitions) PLUS the minimal context\n",
    "needed to understand/apply them. The style should fit the question.\n",
    "\n",
    "Formatting policy (choose ONE style):\n",
    "- If the question effectively asks for ONE core item (e.g., a single definition or formula),\n",
    "  write a compact PARAGRAPH (1-3 sentences). Include the equation inline and any essential\n",
    "  interpretation conventions or constraints in the same paragraph.\n",
    "- If there are 2-5 core items, use a clean NUMBERED LIST (1 level only; no sub-bullets unless unavoidable).\n",
    "- If the question names “laws/rules/steps” or there are >5 items, use a numbered list.\n",
    "\n",
    "Content rules (apply regardless of style):\n",
    "- For EACH core item:\n",
    "  • Include its formal statement/equation verbatim (symbols/notation unchanged).\n",
    "  • Add one brief generic interpretation (e.g., “gives the relationship between …”, “ratio of …”).\n",
    "  • Attach any essential interpretation conventions (e.g., sign/value → meaning) and applicability constraints.\n",
    "  • Inline the minimal meanings of symbols/variables/constants needed to read the item on its own\n",
    "    (use the provided glossary/facts; keep it brief).\n",
    "- DO NOT move essentials to “Additional Points.” Essentials must appear with the item.\n",
    "- Preserve all numbers, symbols, and precise terms from the reasoning facts/glossary.\n",
    "- Prefer exact phrasing from the excerpts for formal names and equations.\n",
    "- Avoid verbose sub-lists. One level only. Keep it crisp and readable.\n",
    "\n",
    "Additional Points (optional):\n",
    "- Add a section titled exactly: **Additional Points:**\n",
    "- Include 0–2 short bullets that are helpful but NOT required to interpret/apply the main items\n",
    "  (e.g., a common pitfall, a tiny tip, or a closely related consequence).\n",
    "- Do NOT duplicate content already stated in the main section.\n",
    "\n",
    "Special handling:\n",
    "- If the question looks like multiple-choice, clearly state the correct option(s) with a brief justification.\n",
    "- If a length is requested (e.g., “in N words”), match it (±10%) prioritizing facts from the excerpts.\n",
    "- If explicitly asked to compare/contrast, present a 2-column Markdown table.\n",
    "\n",
    "Return ONLY the final answer text (main section + the **Additional Points** section, if any).\n",
    "\"\"\")\n",
    "\n",
    "answer_chain = answer_prompt | llm\n",
    "answer_response = answer_chain.invoke({\n",
    "    \"query\": query,\n",
    "    \"reasoning\": reasoning_response.content\n",
    "})\n",
    "\n",
    "print(\"✅ Final Answer:\\n\", answer_response.content)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Expose results as true globals for later cells\n",
    "global final_answer_text, reasoning_text\n",
    "final_answer_text = answer_response.content\n",
    "reasoning_text    = reasoning_response.content\n",
    "\n",
    "# 🔎 Print supporting chunks for transparency\n",
    "print(\"\\n🔍 Supporting Chunks:\\n\" + \"-\" * 60)\n",
    "for doc in retrieved:\n",
    "    uid = doc.metadata.get(\"uuid\")\n",
    "    entry = uuid_lookup.get(uid)\n",
    "    if entry:\n",
    "        print(f\"🆔 {uid} | 📄 Page: {entry['page']}\")\n",
    "        print(entry[\"text\"] + \"\\n\" + \"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d338503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import syntok.segmenter as segmenter\n",
    "# import json\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain.output_parsers import PydanticOutputParser\n",
    "# from pydantic import BaseModel\n",
    "# from typing import List\n",
    "# from math import ceil\n",
    "\n",
    "# # 1️⃣ Sentence segmentation using syntok\n",
    "# sentence_candidates = []\n",
    "# sentence_lookup = {}\n",
    "# sid_counter = 1\n",
    "\n",
    "# for doc in retrieved:\n",
    "#     chunk_uuid = doc.metadata[\"uuid\"]\n",
    "#     page = doc.metadata[\"page\"]\n",
    "#     source = doc.metadata[\"source\"]\n",
    "#     chunk_text = doc.page_content\n",
    "\n",
    "#     # Break into paragraphs and sentences\n",
    "#     paragraphs = segmenter.analyze(chunk_text)\n",
    "#     for paragraph in paragraphs:\n",
    "#         for sentence in paragraph:\n",
    "#             sentence_text = \" \".join(token.value for token in sentence).strip()\n",
    "#             if not sentence_text:\n",
    "#                 continue\n",
    "\n",
    "#             sid = f\"s{sid_counter}\"\n",
    "#             sid_counter += 1\n",
    "\n",
    "#             entry = {\n",
    "#                 \"sid\": sid,\n",
    "#                 \"sentence\": sentence_text,\n",
    "#                 \"page\": page,\n",
    "#                 \"chunk_uuid\": chunk_uuid,\n",
    "#                 \"source\": source\n",
    "#             }\n",
    "\n",
    "#             sentence_candidates.append({\"sid\": sid, \"sentence\": sentence_text})\n",
    "#             sentence_lookup[sid] = entry\n",
    "\n",
    "# # 🔒 Save sentence metadata\n",
    "# with open(\"sentence_lookup.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(sentence_lookup, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# # 2️⃣ Schema for LLM response\n",
    "# class SupportIDs(BaseModel):\n",
    "#     ids: List[str]\n",
    "\n",
    "# parser = PydanticOutputParser(pydantic_object=SupportIDs)\n",
    "# format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# # 3️⃣ Prompt for justification\n",
    "# prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "# You are helping a student understand which textbook sentences support a given answer.\n",
    "\n",
    "# You are given:\n",
    "# - A user question\n",
    "# - An answer to that question\n",
    "# - A list of textbook sentences, each with a short ID\n",
    "\n",
    "# Your task is to identify the **minimum set** of sentence IDs that directly support the answer.\n",
    "\n",
    "# Only include:\n",
    "# - Sentences that clearly explain the final answer\n",
    "# - Definitions, laws, rules, or steps that are explicitly used in the answer\n",
    "\n",
    "# Do NOT include:\n",
    "# - General background statements\n",
    "# - Sentences that are only loosely related\n",
    "# - Sentences that repeat the same content with different wording\n",
    "\n",
    "# If multiple sentences say the same thing, prefer the **clearest and most complete** one.\n",
    "# If the same keyword is used is found in more than one place, use it only if it clearly is required to answer the answer accurately,\n",
    "# this doesn't include the introductory sentences that lead to the main point.\n",
    "\n",
    "# {format_instructions}\n",
    "\n",
    "# ---\n",
    "\n",
    "# Question: {query}\n",
    "# Answer: {answer}\n",
    "\n",
    "# Candidate Sentences:\n",
    "# {sentences}\n",
    "# \"\"\")\n",
    "\n",
    "# # 4️⃣ Process in batches\n",
    "# batch_size = 20\n",
    "# num_batches = ceil(len(sentence_candidates) / batch_size)\n",
    "# all_ids = []\n",
    "\n",
    "# for i in range(num_batches):\n",
    "#     batch = sentence_candidates[i * batch_size:(i + 1) * batch_size]\n",
    "#     formatted_sentences = \"\\n\".join(\n",
    "#         f\"- [sid: {s['sid']}] {s['sentence']}\" for s in batch\n",
    "#     )\n",
    "\n",
    "#     structured_chain = prompt | llm | parser\n",
    "\n",
    "#     try:\n",
    "#         result = structured_chain.invoke({\n",
    "#             \"query\": query,\n",
    "#             \"answer\": answer_response.content,   # ✅ use final two-step answer\n",
    "#             \"sentences\": formatted_sentences,\n",
    "#             \"format_instructions\": format_instructions\n",
    "#         })\n",
    "\n",
    "#         cleaned = [sid.replace(\"sid:\", \"\").strip() for sid in result.ids]\n",
    "#         all_ids.extend(cleaned)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"⚠️ Batch {i+1} failed: {e}\")\n",
    "#         continue\n",
    "\n",
    "# # ✅ Final cleanup\n",
    "# returned_sids = sorted(set(all_ids), key=lambda x: int(x[1:]))\n",
    "\n",
    "# # ✅ Output\n",
    "# print(\"\\n✅ Supporting Sentence IDs:\\n\" + \"=\" * 50)\n",
    "# print(returned_sids)\n",
    "\n",
    "# print(\"\\n📝 Sentences to Highlight:\\n\" + \"=\" * 50)\n",
    "# for sid in returned_sids:\n",
    "#     entry = sentence_lookup.get(sid)\n",
    "#     if entry:\n",
    "#         print(f\"{sid} (Page {entry['page']}): {entry['sentence']}\")\n",
    "\n",
    "# with open(\"highlight_ids.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(returned_sids, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b53628a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Supporting Sentence IDs (merged+minimal):\n",
      "==================================================\n",
      "['s6', 's8', 's9', 's11', 's12', 's13', 's14', 's15', 's16', 's19', 's20', 's29', 's30', 's31', 's32']\n",
      "\n",
      "📝 Sentences to Highlight:\n",
      "==================================================\n",
      "s6 (Page 15): The value of the refractive index for a given pair of media depends upon the speed of light in the two media , as given below .\n",
      "s8 (Page 15): Let v1 be the speed of light in medium 1 and v2 be the speed of light in medium 2 .\n",
      "s9 (Page 15): The refractive index of medium 2 with respect to medium 1 is given by the ratio of the speed of light in medium 1 and the speed of light in medium 2 .\n",
      "s11 (Page 15): This can be expressed in an equation form as n21 = Speed of light in medium 1 Speed of light in medium 2 = v v 1 2 ( 9.5 )\n",
      "s12 (Page 15): By the same argument , the refractive index of medium 1 with respect to medium 2 is represented as n12 .\n",
      "s13 (Page 15): It is given by n12 = Speed of light in medium 2 Speed of light in medium 1 = v v 2 1 ( 9.6 )\n",
      "s14 (Page 15): If medium 1 is vacuum or air , then the refractive index of medium 2 is considered with respect to vacuum .\n",
      "s15 (Page 15): This is called the absolute refractive index of the medium .\n",
      "s16 (Page 15): It is simply represented as n2 .\n",
      "s19 (Page 16): flint glass Turpentine 1.47 oil Ruby 1.71 Benzene 1.50 Sapphire 1.77 Crown 1.52 glass Diamond 2.42 light in air and v is the speed of light in the medium , then , the refractive index of the medium nm is given by nm = Speed of light in air Speed of light in the medium = c v ( 9.7 )\n",
      "s20 (Page 16): The absolute refractive index of a medium is simply called its refractive index .\n",
      "s29 (Page 15): It is given by n12 = Speed of light in medium 2 Speed of light in medium 1 = v v 2 1 ( 9.6 )\n",
      "s30 (Page 15): If medium 1 is vacuum or air , then the refractive index of medium 2 is considered with respect to vacuum .\n",
      "s31 (Page 15): This is called the absolute refractive index of the medium .\n",
      "s32 (Page 15): It is simply represented as n2 .\n"
     ]
    }
   ],
   "source": [
    "# ===== CELL 3 — v2's repair/merge + v1's minimal LLM selection (with parenthetical stitch) =====\n",
    "import syntok.segmenter as segmenter\n",
    "import json, re\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from math import ceil\n",
    "\n",
    "# --------- repair/merge logic (from your v2, unchanged except for a tiny generic addition) ---------\n",
    "OP_CHARS = r\"<>=±+\\-*/^:≈≠≤≥→∝\"\n",
    "OP_TAIL_RE    = re.compile(rf\"[{re.escape(OP_CHARS)}]\\s*$\")           # e.g., \"0 <\"\n",
    "OP_HEAD_RE    = re.compile(rf\"^\\s*[{re.escape(OP_CHARS)}]\")           # e.g., \"< i\"\n",
    "CLOSE_HEAD_RE = re.compile(r\"^\\s*[\\)\\]\\}.,;:]\")                        # e.g., \") then\"\n",
    "LOWVAR_HEAD   = re.compile(r\"^\\s*[a-z]\\b\", re.I)                       # e.g., \"i\", \"r\"\n",
    "ENUM_HEAD_RE  = re.compile(r\"^\\s*(\\(?[ivxlcdmIVXLCDM]+\\)|\\(?[a-zA-Z]\\)|\\d+[.)])\\s+\")  # (i), 1., a)\n",
    "\n",
    "# NEW: generic detectors for tiny stand-alone parenthetical/micro fragments\n",
    "PAREN_ONLY    = re.compile(r\"^\\(\\s*[^()]{1,12}\\s*\\)\\.?\\s*$\")           # e.g., \"( f )\", \"(h′)\", \"(9.8)\"\n",
    "# Treat as \"likely list head\" if prev ends with \":\" (we avoid merging (i) etc after colons)\n",
    "ENDS_WITH_COLON = re.compile(r\":\\s*$\")\n",
    "\n",
    "# NEW (minimal): detect a bare enumeration token like \"(ii)\", \"(b)\", \"(2)\"\n",
    "ENUM_PAREN_ONLY = re.compile(r\"^\\(\\s*(?:[ivxlcdmIVXLCDM]+|[a-zA-Z]|\\d+)\\s*\\)\\.?\\s*$\")\n",
    "\n",
    "def _norm_spaces(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "def unbalanced_paren_or_quote(s: str) -> bool:\n",
    "    return (s.count(\"(\") != s.count(\")\")) or (s.count('\"') % 2 != 0) or (s.count(\"'\") % 2 != 0)\n",
    "\n",
    "def is_parenthetical_fragment(s: str) -> bool:\n",
    "    \"\"\"Tiny stand-alone parenthetical like '( f )', '(h′)', '(9.8)'. Keeps variables/equation refs with prior line.\"\"\"\n",
    "    return bool(PAREN_ONLY.match(_norm_spaces(s)))\n",
    "\n",
    "def should_merge(prev: str, curr: str) -> bool:\n",
    "    \"\"\"Merge only when the break is clearly structural (paren/math), not normal prose.\"\"\"\n",
    "    if not prev or not curr:\n",
    "        return False\n",
    "    # --- existing rules ---\n",
    "    if ENUM_HEAD_RE.search(curr) and not (unbalanced_paren_or_quote(prev) or OP_TAIL_RE.search(prev)):\n",
    "        return False\n",
    "    if unbalanced_paren_or_quote(prev):\n",
    "        return True\n",
    "    if OP_TAIL_RE.search(prev) and (OP_HEAD_RE.search(curr) or CLOSE_HEAD_RE.search(curr) or LOWVAR_HEAD.search(curr)):\n",
    "        return True\n",
    "    if CLOSE_HEAD_RE.search(curr) and not re.search(r\"[.?!:]$\", prev.strip()):\n",
    "        return True\n",
    "    # --- NEW: stitch tiny parenthetical fragment into the previous sentence unless the previous ends with a colon\n",
    "    # This keeps things like \"( f )\" attached to \"... focal length\" while avoiding \"(i)\" after a list header \"The laws are:\"\n",
    "    if is_parenthetical_fragment(curr) and not ENDS_WITH_COLON.search(prev.strip()):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def repair_fragments(sentences: List[str]) -> List[str]:\n",
    "    \"\"\"Single L→R pass; minimal touch. Never deletes text.\n",
    "    Minimal tweak: if a standalone '(ii)'/'(b)'/'(2)' line is seen, carry it forward and prefix the next sentence.\n",
    "    \"\"\"\n",
    "    out: List[str] = []\n",
    "    pending_enum: str = \"\"  # holds a bare enumeration token to attach to the next sentence\n",
    "\n",
    "    for s in sentences:\n",
    "        s = _norm_spaces(s)\n",
    "        if not s:\n",
    "            continue\n",
    "\n",
    "        # If this line is ONLY an enumeration token like \"( ii )\", don't merge to previous; stash for the next sentence.\n",
    "        if ENUM_PAREN_ONLY.match(s):\n",
    "            pending_enum = s\n",
    "            continue\n",
    "\n",
    "        # If we have a pending enumeration token, prepend it to the current sentence (the \"next\" sentence).\n",
    "        if pending_enum:\n",
    "            s = f\"{pending_enum} {s}\"\n",
    "            pending_enum = \"\"\n",
    "\n",
    "        if out and should_merge(out[-1], s):\n",
    "            out[-1] = _norm_spaces(out[-1] + \" \" + s)\n",
    "        else:\n",
    "            out.append(s)\n",
    "\n",
    "    # If the very last item was a dangling enumeration token (no next sentence existed), keep it as its own line.\n",
    "    if pending_enum:\n",
    "        out.append(pending_enum)\n",
    "\n",
    "    # if something still unbalanced, greedily attach next until balanced\n",
    "    i, fixed = 0, []\n",
    "    while i < len(out):\n",
    "        cur = out[i]\n",
    "        if unbalanced_paren_or_quote(cur) and i + 1 < len(out):\n",
    "            combo = cur + \" \" + out[i+1]\n",
    "            i += 2\n",
    "            while i < len(out) and unbalanced_paren_or_quote(combo):\n",
    "                combo += \" \" + out[i]\n",
    "                i += 1\n",
    "            fixed.append(_norm_spaces(combo))\n",
    "        else:\n",
    "            fixed.append(cur)\n",
    "            i += 1\n",
    "    return fixed\n",
    "\n",
    "# --------- segment retrieved into repaired candidates ---------\n",
    "sentence_candidates = []   # [{sid, sentence}]\n",
    "sentence_lookup = {}       # sid -> full entry\n",
    "sid_counter = 1\n",
    "\n",
    "for doc in retrieved:\n",
    "    chunk_uuid = doc.metadata[\"uuid\"]\n",
    "    page = doc.metadata[\"page\"]\n",
    "    source = doc.metadata[\"source\"]\n",
    "    chunk_text = doc.page_content or \"\"\n",
    "\n",
    "    raw_sents: List[str] = []\n",
    "    paragraphs = segmenter.analyze(chunk_text)\n",
    "    for paragraph in paragraphs:\n",
    "        for sentence in paragraph:\n",
    "            sent_text = \" \".join(token.value for token in sentence).strip()\n",
    "            if sent_text:\n",
    "                raw_sents.append(sent_text)\n",
    "\n",
    "    fixed_sents = repair_fragments(raw_sents)\n",
    "\n",
    "    for sentence_text in fixed_sents:\n",
    "        sid = f\"s{sid_counter}\"\n",
    "        sid_counter += 1\n",
    "        entry = {\n",
    "            \"sid\": sid,\n",
    "            \"sentence\": sentence_text,\n",
    "            \"page\": page,\n",
    "            \"chunk_uuid\": chunk_uuid,\n",
    "            \"source\": source\n",
    "        }\n",
    "        sentence_candidates.append({\"sid\": sid, \"sentence\": sentence_text})\n",
    "        sentence_lookup[sid] = entry\n",
    "\n",
    "# 🔒 Save chunk-level sentence metadata\n",
    "with open(\"sentence_lookup.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sentence_lookup, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# --------- v1-style minimal LLM selection (unchanged logic) ---------\n",
    "class SupportIDs(BaseModel):\n",
    "    ids: List[str]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=SupportIDs)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are helping a student understand which textbook sentences support a given answer.\n",
    "\n",
    "You are given:\n",
    "- A user question\n",
    "- An answer to that question\n",
    "- A list of textbook sentences, each with a short ID\n",
    "\n",
    "Your task is to identify the **minimum set** of sentence IDs that directly support the answer.\n",
    "\n",
    "Only include:\n",
    "- Sentences that clearly explain the final answer\n",
    "- Definitions, laws, rules, or steps that are explicitly used in the answer\n",
    "- The answer might be a combination of different related sentences—include those too.\n",
    "- Ensure to include everything accurately required to completely and clearly answer the student's query.AIMessage\n",
    "- If any constraints are mentioned, or any point which is important to know for a particular concept include it.\n",
    "- Unless the query specifies to give only the main point, formula, law, etc. Then no need to include additional ending statements or introductory statements.\n",
    "\n",
    "Do NOT include:\n",
    "- General background statements\n",
    "- Sentences that are only loosely related\n",
    "- Sentences that repeat the same content with different wording\n",
    "\n",
    "If multiple sentences say the same thing, prefer the **clearest and most complete** one.\n",
    "If the same keyword is found in more than one place, use it only if it is clearly required for the answer (don’t keep introductions).\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {query}\n",
    "Answer: {answer}\n",
    "\n",
    "Candidate Sentences:\n",
    "{sentences}\n",
    "\"\"\")\n",
    "\n",
    "batch_size = 20\n",
    "num_batches = ceil(len(sentence_candidates) / batch_size)\n",
    "all_ids = []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    batch = sentence_candidates[i * batch_size:(i + 1) * batch_size]\n",
    "    formatted_sentences = \"\\n\".join(\n",
    "        f\"- [sid: {s['sid']}] {s['sentence']}\" for s in batch\n",
    "    )\n",
    "\n",
    "    structured_chain = prompt | llm | parser\n",
    "    try:\n",
    "        result = structured_chain.invoke({\n",
    "            \"query\": query,\n",
    "            \"answer\": answer_response.content,   # use final two-step answer\n",
    "            \"sentences\": formatted_sentences,\n",
    "            \"format_instructions\": format_instructions\n",
    "        })\n",
    "        cleaned = []\n",
    "        for tok in result.ids:\n",
    "            m = re.search(r\"(s\\d+)\", str(tok))\n",
    "            if m:\n",
    "                sid = m.group(1)\n",
    "                if sid in sentence_lookup:\n",
    "                    cleaned.append(sid)\n",
    "        all_ids.extend(cleaned)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Batch {i+1} failed: {e}\")\n",
    "        continue\n",
    "\n",
    "# ✅ Final cleanup (document order)\n",
    "keep = set(all_ids)\n",
    "returned_sids = [c[\"sid\"] for c in sentence_candidates if c[\"sid\"] in keep]\n",
    "\n",
    "# ✅ Output\n",
    "print(\"\\n✅ Supporting Sentence IDs (merged+minimal):\\n\" + \"=\" * 50)\n",
    "print(returned_sids)\n",
    "\n",
    "print(\"\\n📝 Sentences to Highlight:\\n\" + \"=\" * 50)\n",
    "for sid in returned_sids:\n",
    "    entry = sentence_lookup.get(sid)\n",
    "    if entry:\n",
    "        print(f\"{sid} (Page {entry['page']}): {entry['sentence']}\")\n",
    "\n",
    "with open(\"highlight_ids.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(returned_sids, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c1b86321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CELL 4 — coverage-aware selection from Cell 3 + safe cleaning (verbatim)\n",
    "# import json, re\n",
    "# from pathlib import Path\n",
    "# from typing import List\n",
    "# from pydantic import BaseModel\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "# base = Path(\".\")\n",
    "# sel_ids_path = base / \"highlight_ids.json\"       # from Cell 3\n",
    "# lookup_path  = base / \"sentence_lookup.json\"     # from Cell 3\n",
    "# clean_out    = base / \"ai_cleaned_sentences.json\"\n",
    "\n",
    "# # ---- load Cell 3 outputs\n",
    "# with open(sel_ids_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     selected_ids = json.load(f)\n",
    "\n",
    "# with open(lookup_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     sentence_lookup = json.load(f)\n",
    "\n",
    "# # keep original order coming from Cell 3\n",
    "# candidates = []\n",
    "# for sid in selected_ids:\n",
    "#     ent = sentence_lookup.get(sid)\n",
    "#     if not ent: \n",
    "#         continue\n",
    "#     candidates.append({\n",
    "#         \"sid\": sid,\n",
    "#         \"page\": int(ent[\"page\"]),\n",
    "#         \"sentence\": ent[\"sentence\"].strip()\n",
    "#     })\n",
    "\n",
    "# # ===== PASS 1: SELECT IDS THAT COVER THE FINAL ANSWER =====\n",
    "\n",
    "# class KeepIDs(BaseModel):\n",
    "#     ids: List[str]  # subset of provided sids, in doc order\n",
    "\n",
    "# select_parser = PydanticOutputParser(pydantic_object=KeepIDs)\n",
    "# select_instr = select_parser.get_format_instructions()\n",
    "\n",
    "# select_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "# You will choose the MINIMUM set of candidate sentences that collectively support the FINAL ANSWER.\n",
    "\n",
    "# Inputs:\n",
    "# - QUESTION: {query}\n",
    "# - FINAL ANSWER: {answer}\n",
    "\n",
    "# CANDIDATES (in document order):\n",
    "# {cands}\n",
    "\n",
    "# Selection rules:\n",
    "# - Pick ONLY from the listed IDs. Preserve the same order.\n",
    "# - Cover EVERY major claim in the Final Answer at least once (definition/identity, extent/locations, subdivisions or lists,\n",
    "#   causes/processes, consequences, examples, etc.).\n",
    "# - Prefer clear, specific sentences that directly *state* the claim used in the answer.\n",
    "# - Exclude headings, labels, page furniture, and fragmentary tokens (e.g., \"( b ) Himalayas\", single words, things ending with \":\").\n",
    "# - For short answers (~≤120 words): keep ~2–6 sentences.\n",
    "#   For long answers (~≥150–300 words): keep ~6–12 sentences.\n",
    "# - NEVER drop a sentence if it is the ONLY one that states a kept claim.\n",
    "\n",
    "# Return ONLY this JSON:\n",
    "# {format_instructions}\n",
    "# \"\"\")\n",
    "\n",
    "# def render_cands(cands):\n",
    "#     return \"\\n\".join(f\"- [sid: {c['sid']}] {c['sentence']}\" for c in cands)\n",
    "\n",
    "# select_chain = select_prompt | llm | select_parser\n",
    "# keep = select_chain.invoke({\n",
    "#     \"query\": query,\n",
    "#     \"answer\": answer_response.content,\n",
    "#     \"cands\": render_cands(candidates),\n",
    "#     \"format_instructions\": select_instr\n",
    "# }).ids\n",
    "\n",
    "# # keep IDs in the original order & intersect with available\n",
    "# keep_set = {sid for sid in keep if sid in {c[\"sid\"] for c in candidates}}\n",
    "# kept = [c for c in candidates if c[\"sid\"] in keep_set]\n",
    "\n",
    "# # ===== PASS 2: CLEAN THE KEPT SENTENCES SAFELY (NO PROSE FRACTURE) =====\n",
    "\n",
    "# # heuristics\n",
    "# BULLET_JOIN_RE = re.compile(r\"(?:\\s[•●▪‣■]\\s|\\sz\\s)\", re.I)  # split only when bullets are truly joined\n",
    "# NUM_LIST_RE    = re.compile(r\"\\b(?:\\(?[0-9]+|[ivxlcdm]+)\\)?[.)]\\s+\\S+\", re.I)\n",
    "# TAIL_RE = re.compile(\n",
    "#     r\"(?i)\\b(observe|write|group|classify|find out|according to|answer|project|activity|exercise|assignment|\"\n",
    "#     r\"try this|can you|let us|discuss|for example|figure|fig\\.|table|chart)\\b\"\n",
    "# )\n",
    "\n",
    "# def looks_joined(s: str) -> bool:\n",
    "#     return bool(BULLET_JOIN_RE.search(s)) or (len(re.findall(NUM_LIST_RE, s)) >= 2)\n",
    "\n",
    "# def trim_tail(s: str) -> str:\n",
    "#     m = TAIL_RE.search(s)\n",
    "#     return s[:m.start()].rstrip(\" .,:;–—-\") if m else s\n",
    "\n",
    "# def split_joined(s: str) -> List[str]:\n",
    "#     # turn ' z ' into ' • ' split marker without touching 'Brazil'\n",
    "#     t = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "#     t = re.sub(r\"^(?:z)\\s+\", \"• \", t)    # start\n",
    "#     t = re.sub(r\"\\s(z)\\s\", \" • \", t)     # middle\n",
    "#     parts = [p.strip(\" •-–—\") for p in t.split(\" • \")]\n",
    "#     parts = [p for p in parts if len(p.split()) >= 2]\n",
    "#     return parts if len(parts) >= 2 else [s.strip()]\n",
    "\n",
    "# def strip_leading_markers(s: str) -> str:\n",
    "#     s = re.sub(r\"^\\s*[•●▪‣■]\\s*\", \"\", s)                        # symbol bullet\n",
    "#     s = re.sub(r\"^\\s*(?:\\(?[0-9]+|[ivxlcdm]+)\\)?[.)-]\\s*\", \"\", s, flags=re.I)  # 1.  (i)  2)\n",
    "#     return s.strip()\n",
    "\n",
    "# def present_in_original(orig: str, piece: str) -> bool:\n",
    "#     if piece in orig: return True\n",
    "#     def norm(x: str):\n",
    "#         x = re.sub(r\"\\s*\\(\\s*\", \" (\", x)\n",
    "#         x = re.sub(r\"\\s*\\)\\s*\", \")\", x)\n",
    "#         x = re.sub(r\"\\s+\", \" \", x)\n",
    "#         return x.strip()\n",
    "#     return norm(piece) in norm(orig)\n",
    "\n",
    "# class CleanItem(BaseModel):\n",
    "#     sid: str\n",
    "#     pieces: List[str]  # verbatim substrings after allowed trims (order preserved)\n",
    "\n",
    "# class CleanBatch(BaseModel):\n",
    "#     items: List[CleanItem]\n",
    "\n",
    "# clean_parser = PydanticOutputParser(pydantic_object=CleanBatch)\n",
    "# clean_instr = clean_parser.get_format_instructions()\n",
    "\n",
    "# clean_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "# You will prepare the kept sentences for highlighting EXACTLY as they appear.\n",
    "\n",
    "# For each entry:\n",
    "# - If it is a normal prose sentence, return ONE piece (verbatim), only trimming an instruction tail if present.\n",
    "# - If it is a run-on bullet line (joined by \"•\" / \"z\" / numbered items), split into separate pieces (order preserved).\n",
    "# - Remove ONLY leading bullet/number markers from each piece.\n",
    "# - Keep parentheses that belong to the phrase.\n",
    "# - Do NOT rephrase or shorten normal sentences.\n",
    "# - Ignore headings/labels (e.g., items that are just a letter/number or end with \":\").\n",
    "\n",
    "# Return ONLY this JSON:\n",
    "# {format_instructions}\n",
    "\n",
    "# Entries:\n",
    "# {entries}\n",
    "# \"\"\")\n",
    "\n",
    "# # Build small cleaning batches with only kept items\n",
    "# BATCH = 16\n",
    "# kept_batches = [kept[i:i+BATCH] for i in range(0, len(kept), BATCH)]\n",
    "# clean_results = []  # [{\"sid\",\"page\",\"pieces\":[...]}]\n",
    "\n",
    "# for group in kept_batches:\n",
    "#     entries_text = \"\\n\".join(f\"- [sid: {e['sid']}] {e['sentence']}\" for e in group)\n",
    "#     chain = clean_prompt | llm | clean_parser\n",
    "#     out = chain.invoke({\n",
    "#         \"entries\": entries_text,\n",
    "#         \"format_instructions\": clean_instr\n",
    "#     })\n",
    "\n",
    "#     # guard-rails: verify verbatim & fall back for prose\n",
    "#     page_map = {e[\"sid\"]: e[\"page\"] for e in group}\n",
    "#     orig_map = {e[\"sid\"]: e[\"sentence\"] for e in group}\n",
    "\n",
    "#     for item in out.items:\n",
    "#         sid = item.sid\n",
    "#         orig = orig_map.get(sid, \"\")\n",
    "#         page = page_map.get(sid, 1)\n",
    "\n",
    "#         # decide prose vs joined using the original text (not AI output)\n",
    "#         prose = not looks_joined(orig)\n",
    "\n",
    "#         if prose:\n",
    "#             # one piece: trim tail; if AI gave >1 or tiny fragments, fall back to full trimmed sentence\n",
    "#             cand = item.pieces[0] if item.pieces else orig\n",
    "#             cand = strip_leading_markers(trim_tail(cand)).strip() or strip_leading_markers(trim_tail(orig))\n",
    "#             # minimal presence/length check\n",
    "#             if not present_in_original(orig, cand) or len(cand.split()) < max(3, int(0.4 * len(orig.split()))):\n",
    "#                 cand = strip_leading_markers(trim_tail(orig))\n",
    "#             clean_results.append({\"sid\": sid, \"page\": page, \"pieces\": [cand]})\n",
    "#         else:\n",
    "#             # run-on bullets: split locally too (safety), then verify each piece\n",
    "#             ai_pieces = item.pieces if item.pieces else split_joined(orig)\n",
    "#             pieces = []\n",
    "#             for p in ai_pieces:\n",
    "#                 p2 = strip_leading_markers(trim_tail(p)).strip()\n",
    "#                 if p2 and present_in_original(orig, p2):\n",
    "#                     pieces.append(p2)\n",
    "#             if not pieces:\n",
    "#                 pieces = [strip_leading_markers(trim_tail(orig))]\n",
    "#             clean_results.append({\"sid\": sid, \"page\": page, \"pieces\": pieces})\n",
    "\n",
    "# # ---- write\n",
    "# with open(clean_out, \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(clean_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(f\"✅ Kept {len(kept)} ids; prepared {sum(len(r['pieces']) for r in clean_results)} highlight pieces → {clean_out.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "306dd156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Kept 11 ids; prepared 11 highlight pieces → ai_cleaned_sentences.json\n"
     ]
    }
   ],
   "source": [
    "# CELL 4 — coverage-aware selection + safe cleaning (balanced refinement)\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "base = Path(\".\")\n",
    "sel_ids_path = base / \"highlight_ids.json\"       # from Cell 3\n",
    "lookup_path  = base / \"sentence_lookup.json\"     # from Cell 3\n",
    "clean_out    = base / \"ai_cleaned_sentences.json\"\n",
    "\n",
    "# ---- load Cell 3 outputs\n",
    "with open(sel_ids_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    selected_ids = json.load(f)\n",
    "\n",
    "with open(lookup_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    sentence_lookup = json.load(f)\n",
    "\n",
    "candidates = []\n",
    "allowed = set(selected_ids)\n",
    "for sid in selected_ids:\n",
    "    ent = sentence_lookup.get(sid)\n",
    "    if not ent:\n",
    "        continue\n",
    "    candidates.append({\n",
    "        \"sid\": sid,\n",
    "        \"page\": int(ent[\"page\"]),\n",
    "        \"sentence\": (ent[\"sentence\"] or \"\").strip()\n",
    "    })\n",
    "\n",
    "# Get the final answer text\n",
    "try:\n",
    "    answer_text = answer\n",
    "except NameError:\n",
    "    try:\n",
    "        answer_text = answer_response.content\n",
    "    except NameError:\n",
    "        try:\n",
    "            answer_text = final_answer_text\n",
    "        except NameError:\n",
    "            answer_text = \"\"\n",
    "\n",
    "# ---- intent phrase (neutral unless the query clearly asks for relative or absolute) ----\n",
    "def _intent_phrase(q: str, a: str) -> str:\n",
    "    s = f\"{q or ''} {a or ''}\".lower()\n",
    "    if re.search(r\"\\b(between\\s+(two|2)\\s+(media|mediums)|relative refractive|with respect to|n\\s*21|n\\s*12|v1|v2)\\b\", s):\n",
    "        return \"between two media → favor relative forms (n21 = v1/v2 and n12 = v2/v1); keep both reciprocals\"\n",
    "    if re.search(r\"\\b(absolute|vacuum|air|c\\s*/\\s*v|nm\\b)\\b\", s):\n",
    "        return \"with respect to vacuum/air → include absolute form (nm = c/v) when relevant\"\n",
    "    return \"neutral (no preference)\"\n",
    "\n",
    "# ===== PASS 1: CONTEXT-AWARE SELECTION =====\n",
    "class KeepIDs(BaseModel):\n",
    "    ids: List[str]\n",
    "\n",
    "select_parser = PydanticOutputParser(pydantic_object=KeepIDs)\n",
    "select_instr = select_parser.get_format_instructions()\n",
    "\n",
    "def render_cands(cands):\n",
    "    return \"\\n\".join(f\"- [sid: {c['sid']}] (page {c['page']}) {c['sentence']}\" for c in cands)\n",
    "\n",
    "kept = []\n",
    "if candidates:\n",
    "    select_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You will choose the set of candidate sentences that best support the FINAL ANSWER.\n",
    "\n",
    "    Inputs:\n",
    "    - QUESTION: {query}\n",
    "    - FINAL ANSWER: {answer}\n",
    "    - INTENT: {intent_phrase}\n",
    "\n",
    "    CANDIDATES (document order, with page numbers):\n",
    "    {cands}\n",
    "\n",
    "    Selection rules:\n",
    "    - Keep ALL sentences that directly support or clarify the Final Answer.\n",
    "    - INCLUDE useful introductory context if it frames numbered laws/definitions (e.g., “The following are the laws of …”).\n",
    "    - Prefer canonical formulations (laws/definitions/rules/equations).\n",
    "    - Read the INTENT:\n",
    "      • If it says \"between two media\", favor **relative refractive index** statements and **keep both reciprocal forms** (n21 = v1/v2 and n12 = v2/v1).\n",
    "      • If it says \"with respect to vacuum/air\", allow **absolute refractive index** (nm = c/v) when relevant.\n",
    "      • If it says \"neutral\", apply no preference.\n",
    "    - Remove only:\n",
    "      • Exact duplicates of the same sentence (from chunking)\n",
    "      • Obvious page furniture (headers/footers like “Science”, “Figure…”, “Table…”)\n",
    "      • Irrelevant reminders like “Let us recall…”, “Remember that…”\n",
    "    - Do NOT remove sentences just to reduce the count.\n",
    "    - Preserve order.\n",
    "\n",
    "    Return ONLY this JSON:\n",
    "    {format_instructions}\n",
    "    \"\"\")\n",
    "\n",
    "    try:\n",
    "        keep_ids = select_prompt | llm | select_parser\n",
    "        keep_ids = keep_ids.invoke({\n",
    "            \"query\": query,\n",
    "            \"answer\": answer_text,\n",
    "            \"cands\": render_cands(candidates),\n",
    "            \"format_instructions\": select_instr,\n",
    "            \"intent_phrase\": _intent_phrase(query, answer_text),\n",
    "        }).ids\n",
    "    except Exception:\n",
    "        keep_ids = []\n",
    "\n",
    "    keep_set = set(i for i in keep_ids if i in allowed)\n",
    "    kept = [c for c in candidates if c[\"sid\"] in keep_set]\n",
    "\n",
    "# ===== Fallback if LLM fails =====\n",
    "if not kept:\n",
    "    def _norm_tokens(s: str):\n",
    "        s = re.sub(r\"[^\\w\\s]\", \" \", s.lower())\n",
    "        return set(w for w in s.split() if len(w) > 2)\n",
    "\n",
    "    INTRO_PAT = re.compile(r\"^(let us recall|remember that|you are already familiar)\\b\", re.I)\n",
    "    FURNITURE_PAT = re.compile(r\"\\b(science|figure|fig\\.|table|chart|exercise|activity)\\b\", re.I)\n",
    "\n",
    "    ans_toks = _norm_tokens(answer_text)\n",
    "    scored = []\n",
    "    for c in candidates:\n",
    "        if INTRO_PAT.search(c[\"sentence\"]) or FURNITURE_PAT.search(c[\"sentence\"]):\n",
    "            continue\n",
    "        toks = _norm_tokens(c[\"sentence\"])\n",
    "        overlap = len(ans_toks & toks)\n",
    "        scored.append((overlap, c[\"page\"], c))\n",
    "    scored.sort(key=lambda x: (-x[0], x[1]))\n",
    "    kept = [t[2] for t in scored[:max(6, len(candidates)//2)]] or candidates[:2]\n",
    "\n",
    "# ===== PASS 2: CLEANING (unchanged) =====\n",
    "BULLET_JOIN_RE = re.compile(r\"(?:\\s[•●▪‣■]\\s|\\sz\\s)\", re.I)\n",
    "NUM_LIST_RE    = re.compile(r\"\\b(?:\\(?[0-9]+|[ivxlcdm]+)\\)?[.)]\\s+\\S+\", re.I)\n",
    "TAIL_RE = re.compile(\n",
    "    r\"(?i)\\b(observe|write|group|classify|find out|according to|answer|project|exercise|assignment|\"\n",
    "    r\"try this|can you|let us|discuss|for example|figure|fig\\.|table|chart|science)\\b\"\n",
    ")\n",
    "\n",
    "def looks_joined(s: str) -> bool:\n",
    "    return bool(BULLET_JOIN_RE.search(s)) or (len(re.findall(NUM_LIST_RE, s)) >= 2)\n",
    "\n",
    "def trim_tail(s: str) -> str:\n",
    "    m = TAIL_RE.search(s)\n",
    "    return s[:m.start()].rstrip(\" .,:;–—-\") if m else s\n",
    "\n",
    "def split_joined(s: str) -> List[str]:\n",
    "    t = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    t = re.sub(r\"^(?:z)\\s+\", \"• \", t)\n",
    "    t = re.sub(r\"\\s(z)\\s\", \" • \", t)\n",
    "    parts = [p.strip(\" •-–—\") for p in t.split(\" • \")]\n",
    "    parts = [p for p in parts if len(p.split()) >= 2]\n",
    "    return parts if len(parts) >= 2 else [s.strip()]\n",
    "\n",
    "def strip_leading_markers(s: str) -> str:\n",
    "    s = re.sub(r\"^\\s*[•●▪‣■]\\s*\", \"\", s)\n",
    "    s = re.sub(r\"^\\s*(?:\\(?[0-9]+|[ivxlcdm]+)\\)?[.)-]\\s*\", \"\", s, flags=re.I)\n",
    "    return s.strip()\n",
    "\n",
    "def present_in_original(orig: str, piece: str) -> bool:\n",
    "    if piece in orig: return True\n",
    "    def norm(x: str):\n",
    "        x = re.sub(r\"\\s*\\(\\s*\", \" (\", x)\n",
    "        x = re.sub(r\"\\s*\\)\\s*\", \")\", x)\n",
    "        x = re.sub(r\"\\s+\", \" \", x)\n",
    "        return x.strip()\n",
    "    return norm(piece) in norm(orig)\n",
    "\n",
    "class CleanItem(BaseModel):\n",
    "    sid: str\n",
    "    pieces: List[str]\n",
    "\n",
    "class CleanBatch(BaseModel):\n",
    "    items: List[CleanItem]\n",
    "\n",
    "clean_parser = PydanticOutputParser(pydantic_object=CleanBatch)\n",
    "clean_instr = clean_parser.get_format_instructions()\n",
    "\n",
    "clean_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You will prepare the kept sentences for highlighting EXACTLY as they appear.\n",
    "\n",
    "For each entry:\n",
    "- If it is a normal prose sentence, return ONE piece (verbatim), only trimming an instruction tail if present.\n",
    "- If it is a run-on bullet line (joined by \"•\" / \"z\" / numbered items), split into separate pieces (order preserved).\n",
    "- Remove ONLY leading bullet/number markers from each piece.\n",
    "- Keep parentheses that belong to the phrase.\n",
    "- Do NOT rephrase or shorten normal sentences.\n",
    "- Ignore headings/labels or page furniture (e.g., \"Science\", \"Exercise\", \"Figure\", \"Table\").\n",
    "Return ONLY this JSON:\n",
    "{format_instructions}\n",
    "\n",
    "Entries:\n",
    "{entries}\n",
    "\"\"\")\n",
    "\n",
    "BATCH = 16\n",
    "kept_batches = [kept[i:i+BATCH] for i in range(0, len(kept), BATCH)]\n",
    "clean_results = []\n",
    "\n",
    "for group in kept_batches:\n",
    "    entries_text = \"\\n\".join(f\"- [sid: {e['sid']}] {e['sentence']}\" for e in group)\n",
    "    chain = clean_prompt | llm | clean_parser\n",
    "    out = chain.invoke({\n",
    "        \"entries\": entries_text,\n",
    "        \"format_instructions\": clean_instr\n",
    "    })\n",
    "\n",
    "    page_map = {e[\"sid\"]: e[\"page\"] for e in group}\n",
    "    orig_map = {e[\"sid\"]: e[\"sentence\"] for e in group}\n",
    "\n",
    "    for item in out.items:\n",
    "        sid = item.sid\n",
    "        orig = orig_map.get(sid, \"\")\n",
    "        page = page_map.get(sid, 1)\n",
    "\n",
    "        prose = not looks_joined(orig)\n",
    "\n",
    "        if prose:\n",
    "            cand = item.pieces[0] if item.pieces else orig\n",
    "            cand = strip_leading_markers(trim_tail(cand)).strip() or strip_leading_markers(trim_tail(orig))\n",
    "            if not present_in_original(orig, cand) or len(cand.split()) < max(3, int(0.4 * len(orig.split()))):\n",
    "                cand = strip_leading_markers(trim_tail(orig))\n",
    "            clean_results.append({\"sid\": sid, \"page\": page, \"pieces\": [cand]})\n",
    "        else:\n",
    "            ai_pieces = item.pieces if item.pieces else split_joined(orig)\n",
    "            pieces = []\n",
    "            for p in ai_pieces:\n",
    "                p2 = strip_leading_markers(trim_tail(p)).strip()\n",
    "                if p2 and present_in_original(orig, p2):\n",
    "                    pieces.append(p2)\n",
    "            if not pieces:\n",
    "                pieces = [strip_leading_markers(trim_tail(orig))]\n",
    "            clean_results.append({\"sid\": sid, \"page\": page, \"pieces\": pieces})\n",
    "\n",
    "# ---- write\n",
    "with open(clean_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(clean_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Kept {len(kept)} ids; prepared {sum(len(r['pieces']) for r in clean_results)} highlight pieces → {clean_out.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1827c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CELL 5 — highlight using sliding-window matcher with hyphen-aware tokenization\n",
    "# import fitz  # PyMuPDF\n",
    "# import json\n",
    "# import re\n",
    "# import unicodedata\n",
    "# from pathlib import Path\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # 📁 Paths\n",
    "# base_dir = Path(\".\")\n",
    "# pdf_in = base_dir / \"geography.pdf\"\n",
    "# pdf_out = base_dir / \"highlighted_output.pdf\"\n",
    "\n",
    "# # Primary input: AI-cleaned pieces from Cell 4\n",
    "# clean_in = base_dir / \"ai_cleaned_sentences.json\"\n",
    "\n",
    "# # Fallback (if you skip Cell 4): original ids + lookup from Cell 3\n",
    "# ids_path = base_dir / \"highlight_ids.json\"\n",
    "# lookup_path = base_dir / \"sentence_lookup.json\"\n",
    "\n",
    "# # 🔤 Hyphen/ligature/space normalization → tokens\n",
    "# HYPHENS = r\"[\\-\\u2010-\\u2015\\u2212]\"  # -, ‐ - ‒ – — −\n",
    "# def norm_tokenize(s: str):\n",
    "#     # NFKC to unify glyphs; drop soft hyphen; turn hyphens into spaces (so \"east-flowing\" -> \"east flowing\")\n",
    "#     s = unicodedata.normalize(\"NFKC\", s).replace(\"\\u00ad\", \"\")\n",
    "#     s = re.sub(HYPHENS, \" \", s)\n",
    "#     s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "#     s = re.sub(r\"\\s+\", \" \", s)\n",
    "#     s = s.strip().lower()\n",
    "#     return s.split() if s else []\n",
    "\n",
    "# # Parentheses variants for fallback search\n",
    "# def paren_variants(s: str):\n",
    "#     v1 = s\n",
    "#     v2 = re.sub(r\"\\s*\\(\\s*\", \" (\", v1)\n",
    "#     v2 = re.sub(r\"\\s*\\)\\s*\", \")\", v2)\n",
    "#     v3 = re.sub(r\"[()]\", \"\", v2)\n",
    "#     v3 = re.sub(r\"\\s+\", \" \", v3).strip()\n",
    "#     return [v1, v2, v3]\n",
    "\n",
    "# # Load inputs\n",
    "# targets = []  # {\"page\": int, \"text\": str}\n",
    "# if clean_in.exists():\n",
    "#     data = json.loads(clean_in.read_text(encoding=\"utf-8\"))\n",
    "#     for row in data:\n",
    "#         page = int(row[\"page\"])\n",
    "#         for piece in row.get(\"pieces\", []):\n",
    "#             piece = piece.strip()\n",
    "#             if piece:\n",
    "#                 targets.append({\"page\": page, \"text\": piece})\n",
    "# else:\n",
    "#     highlight_ids = json.loads(ids_path.read_text(encoding=\"utf-8\"))\n",
    "#     sentence_lookup = json.loads(lookup_path.read_text(encoding=\"utf-8\"))\n",
    "#     for sid in highlight_ids:\n",
    "#         entry = sentence_lookup.get(sid)\n",
    "#         if entry:\n",
    "#             targets.append({\"page\": int(entry[\"page\"]), \"text\": entry[\"sentence\"]})\n",
    "\n",
    "# # Group per page\n",
    "# by_page = defaultdict(list)\n",
    "# for t in targets:\n",
    "#     by_page[int(t[\"page\"])].append(t[\"text\"])\n",
    "\n",
    "# doc = fitz.open(pdf_in)\n",
    "# total_spans, misses = 0, []\n",
    "\n",
    "# for page_num, pieces in sorted(by_page.items()):\n",
    "#     page = doc[page_num - 1]\n",
    "\n",
    "#     # words: [x0, y0, x1, y1, \"word\", block_no, line_no, word_no]  (older builds may have only first 5)\n",
    "#     words = page.get_text(\"words\")\n",
    "#     has_blocks = len(words[0]) >= 8 if words else False\n",
    "\n",
    "#     # Build block -> word list\n",
    "#     blocks = defaultdict(list)\n",
    "#     if has_blocks:\n",
    "#         for w in words:\n",
    "#             blocks[w[5]].append(w)\n",
    "#     else:\n",
    "#         blocks[-1] = words\n",
    "\n",
    "#     for piece in pieces:\n",
    "#         target_tokens = norm_tokenize(piece)\n",
    "#         if not target_tokens:\n",
    "#             continue\n",
    "\n",
    "#         found = False\n",
    "\n",
    "#         # Try block-scoped sliding-window with hyphen-aware tokenization\n",
    "#         for b_idx, wlist in blocks.items():\n",
    "#             if not wlist:\n",
    "#                 continue\n",
    "\n",
    "#             # Build token stream with a map back to word indices\n",
    "#             tok_stream = []     # list of tokens\n",
    "#             tok2word   = []     # parallel list of word indices\n",
    "#             for wi, w in enumerate(wlist):\n",
    "#                 w_tokens = norm_tokenize(w[4])  # may yield multiple tokens if word had hyphen\n",
    "#                 for _t in w_tokens:\n",
    "#                     tok_stream.append(_t)\n",
    "#                     tok2word.append(wi)\n",
    "\n",
    "#             n, m = len(tok_stream), len(target_tokens)\n",
    "#             if m == 0 or n < m:\n",
    "#                 continue\n",
    "\n",
    "#             for i in range(n - m + 1):\n",
    "#                 if tok_stream[i:i+m] == target_tokens:\n",
    "#                     # Map token span back to word rectangles (unique word indices in order)\n",
    "#                     word_idx_span = sorted(set(tok2word[i:i+m]))\n",
    "#                     rects = []\n",
    "#                     for idx in word_idx_span:\n",
    "#                         x0, y0, x1, y1 = wlist[idx][0:4]\n",
    "#                         rects.append(fitz.Rect(x0, y0, x1, y1))\n",
    "#                     for r in rects:\n",
    "#                         page.add_highlight_annot(r)\n",
    "#                         total_spans += 1\n",
    "#                     found = True\n",
    "#                     break\n",
    "#             if found:\n",
    "#                 break\n",
    "\n",
    "#         if not found:\n",
    "#             # Fallback: page.search_for with parentheses & hyphen variants\n",
    "#             # Try with original, then with hyphens normalized to spaces\n",
    "#             tried = False\n",
    "#             for variant in paren_variants(piece):\n",
    "#                 # replace hyphens with spaces in search text for safety\n",
    "#                 v = unicodedata.normalize(\"NFKC\", variant).replace(\"\\u00ad\", \"\")\n",
    "#                 v = re.sub(HYPHENS, \" \", v)\n",
    "#                 v = re.sub(r\"\\s+\", \" \", v).strip()\n",
    "#                 if not v:\n",
    "#                     continue\n",
    "#                 quads = page.search_for(v, quads=True)\n",
    "#                 tried = True\n",
    "#                 if quads:\n",
    "#                     for q in quads:\n",
    "#                         page.add_highlight_annot(q)\n",
    "#                         total_spans += 1\n",
    "#                     found = True\n",
    "#                     break\n",
    "\n",
    "#             if not found:\n",
    "#                 misses.append(piece[:90])\n",
    "\n",
    "# doc.save(pdf_out, garbage=4, deflate=True)\n",
    "# doc.close()\n",
    "\n",
    "# print(f\"✅ Highlighted {total_spans} spans → {pdf_out}\")\n",
    "# if misses:\n",
    "#     print(\"⚠️ Not found (sample):\", misses[:6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30b59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Highlighted 249 spans → highlighted_output.pdf\n"
     ]
    }
   ],
   "source": [
    "# # CELL 5 — robust highlighter (hyphen/quote/paren aware + tolerant fallback)\n",
    "# import fitz  # PyMuPDF\n",
    "# import json\n",
    "# import re\n",
    "# import unicodedata\n",
    "# from math import ceil\n",
    "# from pathlib import Path\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # ---------- paths\n",
    "# base_dir = Path(\".\")\n",
    "# # pdf_in   = base_dir / \"geography.pdf\"\n",
    "# # pdf_in   = base_dir / \"biology.pdf\"\n",
    "# # pdf_in   = base_dir / \"class10phy.pdf\"\n",
    "# # pdf_in   = base_dir / \"history.pdf\"\n",
    "\n",
    "# pdf_out  = base_dir / \"highlighted_output.pdf\"\n",
    "# clean_in = base_dir / \"ai_cleaned_sentences.json\"     # from Cell 4\n",
    "# ids_path = base_dir / \"highlight_ids.json\"            # fallback if Cell 4 is skipped\n",
    "# lookup_path = base_dir / \"sentence_lookup.json\"       # fallback\n",
    "\n",
    "# # ---------- normalization / tokenization\n",
    "\n",
    "# HYPHENS = r\"[\\-\\u2010-\\u2015\\u2212]\"  # -, ‐, -, ‒, –, —, −\n",
    "# QUOTES  = \"[\\u2018\\u2019\\u201A\\u2032\\u2035\\u201C\\u201D\\u201E\\u2033\\u2036']\"\n",
    "\n",
    "# def fold_diacritics(s: str) -> str:\n",
    "#     # keep base letters; strip combining marks\n",
    "#     s = unicodedata.normalize(\"NFD\", s)\n",
    "#     s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "#     return unicodedata.normalize(\"NFC\", s)\n",
    "\n",
    "# def norm_tokenize(s: str):\n",
    "#     # unify glyphs, strip soft hyphen, treat hyphens/dashes as spaces, normalize quotes to spaces\n",
    "#     s = unicodedata.normalize(\"NFKC\", s).replace(\"\\u00ad\", \"\")\n",
    "#     s = fold_diacritics(s)\n",
    "#     s = re.sub(HYPHENS, \" \", s)\n",
    "#     s = re.sub(QUOTES, \" \", s)\n",
    "#     s = re.sub(r\"[^\\w\\s]\", \" \", s)        # drop remaining punctuation\n",
    "#     s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
    "#     return s.split() if s else []\n",
    "\n",
    "# def paren_variants(s: str):\n",
    "#     v1 = s\n",
    "#     v2 = re.sub(r\"\\s*\\(\\s*\", \" (\", v1)\n",
    "#     v2 = re.sub(r\"\\s*\\)\\s*\", \")\", v2)\n",
    "#     v3 = re.sub(r\"[()]\", \"\", v2)\n",
    "#     v3 = re.sub(r\"\\s+\", \" \", v3).strip()\n",
    "#     return [v1, v2, v3]\n",
    "\n",
    "# # ---------- utilities\n",
    "\n",
    "# def build_blocks(page):\n",
    "#     \"\"\"Return {block_idx: [word tuples]} using page.get_text('words').\"\"\"\n",
    "#     words = page.get_text(\"words\")  # (x0,y0,x1,y1,text,block_no,line_no,word_no) on most builds\n",
    "#     if not words:\n",
    "#         return {-1: []}\n",
    "#     has_blocks = len(words[0]) >= 8\n",
    "#     blocks = defaultdict(list)\n",
    "#     if has_blocks:\n",
    "#         for w in words:\n",
    "#             blocks[w[5]].append(w)\n",
    "#     else:\n",
    "#         blocks[-1] = words\n",
    "#     return blocks\n",
    "\n",
    "# def stream_from_words(wlist):\n",
    "#     \"\"\"Token stream from a block's word list, mapping back to word indices.\"\"\"\n",
    "#     stream, tok2word = [], []\n",
    "#     for wi, w in enumerate(wlist):\n",
    "#         toks = norm_tokenize(w[4])  # hyphen-aware; might yield multiple tokens\n",
    "#         for t in toks:\n",
    "#             stream.append(t)\n",
    "#             tok2word.append(wi)\n",
    "#     return stream, tok2word\n",
    "\n",
    "# def rects_from_span(wlist, tok2word, i0, i1):\n",
    "#     \"\"\"Rectangles covering unique word indices in token window [i0, i1] inclusive.\"\"\"\n",
    "#     word_idx_span = sorted(set(tok2word[i0:i1+1]))\n",
    "#     return [fitz.Rect(*wlist[idx][0:4]) for idx in word_idx_span]\n",
    "\n",
    "# def exact_window(stream, target):\n",
    "#     \"\"\"Exact contiguous match. Returns (i0, i1) or None.\"\"\"\n",
    "#     n, m = len(stream), len(target)\n",
    "#     if m == 0 or n < m:\n",
    "#         return None\n",
    "#     for i in range(n - m + 1):\n",
    "#         if stream[i:i+m] == target:\n",
    "#             return (i, i+m-1)\n",
    "#     return None\n",
    "\n",
    "# def tolerant_window(stream, target, max_miss=1, min_ratio=0.9):\n",
    "#     \"\"\"Allow a small # of token mismatches for long targets (>= 10 tokens).\"\"\"\n",
    "#     n, m = len(stream), len(target)\n",
    "#     if m == 0 or n < m:\n",
    "#         return None\n",
    "#     need = max(int(ceil(min_ratio * m)), m - max_miss)\n",
    "#     best = None\n",
    "#     for i in range(n - m + 1):\n",
    "#         win = stream[i:i+m]\n",
    "#         matches = sum(1 for a, b in zip(win, target) if a == b)\n",
    "#         if matches >= need:\n",
    "#             # tighten to left/right actual match extent (approx): choose contiguous hits\n",
    "#             # simple approach: use the full window mapping; it's fine for highlighting\n",
    "#             best = (i, i+m-1)\n",
    "#             break\n",
    "#     return best\n",
    "\n",
    "# # ---------- load targets\n",
    "\n",
    "# targets = []  # {\"page\": int, \"text\": str}\n",
    "\n",
    "# if clean_in.exists():\n",
    "#     data = json.loads(clean_in.read_text(encoding=\"utf-8\"))\n",
    "#     for row in data:\n",
    "#         page = int(row[\"page\"])\n",
    "#         for piece in row.get(\"pieces\", []):\n",
    "#             piece = piece.strip()\n",
    "#             if piece:\n",
    "#                 targets.append({\"page\": page, \"text\": piece})\n",
    "# else:\n",
    "#     # fallback to raw ids\n",
    "#     ids = json.loads(ids_path.read_text(encoding=\"utf-8\"))\n",
    "#     lookup = json.loads(lookup_path.read_text(encoding=\"utf-8\"))\n",
    "#     for sid in ids:\n",
    "#         ent = lookup.get(sid)\n",
    "#         if ent:\n",
    "#             targets.append({\"page\": int(ent[\"page\"]), \"text\": ent[\"sentence\"]})\n",
    "\n",
    "# # group targets per page\n",
    "# by_page = defaultdict(list)\n",
    "# for t in targets:\n",
    "#     by_page[int(t[\"page\"])].append(t[\"text\"])\n",
    "\n",
    "# # ---------- highlight\n",
    "\n",
    "# doc = fitz.open(pdf_in)\n",
    "# total_spans, misses = 0, []\n",
    "\n",
    "# for page_num, pieces in sorted(by_page.items()):\n",
    "#     page = doc[page_num - 1]\n",
    "#     blocks = build_blocks(page)\n",
    "\n",
    "#     for raw in pieces:\n",
    "#         found = False\n",
    "#         target_tokens = norm_tokenize(raw)\n",
    "#         if not target_tokens:\n",
    "#             continue\n",
    "\n",
    "#         # 1) block-scoped exact token window\n",
    "#         for b_idx, wlist in blocks.items():\n",
    "#             stream, tok2word = stream_from_words(wlist)\n",
    "#             loc = exact_window(stream, target_tokens)\n",
    "#             if loc:\n",
    "#                 rlist = rects_from_span(wlist, tok2word, *loc)\n",
    "#                 for r in rlist:\n",
    "#                     page.add_highlight_annot(r)\n",
    "#                     total_spans += 1\n",
    "#                 found = True\n",
    "#                 break\n",
    "#         if found:\n",
    "#             continue\n",
    "\n",
    "#         # 2) page-wide exact token window\n",
    "#         all_words = [w for bl in blocks.values() for w in bl]\n",
    "#         stream, tok2word = stream_from_words(all_words)\n",
    "#         loc = exact_window(stream, target_tokens)\n",
    "#         if loc:\n",
    "#             rlist = rects_from_span(all_words, tok2word, *loc)\n",
    "#             for r in rlist:\n",
    "#                 page.add_highlight_annot(r)\n",
    "#                 total_spans += 1\n",
    "#             continue\n",
    "\n",
    "#         # 3) search_for with paren/quote/hyphen variants\n",
    "#         tried = False\n",
    "#         for variant in paren_variants(raw):\n",
    "#             v = unicodedata.normalize(\"NFKC\", variant).replace(\"\\u00ad\", \"\")\n",
    "#             v = fold_diacritics(v)\n",
    "#             v = re.sub(HYPHENS, \" \", v)\n",
    "#             v = re.sub(QUOTES, \" \", v)\n",
    "#             v = re.sub(r\"\\s+\", \" \", v).strip()\n",
    "#             if not v:\n",
    "#                 continue\n",
    "#             tried = True\n",
    "#             quads = page.search_for(v, quads=True)\n",
    "#             if quads:\n",
    "#                 for q in quads:\n",
    "#                     page.add_highlight_annot(q)\n",
    "#                     total_spans += 1\n",
    "#                 found = True\n",
    "#                 break\n",
    "#         if found:\n",
    "#             continue\n",
    "\n",
    "#         # 4) tolerant page-wide token window (only for longer phrases)\n",
    "#         if len(target_tokens) >= 10:\n",
    "#             loc = tolerant_window(stream, target_tokens, max_miss=1, min_ratio=0.9)\n",
    "#             if loc:\n",
    "#                 rlist = rects_from_span(all_words, tok2word, *loc)\n",
    "#                 for r in rlist:\n",
    "#                     page.add_highlight_annot(r)\n",
    "#                     total_spans += 1\n",
    "#                 found = True\n",
    "\n",
    "#         if not found:\n",
    "#             misses.append(raw[:90])\n",
    "\n",
    "# doc.save(pdf_out, garbage=4, deflate=True)\n",
    "# doc.close()\n",
    "\n",
    "# print(f\"✅ Highlighted {total_spans} spans → {pdf_out}\")\n",
    "# if misses:\n",
    "#     print(\"⚠️ Not found (sample):\", misses[:6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b91c008e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Highlighted 263 spans → highlighted_output.pdf\n"
     ]
    }
   ],
   "source": [
    "# CELL 5 — robust highlighter (token-aware + tolerant + scoped symbol highlights)\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------- paths\n",
    "base_dir = Path(\".\")\n",
    "# pdf_in   = base_dir / \"geography.pdf\"\n",
    "# pdf_in   = base_dir / \"biology.pdf\"\n",
    "pdf_in   = base_dir / \"class10phy.pdf\"\n",
    "# pdf_in   = base_dir / \"history.pdf\"\n",
    "\n",
    "pdf_out  = base_dir / \"highlighted_output.pdf\"\n",
    "clean_in = base_dir / \"ai_cleaned_sentences.json\"     # from Cell 4\n",
    "ids_path = base_dir / \"highlight_ids.json\"            # fallback if Cell 4 is skipped\n",
    "lookup_path = base_dir / \"sentence_lookup.json\"       # fallback\n",
    "\n",
    "# ---------- normalization / tokenization\n",
    "\n",
    "# Hyphens to normalize to spaces\n",
    "HYPHENS = r\"[\\-\\u2010-\\u2015\\u2212]\"  # -, ‐, –, —, −\n",
    "\n",
    "# IMPORTANT: do NOT include Unicode primes here; we want to preserve them.\n",
    "# Keep only typographic quotes/apostrophes.\n",
    "QUOTES  = r\"[\\u2018\\u2019\\u201A\\u201C\\u201D\\u201E']\"\n",
    "\n",
    "# Unicode prime characters (′ ″ ‴ ⁗)\n",
    "PRIME_CHARS = \"\\u2032\\u2033\\u2034\\u2057\"\n",
    "\n",
    "def fold_diacritics(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    return unicodedata.normalize(\"NFC\", s)\n",
    "\n",
    "def normalize_primes_to_tokens(s: str) -> str:\n",
    "    # Map primes to textual tokens so token windows align even when PDF splits them.\n",
    "    # e.g., \"h′\" → \"h prime\", \"″\" → \" doubleprime\"\n",
    "    s = s.replace(\"\\u2032\", \" prime\")\n",
    "    s = s.replace(\"\\u2033\", \" doubleprime\")\n",
    "    s = s.replace(\"\\u2034\", \" tripleprime\")\n",
    "    s = s.replace(\"\\u2057\", \" quadrupleprime\")\n",
    "    return s\n",
    "\n",
    "def prime_variants(s: str):\n",
    "    # Return variants that keep primes intact or map them to ASCII apostrophes / remove.\n",
    "    # Used by search_for to increase robustness.\n",
    "    v = s\n",
    "    # 1) raw (keep exact)\n",
    "    yield v\n",
    "    # 2) map primes to ASCII apostrophe\n",
    "    v2 = v.translate({ord(\"\\u2032\"): ord(\"'\"), ord(\"\\u2033\"): ord('\"')})\n",
    "    yield v2\n",
    "    # 3) drop primes (sometimes PDF glues them to letters awkwardly)\n",
    "    v3 = re.sub(f\"[{PRIME_CHARS}]\", \"\", v)\n",
    "    yield v3\n",
    "\n",
    "def norm_tokenize(s: str):\n",
    "    # tokenization that preserves semantic symbols:\n",
    "    # - keep primes by mapping them to tokens (prime/doubleprime/…)\n",
    "    # - keep slashes for ratios via later operator highlighting; for token stream, we drop punctuation\n",
    "    s = unicodedata.normalize(\"NFKC\", s).replace(\"\\u00ad\", \"\")\n",
    "    s = fold_diacritics(s)\n",
    "    s = normalize_primes_to_tokens(s)            # <-- preserve as words\n",
    "    s = re.sub(HYPHENS, \" \", s)\n",
    "    s = re.sub(QUOTES, \" \", s)\n",
    "    # Keep all other punctuation out of tokens\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
    "    return s.split() if s else []\n",
    "\n",
    "def paren_variants(s: str):\n",
    "    # Variants for parentheses spacing; keep primes intact\n",
    "    v1 = s\n",
    "    v2 = re.sub(r\"\\s*\\(\\s*\", \" (\", v1)\n",
    "    v2 = re.sub(r\"\\s*\\)\\s*\", \")\", v2)\n",
    "    v3 = re.sub(r\"[()]\", \"\", v2)\n",
    "    v3 = re.sub(r\"\\s+\", \" \", v3).strip()\n",
    "    return [v1, v2, v3]\n",
    "\n",
    "# ---------- operator/symbol support (scoped to matched sentence region)\n",
    "OP_CHARS = (\n",
    "    \"=<>±≈≠≤≥∝↔→\"          # relations\n",
    "    \"+−-*/^×·÷\"            # arithmetic (ASCII '-' and U+2212 '−')\n",
    "    \"√∞∑Σ∫πθΔ∂µΩ°\"\n",
    ")\n",
    "\n",
    "# Also treat parentheses and primes as micro-highlights inside region\n",
    "EXTRA_REGION_SYMS = \"()\"\n",
    "PRIME_SET = set(PRIME_CHARS)\n",
    "\n",
    "OP_STRINGS = [\"<=\", \">=\", \"==\", \"!=\", \"+=\", \"-=\", \"->\", \"<-\", \"<->\"]\n",
    "\n",
    "def has_ops_or_primes_or_parens(s: str) -> bool:\n",
    "    return (\n",
    "        any(ch in s for ch in OP_CHARS)\n",
    "        or any(seq in s for seq in OP_STRINGS)\n",
    "        or any(ch in s for ch in EXTRA_REGION_SYMS)\n",
    "        or any(ch in s for ch in PRIME_SET)\n",
    "    )\n",
    "\n",
    "# ---------- utilities\n",
    "def build_blocks(page):\n",
    "    \"\"\"Return {block_idx: [word tuples]} using page.get_text('words').\"\"\"\n",
    "    words = page.get_text(\"words\")  # (x0,y0,x1,y1,text,block_no,line_no,word_no)\n",
    "    if not words:\n",
    "        return {-1: []}\n",
    "    has_blocks = len(words[0]) >= 8\n",
    "    blocks = defaultdict(list)\n",
    "    if has_blocks:\n",
    "        for w in words:\n",
    "            blocks[w[5]].append(w)\n",
    "    else:\n",
    "        blocks[-1] = words\n",
    "    return blocks\n",
    "\n",
    "def stream_from_words(wlist):\n",
    "    stream, tok2word = [], []\n",
    "    for wi, w in enumerate(wlist):\n",
    "        toks = norm_tokenize(w[4])    # tokenizing PDF word text with same rules\n",
    "        for t in toks:\n",
    "            stream.append(t)\n",
    "            tok2word.append(wi)\n",
    "    return stream, tok2word\n",
    "\n",
    "def rects_from_span(wlist, tok2word, i0, i1):\n",
    "    idxs = sorted(set(tok2word[i0:i1+1]))\n",
    "    return [fitz.Rect(*wlist[idx][0:4]) for idx in idxs]\n",
    "\n",
    "def exact_window(stream, target):\n",
    "    n, m = len(stream), len(target)\n",
    "    if m == 0 or n < m:\n",
    "        return None\n",
    "    for i in range(n - m + 1):\n",
    "        if stream[i:i+m] == target:\n",
    "            return (i, i+m-1)\n",
    "    return None\n",
    "\n",
    "def tolerant_window(stream, target, max_miss=1, min_ratio=0.9):\n",
    "    n, m = len(stream), len(target)\n",
    "    if m == 0 or n < m:\n",
    "        return None\n",
    "    need = max(int(ceil(min_ratio * m)), m - max_miss)\n",
    "    for i in range(n - m + 1):\n",
    "        win = stream[i:i+m]\n",
    "        matches = sum(1 for a, b in zip(win, target) if a == b)\n",
    "        if matches >= need:\n",
    "            return (i, i+m-1)\n",
    "    return None\n",
    "\n",
    "def union_rect(rects):\n",
    "    if not rects:\n",
    "        return None\n",
    "    u = fitz.Rect(rects[0])\n",
    "    for r in rects[1:]:\n",
    "        u |= r\n",
    "    return u\n",
    "\n",
    "# ---------- load targets\n",
    "targets = []  # {\"page\": int, \"text\": str}\n",
    "\n",
    "if clean_in.exists():\n",
    "    data = json.loads(clean_in.read_text(encoding=\"utf-8\"))\n",
    "    for row in data:\n",
    "        page = int(row[\"page\"])\n",
    "        for piece in row.get(\"pieces\", []):\n",
    "            piece = piece.strip()\n",
    "            if piece:\n",
    "                targets.append({\"page\": page, \"text\": piece})\n",
    "else:\n",
    "    # fallback to raw ids\n",
    "    ids = json.loads(ids_path.read_text(encoding=\"utf-8\"))\n",
    "    lookup = json.loads(lookup_path.read_text(encoding=\"utf-8\"))\n",
    "    for sid in ids:\n",
    "        ent = lookup.get(sid)\n",
    "        if ent:\n",
    "            targets.append({\"page\": int(ent[\"page\"]), \"text\": ent[\"sentence\"]})\n",
    "\n",
    "# group targets per page\n",
    "by_page = defaultdict(list)\n",
    "for t in targets:\n",
    "    by_page[int(t[\"page\"])].append(t[\"text\"])\n",
    "\n",
    "# ---------- highlight\n",
    "doc = fitz.open(pdf_in)\n",
    "total_spans, misses = 0, []\n",
    "\n",
    "for page_num, pieces in sorted(by_page.items()):\n",
    "    page = doc[page_num - 1]\n",
    "    blocks = build_blocks(page)\n",
    "\n",
    "    for raw in pieces:\n",
    "        if not raw.strip():\n",
    "            continue\n",
    "\n",
    "        found = False\n",
    "        sentence_rects = None\n",
    "        target_tokens = norm_tokenize(raw)\n",
    "        if not target_tokens:\n",
    "            continue\n",
    "\n",
    "        # 1) block-scoped exact token window\n",
    "        for _b_idx, wlist in blocks.items():\n",
    "            stream, tok2word = stream_from_words(wlist)\n",
    "            loc = exact_window(stream, target_tokens)\n",
    "            if loc:\n",
    "                rlist = rects_from_span(wlist, tok2word, *loc)\n",
    "                for r in rlist:\n",
    "                    page.add_highlight_annot(r)\n",
    "                    total_spans += 1\n",
    "                sentence_rects = rlist\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        # 2) page-wide exact token window\n",
    "        if not found:\n",
    "            all_words = [w for bl in blocks.values() for w in bl]\n",
    "            stream, tok2word = stream_from_words(all_words)\n",
    "            loc = exact_window(stream, target_tokens)\n",
    "            if loc:\n",
    "                rlist = rects_from_span(all_words, tok2word, *loc)\n",
    "                for r in rlist:\n",
    "                    page.add_highlight_annot(r)\n",
    "                    total_spans += 1\n",
    "                sentence_rects = rlist\n",
    "                found = True\n",
    "\n",
    "        # 3) search_for with paren/prime variants (no stripping of primes!)\n",
    "        if not found:\n",
    "            for base in paren_variants(raw):\n",
    "                for variant in prime_variants(base):\n",
    "                    v = unicodedata.normalize(\"NFKC\", variant).replace(\"\\u00ad\", \"\")\n",
    "                    v = fold_diacritics(v)\n",
    "                    # normalize hyphens/quotes spacing but DO NOT remove primes\n",
    "                    v = re.sub(HYPHENS, \" \", v)\n",
    "                    v = re.sub(QUOTES, \" \", v)\n",
    "                    v = re.sub(r\"\\s+\", \" \", v).strip()\n",
    "                    if not v:\n",
    "                        continue\n",
    "                    quads = page.search_for(v, quads=True)\n",
    "                    if quads:\n",
    "                        for q in quads:\n",
    "                            page.add_highlight_annot(q)\n",
    "                            total_spans += 1\n",
    "                        sentence_rects = [fitz.Rect(q.rect) for q in quads]\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "\n",
    "        # 4) tolerant page-wide token window (only for longer phrases)\n",
    "        if not found and len(target_tokens) >= 10:\n",
    "            all_words = [w for bl in blocks.values() for w in bl]\n",
    "            stream, tok2word = stream_from_words(all_words)\n",
    "            loc = tolerant_window(stream, target_tokens, max_miss=1, min_ratio=0.9)\n",
    "            if loc:\n",
    "                rlist = rects_from_span(all_words, tok2word, *loc)\n",
    "                for r in rlist:\n",
    "                    page.add_highlight_annot(r)\n",
    "                    total_spans += 1\n",
    "                sentence_rects = rlist\n",
    "                found = True\n",
    "\n",
    "        # ----- scoped micro-highlights: operators + primes + parentheses -----\n",
    "        if found and sentence_rects and has_ops_or_primes_or_parens(raw):\n",
    "            region = union_rect(sentence_rects)\n",
    "            pad = 1.5\n",
    "            region = fitz.Rect(region.x0 - pad, region.y0 - pad, region.x1 + pad, region.y1 + pad)\n",
    "\n",
    "            # single-glyph operators\n",
    "            for ch in set(OP_CHARS):\n",
    "                if ch in raw:\n",
    "                    quads = page.search_for(ch, quads=True)\n",
    "                    for q in quads:\n",
    "                        r = fitz.Rect(q.rect)\n",
    "                        if region.contains(r):\n",
    "                            page.add_highlight_annot(q)\n",
    "                            total_spans += 1\n",
    "\n",
    "            # multi-char ASCII sequences\n",
    "            for seq in OP_STRINGS:\n",
    "                if seq in raw:\n",
    "                    quads = page.search_for(seq, quads=True)\n",
    "                    for q in quads:\n",
    "                        r = fitz.Rect(q.rect)\n",
    "                        if region.contains(r):\n",
    "                            page.add_highlight_annot(q)\n",
    "                            total_spans += 1\n",
    "\n",
    "            # parentheses\n",
    "            for ch in \"()\":\n",
    "                if ch in raw:\n",
    "                    quads = page.search_for(ch, quads=True)\n",
    "                    for q in quads:\n",
    "                        r = fitz.Rect(q.rect)\n",
    "                        if region.contains(r):\n",
    "                            page.add_highlight_annot(q)\n",
    "                            total_spans += 1\n",
    "\n",
    "            # primes (′ ″ ‴ ⁗)\n",
    "            for ch in PRIME_SET:\n",
    "                if ch in raw:\n",
    "                    quads = page.search_for(ch, quads=True)\n",
    "                    for q in quads:\n",
    "                        r = fitz.Rect(q.rect)\n",
    "                        if region.contains(r):\n",
    "                            page.add_highlight_annot(q)\n",
    "                            total_spans += 1\n",
    "\n",
    "        if not found:\n",
    "            misses.append(raw[:90])\n",
    "\n",
    "doc.save(pdf_out, garbage=4, deflate=True)\n",
    "doc.close()\n",
    "\n",
    "print(f\"✅ Highlighted {total_spans} spans → {pdf_out}\")\n",
    "if misses:\n",
    "    print(\"⚠️ Not found (sample):\", misses[:6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3e29c299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Highlighted 261 spans → highlighted_output.pdf\n"
     ]
    }
   ],
   "source": [
    "# CELL 5 — robust highlighter (token-aware + tolerant + fully gated operators)\n",
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------- paths\n",
    "base_dir = Path(\".\")\n",
    "pdf_in   = base_dir / \"class10phy.pdf\"\n",
    "pdf_out  = base_dir / \"highlighted_output.pdf\"\n",
    "clean_in = base_dir / \"ai_cleaned_sentences.json\"     # from Cell 4\n",
    "ids_path = base_dir / \"highlight_ids.json\"            # fallback if Cell 4 is skipped\n",
    "lookup_path = base_dir / \"sentence_lookup.json\"       # fallback\n",
    "\n",
    "# ---------- normalization / tokenization\n",
    "HYPHENS = r\"[\\-\\u2010-\\u2015\\u2212]\"  # -, ‐, –, —, −\n",
    "QUOTES  = r\"[\\u2018\\u2019\\u201A\\u201C\\u201D\\u201E']\"\n",
    "PRIME_CHARS = \"\\u2032\\u2033\\u2034\\u2057\"\n",
    "PRIME_SET = set(PRIME_CHARS)\n",
    "\n",
    "def fold_diacritics(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFD\", s)\n",
    "    s = \"\".join(c for c in s if not unicodedata.combining(c))\n",
    "    return unicodedata.normalize(\"NFC\", s)\n",
    "\n",
    "def normalize_for_search(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", s).replace(\"\\u00ad\", \"\")\n",
    "    s = fold_diacritics(s)\n",
    "    s = re.sub(HYPHENS, \" \", s)\n",
    "    s = re.sub(QUOTES, \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def normalize_primes_to_tokens(s: str) -> str:\n",
    "    return (s.replace(\"\\u2032\", \" prime\")\n",
    "             .replace(\"\\u2033\", \" doubleprime\")\n",
    "             .replace(\"\\u2034\", \" tripleprime\")\n",
    "             .replace(\"\\u2057\", \" quadrupleprime\"))\n",
    "\n",
    "def prime_variants(s: str):\n",
    "    yield s\n",
    "    yield s.translate({ord(\"\\u2032\"): ord(\"'\"), ord(\"\\u2033\"): ord('\"')})\n",
    "    yield re.sub(f\"[{PRIME_CHARS}]\", \"\", s)\n",
    "\n",
    "def norm_tokenize(s: str):\n",
    "    s = unicodedata.normalize(\"NFKC\", s).replace(\"\\u00ad\", \"\")\n",
    "    s = fold_diacritics(s)\n",
    "    s = normalize_primes_to_tokens(s)\n",
    "    s = re.sub(HYPHENS, \" \", s)\n",
    "    s = re.sub(QUOTES, \" \", s)\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
    "    return s.split() if s else []\n",
    "\n",
    "def paren_variants(s: str):\n",
    "    v1 = s\n",
    "    v2 = re.sub(r\"\\s*\\(\\s*\", \" (\", v1); v2 = re.sub(r\"\\s*\\)\\s*\", \")\", v2)\n",
    "    v3 = re.sub(r\"[()]\", \"\", v2); v3 = re.sub(r\"\\s+\", \" \", v3).strip()\n",
    "    return [v1, v2, v3]\n",
    "\n",
    "# ---------- operator inventory\n",
    "OPS_MULTI  = [\"<=\", \">=\", \"==\", \"!=\", \"->\", \"<-\", \"<->\"]\n",
    "OPS_SINGLE = list(\"=+-*/^×·÷:<>≤≥≠→↔∝\") + [\"−\"]  # include Unicode minus\n",
    "\n",
    "# Optional on-page glyph aliases (keep conservative to avoid false hits)\n",
    "OP_ALIASES = {\n",
    "    \"-\": [\"-\", \"−\"],   # ASCII minus & Unicode minus\n",
    "    \"−\": [\"−\", \"-\"],   # either way\n",
    "}\n",
    "\n",
    "EXTRA_REGION_SYMS = \"()\"\n",
    "\n",
    "def has_symbols(s: str) -> bool:\n",
    "    return any(ch in s for ch in OPS_SINGLE) or any(seq in s for seq in OPS_MULTI) or any(ch in s for ch in EXTRA_REGION_SYMS) or any(ch in s for ch in PRIME_SET)\n",
    "\n",
    "# ---------- PDF helpers\n",
    "def build_blocks(page):\n",
    "    words = page.get_text(\"words\")\n",
    "    if not words:\n",
    "        return {-1: []}\n",
    "    has_blocks = len(words[0]) >= 8\n",
    "    blocks = defaultdict(list)\n",
    "    if has_blocks:\n",
    "        for w in words: blocks[w[5]].append(w)\n",
    "    else:\n",
    "        blocks[-1] = words\n",
    "    return blocks\n",
    "\n",
    "def stream_from_words(wlist):\n",
    "    stream, tok2word = [], []\n",
    "    for wi, w in enumerate(wlist):\n",
    "        toks = norm_tokenize(w[4])\n",
    "        for t in toks:\n",
    "            stream.append(t); tok2word.append(wi)\n",
    "    return stream, tok2word\n",
    "\n",
    "def rects_from_span(wlist, tok2word, i0, i1):\n",
    "    idxs = sorted(set(tok2word[i0:i1+1]))\n",
    "    return [fitz.Rect(*wlist[idx][0:4]) for idx in idxs]\n",
    "\n",
    "def exact_window(stream, target):\n",
    "    n, m = len(stream), len(target)\n",
    "    if m == 0 or n < m: return None\n",
    "    for i in range(n - m + 1):\n",
    "        if stream[i:i+m] == target:\n",
    "            return (i, i+m-1)\n",
    "    return None\n",
    "\n",
    "def tolerant_window(stream, target, max_miss=1, min_ratio=0.9):\n",
    "    n, m = len(stream), len(target)\n",
    "    if m == 0 or n < m: return None\n",
    "    need = max(int(ceil(min_ratio * m)), m - max_miss)\n",
    "    for i in range(n - m + 1):\n",
    "        win = stream[i:i+m]\n",
    "        matches = sum(1 for a, b in zip(win, target) if a == b)\n",
    "        if matches >= need:\n",
    "            return (i, i+m-1)\n",
    "    return None\n",
    "\n",
    "def union_rect(rects):\n",
    "    if not rects: return None\n",
    "    u = fitz.Rect(rects[0])\n",
    "    for r in rects[1:]: u |= r\n",
    "    return u\n",
    "\n",
    "def median_height(rects):\n",
    "    if not rects: return 1.0\n",
    "    hs = sorted((r.y1 - r.y0) for r in rects)\n",
    "    k = len(hs)//2\n",
    "    return hs[k] if len(hs)%2 else 0.5*(hs[k-1]+hs[k])\n",
    "\n",
    "def same_line(r1, r2, ypad):\n",
    "    yc = 0.5*(r1.y0+r1.y1)\n",
    "    return (r2.y0-ypad) <= yc <= (r2.y1+ypad)\n",
    "\n",
    "# ---------- general gated operators\n",
    "TOKEN = r\"(?:\\d+(?:\\.\\d+)?|[A-Za-z](?:\\d+)?)\"   # 12, 1.33, v2, n1, x\n",
    "\n",
    "def pairs_for_op(text: str, op: str):\n",
    "    \"\"\"Extract (left_token, right_token) pairs for a specific operator from the piece text.\"\"\"\n",
    "    esc = re.escape(op)\n",
    "    pairs = []\n",
    "    # Allow optional spaces around operator\n",
    "    for m in re.finditer(rf\"({TOKEN})\\s*{esc}\\s*({TOKEN})\", text):\n",
    "        pairs.append((m.group(1), m.group(2)))\n",
    "    return pairs\n",
    "\n",
    "def search_token(page, region, tok):\n",
    "    tok = normalize_for_search(tok)\n",
    "    hits = []\n",
    "    for q in page.search_for(tok, quads=True):\n",
    "        r = fitz.Rect(q.rect)\n",
    "        if region.contains(r):\n",
    "            hits.append(r)\n",
    "    return hits\n",
    "\n",
    "def collect_glyph_quads(page, region, op):\n",
    "    \"\"\"All quads for operator glyphs inside region, with alias support.\"\"\"\n",
    "    quads = []\n",
    "    glyphs = OP_ALIASES.get(op, [op])\n",
    "    for g in glyphs:\n",
    "        quads.extend([q for q in page.search_for(g, quads=True) if region.contains(fitz.Rect(q.rect))])\n",
    "    return quads\n",
    "\n",
    "def score_op_quad(qr, Lr, Rr, ypad):\n",
    "    if not (same_line(qr, Lr, ypad) and same_line(qr, Rr, ypad)):\n",
    "        return 1e9\n",
    "    midx = 0.5*(Lr.x1 + Rr.x0)\n",
    "    qx   = 0.5*(qr.x0 + qr.x1)\n",
    "    # prefer centered between tokens and actually between them\n",
    "    between_pen = 0.0 if (Lr.x1 <= qr.x0 and qr.x1 <= Rr.x0) else 10_000.0\n",
    "    center_pen  = 0.1 * abs(qx - midx)\n",
    "    return between_pen + center_pen\n",
    "\n",
    "def highlight_op_gated(page, region, sentence_rects, piece_text, op, total_spans):\n",
    "    \"\"\"Gate a single operator type for this piece. Returns new spans added.\"\"\"\n",
    "    added = 0\n",
    "    # Special-case multi-char ops FIRST (they’re unique sequences)\n",
    "    pairs = pairs_for_op(piece_text, op)\n",
    "    need = len(pairs)\n",
    "    if need == 0:\n",
    "        return 0\n",
    "\n",
    "    op_quads = collect_glyph_quads(page, region, op)\n",
    "    if not op_quads:\n",
    "        return 0\n",
    "\n",
    "    ypad = max(1.0, 0.35*median_height(sentence_rects))\n",
    "    used = set()\n",
    "\n",
    "    for Lt, Rt in pairs:\n",
    "        Ls = search_token(page, region, Lt)\n",
    "        Rs = search_token(page, region, Rt)\n",
    "        if not Ls or not Rs:\n",
    "            continue\n",
    "        # choose best L/R on same baseline with minimal gap\n",
    "        best_pair = None; best_gap = 1e18\n",
    "        for Lr in Ls:\n",
    "            for Rr in Rs:\n",
    "                if Rr.x0 <= Lr.x1:  # wrong order\n",
    "                    continue\n",
    "                if not same_line(Lr, Rr, ypad):\n",
    "                    continue\n",
    "                gap = Rr.x0 - Lr.x1\n",
    "                if gap < best_gap:\n",
    "                    best_gap = gap; best_pair = (Lr, Rr)\n",
    "        if not best_pair:\n",
    "            continue\n",
    "\n",
    "        Lr, Rr = best_pair\n",
    "        # pick the op glyph between L and R\n",
    "        candidates = []\n",
    "        for i, q in enumerate(op_quads):\n",
    "            if i in used: continue\n",
    "            qr = fitz.Rect(q.rect)\n",
    "            s = score_op_quad(qr, Lr, Rr, ypad)\n",
    "            if s < 9e8:\n",
    "                candidates.append((s, i))\n",
    "        if candidates:\n",
    "            candidates.sort(key=lambda t: t[0])\n",
    "            idx = candidates[0][1]\n",
    "            page.add_highlight_annot(op_quads[idx])\n",
    "            used.add(idx)\n",
    "            added += 1\n",
    "\n",
    "    # Fallback: if anchors failed (kerning/OCR), select remaining best matches on the midline\n",
    "    if added < need:\n",
    "        yc = sorted((0.5*(r.y0+r.y1) for r in sentence_rects))[len(sentence_rects)//2]\n",
    "        residual = []\n",
    "        for i, q in enumerate(op_quads):\n",
    "            if i in used: continue\n",
    "            qr = fitz.Rect(q.rect)\n",
    "            residual.append((abs(0.5*(qr.y0+qr.y1)-yc), qr.x0, i))\n",
    "        residual.sort()\n",
    "        for _, _, i in residual[:max(0, need-added)]:\n",
    "            page.add_highlight_annot(op_quads[i])\n",
    "            used.add(i)\n",
    "            added += 1\n",
    "\n",
    "    return added\n",
    "\n",
    "# ---------- load targets\n",
    "targets = []\n",
    "if clean_in.exists():\n",
    "    data = json.loads(clean_in.read_text(encoding=\"utf-8\"))\n",
    "    for row in data:\n",
    "        page = int(row[\"page\"])\n",
    "        for piece in row.get(\"pieces\", []):\n",
    "            piece = piece.strip()\n",
    "            if piece:\n",
    "                targets.append({\"page\": page, \"text\": piece})\n",
    "else:\n",
    "    ids = json.loads(ids_path.read_text(encoding=\"utf-8\"))\n",
    "    lookup = json.loads(lookup_path.read_text(encoding=\"utf-8\"))\n",
    "    for sid in ids:\n",
    "        ent = lookup.get(sid)\n",
    "        if ent:\n",
    "            targets.append({\"page\": int(ent[\"page\"]), \"text\": ent[\"sentence\"]})\n",
    "\n",
    "by_page = defaultdict(list)\n",
    "for t in targets:\n",
    "    by_page[int(t[\"page\"])].append(t[\"text\"])\n",
    "\n",
    "# ---------- highlight\n",
    "doc = fitz.open(pdf_in)\n",
    "total_spans, misses = 0, []\n",
    "\n",
    "for page_num, pieces in sorted(by_page.items()):\n",
    "    page = doc[page_num - 1]\n",
    "    blocks = build_blocks(page)\n",
    "\n",
    "    for raw in pieces:\n",
    "        if not raw.strip():\n",
    "            continue\n",
    "\n",
    "        found = False\n",
    "        sentence_rects = None\n",
    "        target_tokens = norm_tokenize(raw)\n",
    "        if not target_tokens:\n",
    "            continue\n",
    "\n",
    "        # 1) block-scoped exact token window\n",
    "        for _b_idx, wlist in blocks.items():\n",
    "            stream, tok2word = stream_from_words(wlist)\n",
    "            loc = exact_window(stream, target_tokens)\n",
    "            if loc:\n",
    "                rlist = rects_from_span(wlist, tok2word, *loc)\n",
    "                for r in rlist:\n",
    "                    page.add_highlight_annot(r); total_spans += 1\n",
    "                sentence_rects = rlist; found = True\n",
    "                break\n",
    "\n",
    "        # 2) page-wide exact token window\n",
    "        if not found:\n",
    "            all_words = [w for bl in blocks.values() for w in bl]\n",
    "            stream, tok2word = stream_from_words(all_words)\n",
    "            loc = exact_window(stream, target_tokens)\n",
    "            if loc:\n",
    "                rlist = rects_from_span(all_words, tok2word, *loc)\n",
    "                for r in rlist:\n",
    "                    page.add_highlight_annot(r); total_spans += 1\n",
    "                sentence_rects = rlist; found = True\n",
    "\n",
    "        # 3) search_for with paren/prime variants\n",
    "        if not found:\n",
    "            for base in paren_variants(raw):\n",
    "                for variant in prime_variants(base):\n",
    "                    v = normalize_for_search(variant)\n",
    "                    if not v: continue\n",
    "                    quads = page.search_for(v, quads=True)\n",
    "                    if quads:\n",
    "                        for q in quads:\n",
    "                            page.add_highlight_annot(q); total_spans += 1\n",
    "                        sentence_rects = [fitz.Rect(q.rect) for q in quads]\n",
    "                        found = True; break\n",
    "                if found: break\n",
    "\n",
    "        # 4) tolerant page-wide token window\n",
    "        if not found and len(target_tokens) >= 10:\n",
    "            all_words = [w for bl in blocks.values() for w in bl]\n",
    "            stream, tok2word = stream_from_words(all_words)\n",
    "            loc = tolerant_window(stream, target_tokens, max_miss=1, min_ratio=0.9)\n",
    "            if loc:\n",
    "                rlist = rects_from_span(all_words, tok2word, *loc)\n",
    "                for r in rlist:\n",
    "                    page.add_highlight_annot(r); total_spans += 1\n",
    "                sentence_rects = rlist; found = True\n",
    "\n",
    "        if not found:\n",
    "            misses.append(raw[:90])\n",
    "            continue\n",
    "\n",
    "        # Region for this sentence (tight, tiny pad)\n",
    "        region = union_rect(sentence_rects)\n",
    "        pad = 1.5\n",
    "        region = fitz.Rect(region.x0 - pad, region.y0 - pad, region.x1 + pad, region.y1 + pad)\n",
    "\n",
    "        # ----- gated operators for ALL ops -----\n",
    "        # Multi-char ops first to avoid double counting (e.g., '<=' vs '<' and '=')\n",
    "        for op in OPS_MULTI:\n",
    "            total_spans += highlight_op_gated(page, region, sentence_rects, raw, op, total_spans)\n",
    "        for op in OPS_SINGLE:\n",
    "            total_spans += highlight_op_gated(page, region, sentence_rects, raw, op, total_spans)\n",
    "\n",
    "        # ----- micro: parentheses & primes (kept)\n",
    "        if any(ch in raw for ch in \"()\"):\n",
    "            for ch in \"()\":\n",
    "                for q in page.search_for(ch, quads=True):\n",
    "                    r = fitz.Rect(q.rect)\n",
    "                    if region.contains(r):\n",
    "                        page.add_highlight_annot(q); total_spans += 1\n",
    "        for ch in PRIME_SET:\n",
    "            if ch in raw:\n",
    "                for q in page.search_for(ch, quads=True):\n",
    "                    r = fitz.Rect(q.rect)\n",
    "                    if region.contains(r):\n",
    "                        page.add_highlight_annot(q); total_spans += 1\n",
    "\n",
    "doc.save(pdf_out, garbage=4, deflate=True)\n",
    "doc.close()\n",
    "\n",
    "print(f\"✅ Highlighted {total_spans} spans → {pdf_out}\")\n",
    "if misses:\n",
    "    print(\"⚠️ Not found (sample):\", misses[:6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c5e14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
